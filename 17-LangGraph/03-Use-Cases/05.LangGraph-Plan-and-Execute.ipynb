{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../common\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith 추적이 활성화되었습니다. [프로젝트명: 17.LangGraph-Use-Case]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith_tracker import set_tracking\n",
    "\n",
    "# 인스턴스를 생성할 때 필요한 매개변수를 전달합니다.\n",
    "set_tracking(project_name=\"17.LangGraph-Use-Case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan-and-Execute\n",
    "\n",
    "`Plan-and-Execute`는 어떤 작업을 수행할때 계획을 먼저 세우고 실행하는 것을 의미합니다.   \n",
    "복잡한 테스크를 Agent 에게 전달할때 Planner 가 계획을 먼저 세우게 됩니다.  \n",
    "그 다음 순차적으로 Agent 에게 테스크와 계획을 전달하고 Planner 가 세운 계획대로 Agent 가 액션을 취하게 됩니다.  \n",
    "그 다음 replan 에서 계획대로 실행할 수 있게 순차진행 하게됩니다.  \n",
    "  \n",
    "`Plan-and-Execute`는 다음과 같은 특징을 같습니다:  \n",
    "\n",
    "- 장기 계획 수립: 복잡한 작업을 수행하기 전에 큰 그림을 그리는 장기 계획을 수립합니다.\n",
    "- 단계별 실행 및 재계획: 세운 계획을 단계별로 실행하고, 각 단계가 완료될 때마다 계획이 여전히 유효한지 검토한 뒤 수정할 수 있습니다.\n",
    "\n",
    "이 방식은 Plan-and-Solve 논문과 Baby-AGI 프로젝트에서 영감을 받았습니다.   \n",
    "전통적인 ReAct 스타일의 에이전트는 한 번에 한 단계씩 생각하는 반면, `plan-and-execute` 는 명시적이고 장기적인 계획을 강조합니다.\n",
    "\n",
    "##### 참고자료\n",
    "\n",
    "- [LangGraph 공식 문서](https://langchain-ai.github.io/langgraph/)\n",
    "- [Plan-and-Solve 논문](https://arxiv.org/abs/2305.04091)\n",
    "- [Baby-AGI 프로젝트](https://github.com/yoheinakajima/babyagi)\n",
    "- [ReAct 논문](https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "##### 그래프 흐름\n",
    "\n",
    "![plan_and_execute](./assets/langgraph_use_case_plan_and_execute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기능 구현\n",
    "\n",
    "#### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from models import get_model_name, LLMs\n",
    "\n",
    "# 모델명 정의\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o_MINI)\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 웹 검색 도구 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 도구 초기화\n",
    "from tools_tavily import TavilySearch\n",
    "\n",
    "# 최대 검색 결과를 3으로 설정\n",
    "web_search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "tools = [web_search_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 작업 실행 에이전트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer in Korean.\",\n",
    "        ),\n",
    "        (\"human\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM 정의\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# ReAct 에이전트 생성\n",
    "# llm: 생각하고 말하는 뇌\n",
    "# tools: 문제 해결에 필요한 도구들(계산기, 검색기 등)\n",
    "# state_modifier: LLM이 어떤 역할을 할지 정해주는 지침\n",
    "agent_executor = create_react_agent(llm, tools, state_modifier=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='랭체인 한국어 튜토리얼에 대해서 설명해줘', additional_kwargs={}, response_metadata={}, id='b5f655d3-1184-4f1e-8eea-739588717887'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_pbBbhU73b4fX7fvRmXmT0Wpz', 'function': {'arguments': '{\"query\":\"랭체인 한국어 튜토리얼\"}', 'name': 'tavily_web_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 155, 'total_tokens': 183, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5adb9abd-714e-4dd1-8970-42b080bccd02-0', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': '랭체인 한국어 튜토리얼'}, 'id': 'call_pbBbhU73b4fX7fvRmXmT0Wpz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 155, 'output_tokens': 28, 'total_tokens': 183, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='[{\"title\": \"LangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다. 본 ...\", \"url\": \"https://github.com/teddylee777/langchain-kr\", \"content\": \"teddylee777/langchain-kr\\\\nFolders and files\\\\nLatest commit\\\\nHistory\\\\n01-Basic\\\\n01-Basic\\\\n02-Prompt\\\\n02-Prompt\\\\n03-OutputParser\\\\n03-OutputParser\\\\n04-Model\\\\n04-Model\\\\n05-Memory\\\\n05-Memory\\\\n06-Chains\\\\n06-Chains\\\\n07-RAG\\\\n07-RAG\\\\n08-Agent\\\\n08-Agent\\\\n09-FastAPI\\\\n09-FastAPI\\\\n12-Use-Cases\\\\n12-Use-Cases\\\\n99-Projects\\\\n99-Projects\\\\nprompts\\\\nprompts\\\\n.env_sample\\\\n.env_sample\\\\n.gitignore\\\\n.gitignore\\\\nREADME.md\\\\nREADME.md\\\\nRepository files navigation\\\\n📘 LangChain 한국어 튜토리얼\\\\n🌟 LangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다.\\\\n Topics\\\\nResources\\\\nStars\\\\nWatchers\\\\nForks\\\\nReleases\\\\nPackages\\\\n0\\\\nLanguages\\\\nFooter\\\\nFooter navigation 🍿 유튜브\\\\n✏️ 블로그 글 목록\\\\nGeneral\\\\nOpenAI Python API\\\\nLangChain\\\\nLangGraph\\\\n👥 LangChain 밋업 2024 Q1 발표자료\\\\n📜 라이선스\\\\n본 프로젝트는 Apache License 2.0에 따라 라이선스가 부여됩니다.\\\\n 📚 출처\\\\n🌐 추가 자료\\\\n🚀 시작하기\\\\n본 튜토리얼을 시작하기 전에, LangChain과 관련된 기본적인 지식을 갖추는 것이 좋습니다. Saved searches\\\\nUse saved searches to filter your results more quickly\\\\nTo see all available qualifiers, see our documentation.\\\\n\", \"score\": 0.8319231, \"raw_content\": \"Search code, repositories, users, issues, pull requests...\\\\nProvide feedback\\\\nWe read every piece of feedback, and take your input very seriously.\\\\nSaved searches\\\\nUse saved searches to filter your results more quickly\\\\nTo see all available qualifiers, see our documentation.\\\\nLangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\nteddylee777/langchain-kr\\\\nFolders and files\\\\nLatest commit\\\\nHistory\\\\n01-Basic\\\\n01-Basic\\\\n02-Prompt\\\\n02-Prompt\\\\n03-OutputParser\\\\n03-OutputParser\\\\n04-Model\\\\n04-Model\\\\n05-Memory\\\\n05-Memory\\\\n06-Chains\\\\n06-Chains\\\\n07-RAG\\\\n07-RAG\\\\n08-Agent\\\\n08-Agent\\\\n09-FastAPI\\\\n09-FastAPI\\\\n12-Use-Cases\\\\n12-Use-Cases\\\\n99-Projects\\\\n99-Projects\\\\nprompts\\\\nprompts\\\\n.env_sample\\\\n.env_sample\\\\n.gitignore\\\\n.gitignore\\\\nREADME.md\\\\nREADME.md\\\\nRepository files navigation\\\\n📘 LangChain 한국어 튜토리얼\\\\n🌟 LangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다.\\\\n본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\n🍿 유튜브\\\\n✏️ 블로그 글 목록\\\\nGeneral\\\\nOpenAI Python API\\\\nLangChain\\\\nLangGraph\\\\n👥 LangChain 밋업 2024 Q1 발표자료\\\\n📜 라이선스\\\\n본 프로젝트는 Apache License 2.0에 따라 라이선스가 부여됩니다.\\\\n🚫 라이선스 고지\\\\n🔒 본 내용의 저작권은 2024년 테디노트에 있습니다. 모든 권리는 저작권자에게 있으며, teddylee777@gmail.com 으로 문의할 수 있습니다.\\\\n본 내용의 무단 전재 및 재배포를 금지합니다. 본 내용의 전체 혹은 일부를 인용할 경우, 출처를 명확히 밝혀주시기 바랍니다.\\\\n본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\\\n📚 출처\\\\n🌐 추가 자료\\\\n🚀 시작하기\\\\n본 튜토리얼을 시작하기 전에, LangChain과 관련된 기본적인 지식을 갖추는 것이 좋습니다. 위의 출처 링크를 통해 기본적인 정보를 얻을 수 있습니다.\\\\n💡 컨트리뷰션\\\\n본 튜토리얼에 기여하고자 하는 분들은 언제든지 풀 리퀘스트를 보내주시거나, 이슈를 등록하여 의견을 공유해 주시기 바랍니다. 모든 기여는 본 프로젝트의 발전에 큰 도움이 됩니다. 💖\\\\nAbout\\\\nLangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\nTopics\\\\nResources\\\\nStars\\\\nWatchers\\\\nForks\\\\nReleases\\\\nPackages\\\\n0\\\\nLanguages\\\\nFooter\\\\nFooter navigation\"}, {\"title\": \" - LangChain 한국어 튜토리얼 - WikiDocs\", \"url\": \"https://wikidocs.net/book/14314\", \"content\": \" - LangChain 한국어 튜토리얼🇰🇷 CH01 LangChain 시작하기 01. 설치 영상보고 따라하기 02. OpenAI API 키 발급 및 테스트 03. LangSmith 추적 설정 04. OpenAI API 사용(GPT-4o 멀티모달) 05.\", \"score\": 0.81665546, \"raw_content\": \"<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 - WikiDocs\\\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷 CH01 LangChain 시작하기 01. 설치 영상보고 따라하기 02. OpenAI API 키 발급 및 테스트 03. LangSmith 추적 설정 04. OpenAI API 사용(GPT-4o 멀티모달) 05. LangChain Expression Language(LCEL) 06. LCEL 인터페이스 07. Runnable CH02 프롬프트(Prompt) 01. 프롬프트(Prompt) 02. 퓨샷 프롬프트(FewShotPromptTemplate) 03. LangChain Hub 04. 개인화된 프롬프트(Hub에 업로드) CH03 출력 파서(Output Parsers) 01. Pydantic 출력 파서(PydanticOutputParser) 02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser) 03. 구조화된 출력 파서(StructuredOuputParser) 04. JSON 출력 파서(JsonOutputParser) 05. 데이터프레임 출력 파서(PandasDataFrameOutputParser) 06. 날짜 형식 출력 파서(DatetimeOutputParser) 07. 열거형 출력 파서(EnumOutputParser) 08. 출력 수정 파서(OutputFixingParser) CH04 모델(Model) 01. 다양한 LLM 모델 활용 02. 캐싱(Cache) 03. 모델 직렬화(Serialization) - 저장 및 불러오기 04. 토큰 사용량 확인 05. 구글 생성 AI(Google Generative AI) 06. 허깅페이스 엔드포인트(HuggingFace Endpoints) 07. 허깅페이스 로컬(HuggingFace Local) 08. 허깅페이스 파이프라인(HuggingFace Pipeline) 09. 올라마(Ollama) 10. GPT4ALL 11. 비디오(Video) 질의 응답 LLM (Gemini) CH05 메모리(Memory) 01. 대화 버퍼 메모리(ConversationBufferMemory) 02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory) 03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory) 04. 대화 엔티티 메모리(ConversationEntityMemory) 05. 대화 지식그래프 메모리(ConversationKGMemory) 06. 대화 요약 메모리(ConversationSummaryMemory) 07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory) 08. LCEL Chain 에 메모리 추가 09. SQLite 에 대화내용 저장 10. RunnableWithMessageHistory에 ChatMessageHistory추가 CH06 문서 로더(Document Loader) 01. 도큐먼트(Document) 의 구조 02. PDF 03. 한글(HWP) 04. CSV 05. Excel 06. Word 07. PowerPoint 08. 웹 문서(WebBaseLoader) 09. 텍스트(TextLoader) 10. JSON 11. Arxiv 12. UpstageLayoutAnalysisLoader 13. LlamaParser CH07 텍스트 분할(Text Splitter) 01. 문자 텍스트 분할(CharacterTextSplitter) 02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter) 03. 토큰 텍스트 분할(TokenTextSplitter) 04. 시멘틱 청커(SemanticChunker) 05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) 06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter) 07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter) 08. 재귀적 JSON 분할(RecursiveJsonSplitter) CH08 임베딩(Embedding) 01. OpenAIEmbeddings 02. 캐시 임베딩(CacheBackedEmbeddings) 03. 허깅페이스 임베딩(HuggingFace Embeddings) 04. UpstageEmbeddings 05. OllamaEmbeddings 06. GPT4ALL 임베딩 07. Llama CPP 임베딩 CH09 벡터저장소(VectorStore) 01. Chroma 02. FAISS 03. Pinecone CH10 검색기(Retriever) 01. 벡터스토어 기반 검색기(VectorStore-backed Retriever) 02. 문맥 압축 검색기(ContextualCompressionRetriever) 03. 앙상블 검색기(EnsembleRetriever) 04. 긴 문맥 재정렬(LongContextReorder) 05. 상위 문서 검색기(ParentDocumentRetriever) 06. 다중 쿼리 검색기(MultiQueryRetriever) 07. 다중 벡터저장소 검색기(MultiVectorRetriever) 08. 셀프 쿼리 검색기(SelfQueryRetriever) 09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever) 10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 11. Convex Combination(CC) 적용된 앙상블 검색기(EnsembleRetriever) CH11 리랭커(Reranker) 01. Cross Encoder Reranker 02. Cohere Reranker 03. Jina Reranker 04. FlashRank Reranker CH12 Retrieval Augmented Generation(RAG) 01. PDF 문서 기반 QA(Question-Answer) 02. 네이버 뉴스기사 QA(Question-Answer) 03. RAG 의 기능별 다양한 모듈 활용기 04. RAPTOR: 긴 문맥 요약(Long Context Summary) 05. 대화내용을 기억하는 RAG 체인 CH13 LangChain Expression Language(LCEL) 01. RunnablePassthrough 02. Runnable 구조(그래프) 검토 03. RunnableLambda 04. LLM 체인 라우팅(RunnableLambda, RunnableBranch) 05. RunnableParallel 06. 동적 속성 지정(configurable_fields, configurable_alternatives) 07. @chain 데코레이터로 Runnable 구성 08. RunnableWithMessageHistory 09. 사용자 정의 제네레이터(generator) 10. Runtime Arguments 바인딩 11. 폴백(fallback) 모델 지정 CH14 체인(Chains) 01. 문서 요약 02. SQL 03. 구조화된 출력 체인(with_structered_output) CH15 평가(Evaluations) 01. 합성 테스트 데이터셋 생성(RAGAS) 02. RAGAS 를 활용한 평가 03. 생성한 평가용 데이터셋 업로드(HuggingFace Dataset) 04. LangSmith 데이터셋 생성 05. LLM-as-Judge 06. 임베딩 기반 평가(embedding_distance) 07. 사용자 정의(Custom) LLM 평가 08. Rouge, BLEU, METEOR, SemScore 기반 휴리스틱 평가 09. 실험(Experiment) 평가 비교 10. 요약(Summary) 방식의 평가 11. Groundedness(할루시네이션) 평가 12. 실험 비교(Pairwise Evaluation) 13. 반복 평가 14. 온라인 평가를 활용한 평가 자동화 CH16 에이전트(Agent) 01. 도구(Tools) 02. 도구 바인딩(Binding Tools) 03. 에이전트(Agent) 04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent 05. Iteration 기능과 사람 개입(Human-in-the-loop) 06. Agentic RAG 07. CSVExcel 데이터 분석 Agent 08. Toolkits 활용 Agent 09. RAG + Image Generator Agent(보고서 작성) 10. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools) CH17 LangGraph 01. 핵심 기능 01. LangGraph 에 자주 등장하는 Python 문법이해 02. LangGraph를 활용한 챗봇 구축 03. LangGraph를 활용한 Agent 구축 04. Agent 에 메모리(memory) 추가 05. 노드의 단계별 스트리밍 출력 06. Human-in-the-loop(사람의 개입) 07. 중간단계 개입 되돌림을 통한 상태 수정과 Replay 08. 사람(Human)에게 물어보는 노드 추가 09. 메시지 삭제(RemoveMessage) 10. ToolNode 를 사용하여 도구를 호출하는 방법 11. 병렬 노드 실행을 위한 분기 생성 방법 12. 대화 기록 요약을 추가하는 방법 13. 서브그래프 추가 및 사용 방법 14. 서브그래프의 입력과 출력을 변환하는 방법 15. LangGraph 스트리밍 모드의 모든 것 02. 구조 설계 01. 기본 그래프 생성 02. Naive RAG 03. 관련성 체커(Relevance Checker) 모듈 추가 04. 웹 검색 모듈 추가 05. 쿼리 재작성 모듈 추가 06. Agentic RAG 07. Adaptive RAG 03. Use Cases 01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오) 02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트 03. CRAG(Corrective RAG) 04. Self-RAG 05. 계획 후 실행(Plan-and-Execute) 06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network) 07. 멀티 에이전트 감독자(Multi-Agent Supervisor) 08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) 09. SQL 데이터베이스와 상호작용하는 에이전트 10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 CH18 기타 정보 01. StreamEvent 타입별 정리\\\\nPublished with WikiDocs\\\\n\\\\n\\\\n<랭체인LangChain 노트> - Lang…\\\\n\\\\n\\\\nWikiDocs\\\\n\\\\n\\\\n<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷\\\\n\\\\nAuthor: 테디노트\\\\nLast edited by : Dec. 23, 2024, 3:48 a.m.\\\\nCopyright : \\\\n2,442 Like; \\\\\"추천\\\\\")\\\\n추천은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. \\\\\"추천\\\\\" 한 번씩만 부탁 드리겠습니다🙏🙏\\\\n✅ 랭체인 한국어 튜토리얼 강의\\\\n패스트캠퍼스 - RAG 비법노트\\\\n✅ 랭체인 한국어 튜토리얼 코드저장소(GitHub) 📘🖥️\\\\nhttps://github.com/teddylee777/langchain-kr\\\\n✅ 유튜브 \\\\\"테디노트\\\\\" 🎥📚\\\\nhttps://www.youtube.com/c/@teddynote\\\\n✅ 데이터 분석 블로그 https://teddylee777.github.io\\\\n✅ 문의 teddylee777@gmail.com\\\\nLICENSE\\\\n인용 및 출처 표기\\\\n\\\\n본 저작물을 블로그, 유튜브 등 온라인 매체에 인용하여 게재할 경우, Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 반드시 출처를 명시해야 합니다.\\\\n\\\\n상업적 사용에 대한 사전 협의\\\\n\\\\n본 저작물(Wikidocs 및 관련 실습 코드 포함)을 강의, 강연 등 상업적 목적으로 활용하고자 하는 경우, 저작권자와의 사전 서면 협의가 필수적으로 요구됩니다. 해당 협의는 teddylee777@gmail.com으로 문의하여 진행하실 수 있습니다.\\\\n\\\\n본 저작물은 2024년 테디노트에 의해 작성되었습니다. \\\\n모든 권리는 저작권자에게 있으며, 본 저작물은 Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 배포됩니다.\\\\n본 저작물의 무단 전재 및 재배포를 금지하며, 전체 혹은 일부를 인용할 경우 출처를 명확히 밝혀주시기 바랍니다.\\\\n본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\\\nCopyright (c) 테디노트.\\\\nReference\\\\n\\\\nLangChain Github\\\\nLangGraph Github\\\\nLangChain Document\\\\n\\\\nRecent Comments (7) Recent Modifications (10) RSS\\\\n10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기 - 천세현, Jan. 9, 2025, 12:28 p.m.\\\\n출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요?\\\\nCH01 LangChain 시작하기 - NamHyeon, Dec. 8, 2024, 1:17 p.m.\\\\n좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 \\'테디노트의 RAG 비법노트\\' 강의 등록했습니다 ! 물론 제 현업에 필요한 기술이라서, 강의 또한 기쁜 마음에 신청했구요 ~ 정주행 해서, 창공을 날아가 보겠습니다 ^^\\\\n06. Word - Paul, Oct. 27, 2024, 5:38 p.m.\\\\npython-docx도 설치해야 할까요?\\\\n10. JSON - Paul, Oct. 27, 2024, 5:37 p.m.\\\\n!pip install jq 부분이 들어가야 할 것 같습니다.\\\\n02. PDF - Paul, Oct. 27, 2024, 3:29 p.m.\\\\n<html><head> <meta http-equiv=\\\\\"Content-Type\\\\\" content=\\\\\"text/html\\\\\"> </head><body> <span style=\\\\\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:858px;\\\\\"></span> <div style=\\\\\"position:absolute; top:50px;\\\\\"><a name=\\\\\"1\\\\\">Page 1</a></div> <div style=\\\\\"position:absolute; border 이 부분이 출력 결과가 아니라 코드인 것처럼 표시되어 있네요~\\\\n12. UpstageLayoutAnalysisLoader - Paul, Oct. 27, 2024, 10:59 a.m.\\\\n감사히 잘 참고하고 있습니다. 아주 사소한 오기이지만... 11번 Arxiv 다음에 12번이 와야 할 텐데, 원래 넣으시려던 다른 목차가 빠진 것인지 바로 13번이 나왔네요^^\\\\n03. 모델 직렬화(Serialization) - 저장 및 불러오기 - 동구, Sept. 20, 2024, 12:58 p.m.\\\\nloads는 뭐에요?\\\\n04. Self-RAG - Dec. 23, 2024, 3:48 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등) - Dec. 23, 2024, 3:16 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n10. STORM 개념을 도입한 연구를 위한 멀티 에이전트 - Dec. 23, 2024, 3:16 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n03. CRAG(Corrective RAG) - Dec. 23, 2024, 3:04 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n05. 계획 후 실행(Plan-and-Execute) - Dec. 23, 2024, 3:04 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n07. 멀티 에이전트 감독자(Multi-Agent Supervisor) - Dec. 23, 2024, 3:04 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams) - Dec. 23, 2024, 3:04 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n09. SQL 데이터베이스와 상호작용하는 에이전트 - Dec. 23, 2024, 3:04 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트 - Dec. 23, 2024, 2:45 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n03. 토큰 텍스트 분할(TokenTextSplitter) - Dec. 23, 2024, 2:31 a.m.\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\nNext : CH01 LangChain 시작하기\\\\n\\\\n\\\\n×\\\\n책갈피\\\\n추가 닫기\\\\n\\\\n×\\\\nLeave feedback on this page\\\\nEmail address to reply to\\\\nWhat you want to say\\\\n※ Feedback is delivered to the author by email.\\\\nClose Send\"}, {\"title\": \" - LangChain 한국어 튜토리얼\", \"url\": \"https://www.gpters.org/dev/post/langchain-notes-langchain-korean-UbTO5u3bFgmL0SQ\", \"content\": \"본 전자책은 랭체인 한국어 튜토리얼랭체인 공식 GitHub 튜토리얼을 한국어 예제와 문구로 번역을 기반으로 작성하였습니다 📘랭체인LangChain 노트 - LangChain 한국어 튜토리얼🇰🇷 2 LangChain 공식 Dev 팀에 한국어 튜토리얼 등록 문의와 공식 홈페이지의 튜토리얼 번역에 대한 허가도 받게 되어 드디어\", \"score\": 0.8131201, \"raw_content\": null}]', name='tavily_web_search', id='99c3cbfd-1f9a-48b3-80a7-cb6492113970', tool_call_id='call_pbBbhU73b4fX7fvRmXmT0Wpz'),\n",
       "  AIMessage(content='랭체인(LangChain) 한국어 튜토리얼에 대한 정보는 다음과 같습니다:\\n\\n1. **GitHub 튜토리얼**: [LangChain 한국어 튜토리얼](https://github.com/teddylee777/langchain-kr)에서는 LangChain의 공식 문서와 요리책, 실용 예제를 바탕으로 작성된 한국어 튜토리얼을 제공합니다. 이 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\n\\n2. **WikiDocs**: [LangChain 한국어 튜토리얼 - WikiDocs](https://wikidocs.net/book/14314)에서는 LangChain의 설치 방법, OpenAI API 키 발급 및 테스트, 다양한 기능에 대한 자세한 설명을 포함하고 있습니다. 각 장에서는 프롬프트, 출력 파서, 모델, 메모리 등 다양한 주제를 다루고 있습니다.\\n\\n3. **전자책**: [LangChain 한국어 튜토리얼](https://www.gpters.org/dev/post/langchain-notes-langchain-korean-UbTO5u3bFgmL0SQ)에서는 랭체인 공식 GitHub 튜토리얼을 한국어 예제와 문구로 번역하여 제공하고 있습니다.\\n\\n이 자료들을 통해 랭체인에 대한 기본적인 지식과 활용 방법을 익힐 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 304, 'prompt_tokens': 6246, 'total_tokens': 6550, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b0a6e144-4d4c-4d6c-a008-157a5b5145c2-0', usage_metadata={'input_tokens': 6246, 'output_tokens': 304, 'total_tokens': 6550, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에이전트 실행\n",
    "agent_executor.invoke(\n",
    "    {\"messages\": [(\"user\", \"랭체인 한국어 튜토리얼에 대해서 설명해줘\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상태 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class PlanExecute(TypedDict):\n",
    "    input: Annotated[str, \"User's input\"]\n",
    "    plan: Annotated[List[str], \"Current plan\"]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: Annotated[str, \"Final response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 계획(Planner) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Plan 모델 정의\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
    "\n",
    "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계획 수립을 위한 프롬프트 템플릿 생성\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "Answer in Korean.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "planner 동작을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['LangGraph의 정의와 기능을 이해한다.', 'LangGraph의 핵심 장점을 정리한다.', 'LangGraph의 핵심 단점을 정리한다.', 'LangGraph를 사용하는 이유를 정리한다.'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Planner 실행\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"LangGraph 의 핵심 장단점과 LangGraph 를 사용하는 이유는 무엇인가?\",\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Plan 생성\n",
    "\n",
    "이전 단계의 결과를 바탕으로 계획을 다시 수립하는 단계를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    # 사용자 응답\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    # 수행할 작업: \"Response\", \"Plan\". 사용자에게 응답할 경우 Response 사용, 추가 도구 사용이 필요할 경우 Plan 사용\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 계획을 재수립하기 위한 프롬프트 정의\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\n",
    "Answer in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Replanner 생성\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 정의\n",
    "\n",
    "#### 1. 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# 사용자 입력을 기반으로 계획을 생성하고 반환\n",
    "def plan_step(state: PlanExecute):\n",
    "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    # 생성된 계획의 단계 리스트 반환\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "# 에이전트 실행기를 사용하여 주어진 작업을 수행하고 결과를 반환\n",
    "def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    # 계획을 문자열로 변환하여 각 단계에 번호를 매김\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    # 현재 실행할 작업을 포맷팅하여 에이전트에 전달\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
    "    # 에이전트 실행기를 통해 작업 수행 및 결과 수신\n",
    "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
    "    # 이전 단계와 그 결과를 포함하는 딕셔너리 반환\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "# 이전 단계의 결과를 바탕으로 계획을 업데이트하거나 최종 응답을 반환\n",
    "def replan_step(state: PlanExecute):\n",
    "    output = replanner.invoke(state)\n",
    "    # 응답이 사용자에게 반환될 경우\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    # 추가 단계가 필요할 경우 계획의 단계 리스트 반환\n",
    "    else:\n",
    "        next_plan = output.action.steps\n",
    "        if len(next_plan) == 0:\n",
    "            return {\"response\": \"No more steps needed.\"}\n",
    "        else:\n",
    "            return {\"plan\": next_plan}\n",
    "\n",
    "\n",
    "# 에이전트의 실행 종료 여부를 결정하는 함수\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"final_report\"\n",
    "    else:\n",
    "        return \"execute\"\n",
    "\n",
    "\n",
    "final_report_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
    "Final report should be written in professional tone.\n",
    "\n",
    "Your objective was this:\n",
    "\n",
    "{input}\n",
    "\n",
    "Your previously done steps(question and answer pairs):\n",
    "\n",
    "{past_steps}\n",
    "\n",
    "Generate a final report in markdown format. Write your response in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "final_report = (\n",
    "    final_report_prompt\n",
    "    | ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def generate_final_report(state: PlanExecute):\n",
    "    past_steps = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
    "            for past_step in state[\"past_steps\"]\n",
    "        ]\n",
    "    )\n",
    "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 엣지 설정 및 그래프 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"execute\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.add_node(\"final_report\", generate_final_report)\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"execute\")\n",
    "workflow.add_edge(\"execute\", \"replan\")\n",
    "workflow.add_edge(\"final_report\", END)\n",
    "\n",
    "# 조건부 엣지: replan 후 종료 여부를 결정하는 함수 사용\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mplanner\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAG와 Naive RAG의 정의를 이해한다.\n",
      "Modular RAG의 구조와 작동 방식을 설명한다.\n",
      "Naive RAG의 구조와 작동 방식을 설명한다.\n",
      "Modular RAG와 Naive RAG의 주요 차이점을 비교한다.\n",
      "Modular RAG가 production level에서 사용하는 이점을 정리한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (call_HpjQrkPE57l4zIlBU1xsMrNX)\n",
      " Call ID: call_HpjQrkPE57l4zIlBU1xsMrNX\n",
      "  Args:\n",
      "    query: Modular RAG 정의\n",
      "  tavily_web_search (call_OPnEgdxWxdjJiT0XsGSmTQa0)\n",
      " Call ID: call_OPnEgdxWxdjJiT0XsGSmTQa0\n",
      "  Args:\n",
      "    query: Naive RAG 정의\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"title\": \"Modular RAG\", \"url\": \"https://jjaegii.tistory.com/42\", \"content\": \"Modular RAG Modular RAG Modular RAG 모듈 독립적인 모듈 구성 Modular RAG 동적(dynamic) - 그래프 형식의 흐름 구성, 상황에 따른 분기 처리 모듈 모듈, 하위 모듈 및 운영자로 구성된 아키텍처 설계를 통해 통합되고 구조화된 방식으로 RAG 시스템을 정의 각 모듈은 하위 모듈(Sub Module)로 구성 독립적인 모듈 구성 마치 레고 블록처럼 세부 모듈(Sub-Module)을 연결하여 구성 평가자 -> \\\"문서 검색\\\"에 대한 평가(Score) - 문서에 Query에 대한 답변을 위한 정보가 충분한지 판단 보다 명확한 질문 or Query Decomposition을 통한 세부 질문으로 구성 후 검색 Query -> 문서 검색 -> 답변 생성 -> 평가자 -> \\\"답변\\\"에 대한 평가(Score) - Query에 대한 답변이 충분한지 평가 YES: 문서 검색 -> 답변 생성 modular rag, Rag, Retrieval Augmented Generation AI 관련 글 rocky linux 9 k8s 단일노드 구성 #Rag\", \"score\": 0.8900749, \"raw_content\": \"Modular RAG\\n공부 메모장\\nx\\nclose\\n\\n공부 메모장\\n\\n\\n분류 전체보기 (46)\\n\\n개발 (13)\\nk8s (6)\\nTerraform (4)\\nAI (23)\\n\\n\\n\\nGithub\\n\\n홈\\n태그\\n방명록\\n\\nAI · 2024. 10. 26. fullscreen 넓게보기 fullscreen_exit 원래대로\\nModular RAG\\n목차\\n\\nModular RAG\\n모듈\\n독립적인 모듈 구성\\n조립형 모듈\\n그래프 형식의 동적 흐름 구성\\n출처\\n\\nModular RAG\\n레고와 같이 재구성이 용이하고 보다 유연한 흐름을 만들 수 있는 프레임워크\\n\\n\\n독립적(Independant) - 기능 위주로 세분화\\n유연하고 확장성(flexable & scalable) - 모듈의 추가 및 변경이 쉬움\\n동적(dynamic) - 그래프 형식의 흐름 구성, 상황에 따른 분기 처리\\n\\n\\n모듈\\n모듈, 하위 모듈 및 운영자로 구성된 아키텍처 설계를 통해 통합되고 구조화된 방식으로 RAG 시스템을 정의\\n\\n각 단계(Step)를 모듈(Module) 별로 정의\\n각 모듈은 하위 모듈(Sub Module)로 구성\\n\\n\\n하위 모듈은 플러그인 형식의 독립된 구조\\n독립 구조\\n각 모듈은 조립형으로 설계(레고 블록)\\n\\n\\n병렬 구조\\n여러 하위 모듈을 동시실행 후 병합 가능\\n\\n\\n분기 구조\\n상황에 따른 분기 처리가 가능\\n\\n\\n\\n\\n\\n\\n독립적인 모듈 구성\\n동일한 형태의 입력과 출력을 반환하는 Interface\\n\\n레고 방식의 조립이 가능한 Module 설계\\n내부 구현은 모듈 별 목적에 따라 다르게 구현\\n\\n예)\\n```\\ndef custom_retriever(data: RagData) -> RagData:\\n    # 커스텀 retriever 로직 구현\\n    ...\\n    return data\\ndef request_private_api(data: RagData) -> RagData:\\n    # API 호출 로직 구현\\n    ...\\n    return data\\n```\\n\\n조립형 모듈\\n마치 레고 블록처럼 세부 모듈(Sub-Module)을 연결하여 구성\\n\\n모듈의 추가/변경 용이\\n\\n독립적인 모듈의 장점을 활용\\n\\n각 Sub-Module의 교체가 쉬움\\n단계의 전/후에 Sub-Module을 추가\\n\\n\\n그래프 형식의 동적 흐름 구성\\n평가자 -> \\\"문서 검색\\\"에 대한 평가(Score) - 문서에 Query에 대한 답변을 위한 정보가 충분한지 판단\\n\\n\\nSuccess: 답변 생성\\nFail\\n검색된 문서의 정보가 불충분한 경우\\n문서 검색을 위한 Query Rewrite\\n\\n\\n질문이 모호한 경우\\n보다 명확한 질문 or Query Decomposition을 통한 세부 질문으로 구성 후 검색\\n\\n\\n\\n\\n\\n\\nQuery -> 문서 검색 -> 답변 생성 -> 평가자 -> \\\"답변\\\"에 대한 평가(Score) - Query에 대한 답변이 충분한지 평가\\n\\n\\nSuccess: 답변 생성\\nFail\\n질문이 모호한 경우\\nQuery Rewrite\\n\\n\\n정보가 불충분한 경우\\n추가 검색 모듈(Web Search)을 활용하여 문맥(Context) 보강\\n\\n\\n\\n\\n\\n\\n질문에 문서 검색이 필요한지 판단하여 Routing\\n\\n\\nYES: 문서 검색 -> 답변 생성\\nNO\\n즉각 답변 생성: 토큰 비용의 절약\\n예) \\\"대한민국의 수도는 어디입니까?\\\" -> 즉각 답변\\n\\n\\n평가자 2는 RAG 답변에 대한 평가\\n\\n*평가자 : LLM에게 평가 기준을 프롬프트로 제공하여 구현\\n\\n출처\\n\\nhttps://www.youtube.com/watch?v=aMUopbBrAmA\\nhttps://arxiv.org/pdf/2407.21059\\n\\n공유하기\\n게시글 관리\\n공부 메모장\\n저작자표시\\nmodular rag, Rag, Retrieval Augmented Generation\\nAI 관련 글\\n더 보기\\n\\nLlama 3.2-Vision을 사용한 로컬 OCR 애플리케이션 구축하기 2024.11.24\\nModular RAG Pattern 2024.10.26\\nAdvanced_RAG 2024.10.26\\nNaive RAG 2024.10.25\\n\\n공지사항\\n전체 카테고리\\n\\n분류 전체보기 (46)\\n개발 (13)\\nk8s (6)\\nTerraform (4)\\nAI (23)\\n\\n\\n\\n최근 글\\nRHEL 8.8에서 DVD ISO 파일 마운트 및 로컬 레포지토리 설정\\nDPO(Direct Preference Optimization)와 SFT(Supervise⋯\\n텍스트를 벡터로 변환하는 임베딩 모델의 과정\\nHyDE(Hypothetical Document Embedding)란?\\n다국어 텍스트 임베딩 모델 비교: multilingual-e5-large vs multil⋯\\n인기 글\\n[Rocky Linux 9] /home 용량 줄이고 /root 용량 늘리기\\n LLM을 활용한 GitHub PR 코드 리뷰 자동화\\nssh 서버 pem 키 생성 및 접속\\nrocky linux 9 k8s 단일노드 구성\\nKoAlpaca 데이터셋을 활용한 Llama 3.1 LoRA 기반 PEFT\\n최근 댓글\\n\\nmwollossna 10.27 잘보고가요~! 공감!!!\\n크래프트러버 10.26 귀중한 정보 감사합니다! 👍 제 블로그에서도 비슷한 주제를 다루고 있으니 꼭 들러주세요!\\n탑노트작가 10.24 좋은 하루 보내세요! ☀️\\n머스트바로 10.14 유익한 정보 감사합니다! 😊 덕분에 많이 배웠습니다.\\n소울캐시 10.14 포스팅 잘 보고 가요^^❤️\\n\\n태그\\n\\n#ollama\\n#AWS\\n#rocky linux 9\\n#terraform\\n#k8s\\n#쿠버네티스\\n#테라폼\\n#kubernetes\\n#Retrieval Augmented Generation\\n#Rag\\n\\n전체 방문자\\n오늘 39\\n어제 93\\n전체 8,479\\nCopyright © 쭈미로운 생활 All rights reserved.\\nDesigned by JJuum\\n티스토리툴바\"}, {\"title\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) - 개발세발\", \"url\": \"https://g3lu.tistory.com/42\", \"content\": \"RAG (1) RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. Naive RAG Retrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이는 중요한 정보를 놓칠 수 있게 된다. 잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다.\", \"score\": 0.83093774, \"raw_content\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\n본문 바로가기\\n굴러가는 분석가의 일상\\n카테고리\\n검색하기\\n검색하기 블로그 내 검색 \\n굴러가는 분석가의 일상\\nG3LU\\n\\n\\n분류 전체보기 (42)\\n논문리뷰 (5)\\nDeep Learning 이해 (9)\\nGenerative AI (2)\\nLLM (3)\\nRAG (1)\\nFine-tuning (0)\\nVector DB (0)\\nEtc (0)\\n\\n\\nComputer Vision (9)\\nData Science (3)\\nML (2)\\n구현 (1)\\n\\n\\nAlgorithm (11)\\n개념 (0)\\nSofteeer (3)\\nLeetcode (6)\\nPython (2)\\n\\n\\n\\n\\n\\nGuestbook\\nNotice\\nRecent Posts\\n\\nRAG 성능을 좌지우지 하는 PARSING(파싱)⋯\\n[논문리뷰] RAPTOR (장문의 문서에 적합한 ⋯\\nRAG의 패러다임(Naive RAG, Advanc⋯\\nRetrieval-Augmented Generat⋯\\n\\nRecent Comments\\n\\n행목입니다~!!.공감!\\n수정사항 반영하였습니다! 감사드립니다!\\n글 잘 읽었습니다! 상황에 따라선 원시 입력으로 직접 ⋯\\nimport문 두줄이 빠져있어 WebDriverWai⋯\\n\\nLink\\n«   2024/12   »\\n| 일 | 월 | 화 | 수 | 목 | 금 | 토 |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\\n| 8 | 9 | 10 | 11 | 12 | 13 | 14 |\\n| 15 | 16 | 17 | 18 | 19 | 20 | 21 |\\n| 22 | 23 | 24 | 25 | 26 | 27 | 28 |\\n| 29 | 30 | 31 |  |  |  |  |\\nTags\\n\\ncomputer vision\\nleetcode\\n퍼셉트론\\n손실함수\\nanomaly detection\\nLLaVA\\n데이터 파싱\\n이상탐지\\nvisual instruction tuning\\nTime Series\\nLLM\\n오차역전파\\n활성화함수\\nCross Entropy Error\\n시계열\\nMean squared error\\nsegmentation\\nnlp\\n컴퓨터비전\\n머신러닝\\npdf parsing\\n합성곱 신경망\\nE\\n활성화 함수\\nrag parsing\\nNon-Maximum Suppression\\n딥러닝\\ndeep learning\\nCNN\\n\\nmore\\nArchives\\n\\n2024/11 (1)\\n2024/10 (1)\\n2024/08 (1)\\n2024/07 (1)\\n2024/06 (2)\\n\\nToday\\n18\\nTotal\\n6,101\\n닫기 관리 메뉴\\n\\n글쓰기\\n방명록\\nRSS\\n관리\\n\\n굴러가는 분석가의 일상\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문\\nLLM\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\nG3LU 2024. 8. 3. 18:54\\n오픈AI의 GPT 시리즈, Meta의 LLama 시리즈, Google의 Gemini와 같은 대형 언어 모델(LLM)은 생성 AI분야에서 큰 성과를 이루게 되었다. 하지만 위와 같은 모델들의 문제점은 종종 부정확하거나 관련 없는 정보를 생성하며, 오래된 정보에 의존하고, decision-making 과정이 투명하지 않아 블랙박스 추론을 초래하게 된다.\\nRetrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이를 통해 LLM은 더 정확하고 문맥을 고려할 수 있는 답변을 생성하며, 환각(hallucination)을 방지할 수 있게 되었다. 이러한 장점을 가진 RAG는 2023년 이후 LLM 기반 시스템에서 범용적으로 사용되는 아키텍처로 자리 잡았다. 이에 RAG가 어떻게 발전해 왔는지 알아보도록 하겠다.\\n💡 RAG Framework\\n최근 몇 년 동안 RAG 분야에서는 많은 연구가 이루어졌으며, 크게 RAG는 세 가지의 범주로 나눌 수 있다.\\n\\nNaive RAG\\nAdvanced RAG\\nModular RAG \\n\\n그럼 이 세 가지의 범주에 대해서 아래의 그림을 통해 자세히 알아보도록 하겠다.\\n\\n💡 Naive RAG\\n\\nIndexing : Indexing은 RAG에서 수행되는 초기 단계이다. 이 단계는 Raw Data를 추출하는 것에서 시작하며, PDF, HTML, Word와 같은 다양한 파일 형식을 표준화된 plain text로 변환하게 된다. 대규모 언에 모델(LLM)은 한 번에 처리할 수 있는 텍스트의 최대 길이에 제한이 있다. 대규모 문서나 데이터 셋을 더 효율적으로 관리하기 위해 더 작고 관리 가능한 청크로 나누게 되는데, 이를 Chunking이라고 한다. 그런 다음 Embedding Model를 통해 Chunking된 text들을 벡터로 표현이 된다. 마지막으로 벡터화된 청크는 Vector DB에 키-값 쌍으로 저장된 게 된다. 이러한 Vector DB는 이후의 Retreival 단계에서 efficient 하고 scalable search capabilities 기능을 제공한다. \\nRetrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이를 수행하기 위해 사용자 쿼리는 인코딩 모델에 의해 처리되어 의미적으로 관련된 임베딩을 생성하게 된다. 그런 다음 벡터화된 쿼리는 Vector DB에서 유사성 검색을 기반으로 상위 k개 검색을 수행하여 가장 비슷한 데이터를 찾게 된다. \\nGeneration : 사용자의 Query와 검색된 추가적인 정보는 Prompt에 입력되고 LLM를 거쳐 답변을 생성하게 된다. \\n\\n💡 Naive RAG의 문제점\\n1. Indexing\\n\\n정보 추출의 불안정성 : PDF와 같은 비정형 파일 내 이미지와 표에 있는 유용한 정보를 효과적으로 처리하지 못한다. \\n청킹 방법 : 청킹 과정에서 파일 유형의 특성을 고려하지 않고 \\\"one-size-fits-all\\\" 을 사용하는 것이 대반사이다. 이는 각 청크에 일관성과 불필요한 의미 정보가 포함될 가능성이 크며, 기존 텍스트의 문단 구분과 중요한 세부 사항을 놓치게 된다 \\n비최적화 인덱싱 구조 : 인덱싱 구조가 최적화되지 않아 비효율적인 검색 기능을 초래하게 되며, 이는 검색 속도를 현저하게 느리게 만들고 검색 결과의 정확성을 떨어지게 만든다. \\n임베딩 모델의 의미 표현 능력 : 임베딩 모델이 텍스트의 의미를 제대로 파악하지 못해, 검색된 정보의 관련성이 낮아진다. 이는 중요한 정보를 놓칠 수 있게 된다. \\n\\n2. Retrieval\\n\\n제한된 검색 알고리즘: 키워드, 의미, 벡터 검색을 결합하지 않은 등 다양한 검색이나 알고리즘의 통합이 제한적이며, 이는 검색 결과의 다양성과 정확성을 저하시킨다. \\n쿼리 및 임베딩 모델의 한계 : 쿼리가 부족하거나 임베딩 모델의 의미 표현 성능이 낮아 유용한 정보를 검색하지 못한다.\\n답변 정보 중복 : 여러 검색된 컨텍스트가 유사한 정보를 포함하여 생성된 답변에 반복적인 내용이 포함된다.\\n\\n3. Generation\\n\\n잘못된 응답 생성 : LLM이 관련 없거나 편향된 응답을 생성할 가능성이 높다. \\n잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. \\n\\n\\n💡 Advanced RAG\\nAdvanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 하지만 여기에서 고려해야 할 것은 어떠한 방식으로 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이며, 아래와 같은 사항들을 해결해야 한다.\\n\\n문서와 쿼리의 semantic representation의 정확성을 어떻게 극대화할 수 있을까?\\n쿼리와 문서(청크)의 semantic space를 어떻게 align 시킬수 있을까? \\nRetrieval의 출력을 LLM의 선호도에 맞게 조정할 수 있는 방법이 무엇일까? \\n\\n위의 세 가지 고찰을 보완하기 위해 Pre-Retreival 및 Post-Retrieval를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 이들의 역할에 대해 알아보자.\\n1. Pre-Retrieval\\n주요 목적은 색인 구조와 사용자 쿼리를 개선하는 것이다.\\n\\n데이터 품질 향상 : 엔터티와 용어의 모호성을 제거하고, 사실 정확성을 위해 문맥 유지 및 새로운 정보를 업데이트 한다. \\n인덱스 구조 최적화 : 청크 크기를 최적화하여 문맥을 일정화하고 엔터티 간의 관계를 포착하기 위해 그래프 구조의 정보를 추가한다. \\n메타데이터 추가 : dates, chapters, subsections, purposes 등과 같은 관련 정보를 청크에 메타데이터로 추가하여 데이터 필터링을 개선한다. \\n청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 그런 다음 청크를 임베딩하여 내포하고 있는 의미를 도출한다. \\n\\n2. Retrieval\\n청크 크기가 결정된 후, 임베딩하게 된다. 해당 단계에서는 쿼리와 임베딩된 청크 간의 유사성을 계산하여 가장 관련성 높은 청크를 식별하게된다. 여기서 쿼리와 청크에 사용되는 임베딩 모델을 최적화할 수 있다.\\n\\nDomain Knowledge Fine-Tuning : 임베딩 모델이 각 도메인별 정보를 정확하게 포착할 수 있도록,  도메인 특화 데이터셋을 사용하여 fine-tuning한다. 이를 위한 데이터셋에는 쿼리, 코퍼스 및 관련 문서가 포함되어야 한다. \\nDynamic Embedding : 단어가 등장하는 맥락에 맞춰 임베딩을 fine-tuning하는 방식이다. 이는 각 단어에 대해 하나의 벡터만을 사용하여 각 토큰 당 정해진 임베딩을 리턴하는 방식이 아니라 주변 단어에 따라 맥락을 고려할 수 있는 BERT를 사용하는 것과 같다. \\n\\n3. Post-Retrieval\\n관련된 정보(청크)들을 Vector Database 내에서 검색한 후, 쿼리와 함께 LLM에 입력된다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. 이러한 문제를 극복하기 위해 사용되는 방법에 대해 간단하게 알아보자.\\n\\nReranking : 검색된 정보를 재순위하여 가장 관련성 높은 답변을 우선시한다. LLM에 입력이 추가될 때, 성능이 저하되는 경우가 발생한다. 이에 검색된 청크를 재정렬하고 Top-K 가장 관련성 높은 청크를 식별하여 LLM에 사용할 컨텍스트로 제공한다. \\nPrompt Compression : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.  \\n\\n\\n💡Modular RAG\\n\\b앞서 알아보았던 Advanced RAG는 Naive RAG의 컴포턴트를 조금씩 보완한 형태라면, Modular RAG는 조금 더 나아가 다양하고 유연한 구조를 지향한다. 이러한 구조는 RAG의 전반적인 성능을 향상 시켰으며, 현재 어플리케이션을 구축할 때 표준 패러다임이 되었다. 그럼 몇 가지 모듈에 대해 알아보도록 하자.\\n\\nSearch Module : 임베딩 유사도 기반 검색 외에도 추가적인 검색 시나리오를 가능하게 한다. 즉 Search Module은 특정 시나리오에 맞춰 LLM이 생성한 코드나 SQL 등을 사용하여 검색을 수행하는 모듈이다. 이에 다양한 데이터 소스를 사용할 수 있다는 점이다.\\nMemory Module : LLM이 벡터 데이터베이스에서 검색된 청크뿐만 아니라 시스템 메모리에 저장된 이전 쿼리와 결합하여 현재 입력과 가장 유사한 답변을 찾는 모듈이다. \\nFusion Module : 유저의 의도를 정확하게 반영하지 않을 수도 있다는 차관에서 비롯되었다. LLM을 통해 유저의 쿼리로 부터 여러 개의 가상 쿼리를 생성하여 검색하는 방식이다. \\n\\n\\nFusion Model (RAG-Fusion)\\n위의 모듈뿐만 아니라 다양한 모듈이 존재하니, 이에 대해 궁금하시다면 링크 참고하시길 바랍니다.\\n공유하기\\n게시글 관리\\n굴러가는 분석가의 일상\\n'LLM' 카테고리의 다른 글\\nRetrieval-Augmented Generation 이란?  (0)2024.07.21\\n'LLM' Related Articles\\n\\nRetrieval-Augmented Generation 이란? 2024.07.21\\n\\nmore\\n\\nBlog is powered by kakao / Designed by Tistory\\n티스토리툴바\\n굴러가는 분석가의 일상구독하기\"}, {\"title\": \"[Modular RAG]구조 | Syshin\", \"url\": \"https://syshin0116.github.io/posts/Modular-RAG-구조/\", \"content\": \"Modular RAG는 시스템의 유연성과 확장성을 크게 향상시킬 수 있어, 이를 통해 다양한 애플리케이션 요구 사항에 맞게 모듈을 조합하고 최적화하는 방향으로 발전시키고자 한다 ... ├── langgraph/ # LangGraph 관련 로직 폴더 │ ├── node_definitions.py # 노드 정의 (LLM\", \"score\": 0.7992077, \"raw_content\": \"[Modular RAG]구조\\nNaive RAG에 여러 RAG를 하기 위해 Advanced RAG를 충분히 다뤘다고 생각한다. 하지만, 점점 더 복잡한 작업을 최소화하고 효율성을 높이기 위해 Modular RAG 방식을 적용하고자 한다. 마치 LEGO 블록처럼 모듈들을 쌓아가며 나만의 재사용 가능한 RAG 자산을 구축하는 것이 목표다. Modular RAG는 시스템의 유연성과 확장성을 크게 향상시킬 수 있어, 이를 통해 다양한 애플리케이션 요구 사항에 맞게 모듈을 조합하고 최적화하는 방향으로 발전시키고자 한다\\nModular RAG 논문: https://arxiv.org/html/2407.21059v1\\n\\n\\n\\n기술 스택\\n이 프로젝트에서는 다양한 기술을 활용하여 모듈화된 RAG 시스템을 구축함. 주요 기술 스택은 다음과 같음:\\n구조\\n현재:\\n구조 요약\\nservices/retrievals\\n검색 관련 로직을 단계별로 나눔. RAG 방법론에 따라 pre_retrieval, retrieval, post_retrieval로 나누어 각 단계를 처리할 예정\\nservices/vectorstores\\n여러 벡터 스토어(Chroma, Faiss, Milvus 등)를 사용할 수 있도록 구성할 예정\\nservices/preprocessing\\n파싱된 데이터를 벡터 스토어에 넣기 전 전처리 과정 담당. 데이터 클리닝과 변환 등 작업 포함\\nservices/websearch\\n웹 검색 관련 모듈을 구성. 다양한 웹 검색 소스를 처리할 수 있도록 설계\\nservices/prompt_manager\\n프롬프트를 저장하고 관리하는 공간\\nservices/parsers\\nLlama parser와 파일 확장자별 파서들을 정의하여 다양한 파일 형식을 처리\\nservices/llm_handler\\n공개 및 비공개 LLM 호출 함수들을 정의하고 관리\\nlanggraph/node_definitions\\nLLM, 리트리버, 벡터 스토어 등 주요 기능을 각각 노드로 정의. 노드는 독립적으로 동작하며, 다양한 워크플로우에서 유연하게 재사용 가능\\nlanggraph/langgraph_handler\\nLangGraph를 관리하고, node_definitions에서 정의된 노드들을 불러와 엣지로 연결해 워크플로우를 구성. 사용자의 요구에 맞춰 동적으로 노드를 선택하거나 연결할 수 있도록 구조화\\nFurther Reading\\n[Modular RAG]RDBMS vs NoSQL\\n RDBMS와 NoSQL의 차이점 및 Modular RAG에 적합한 선택 Modular RAG는 다양한 모듈을 조합하며 유연성과 확장성을 중요하게 여기는 프로젝트다. 이를 구현하기 위한 데이터베이스 선택에서 RDBMS와 NoSQL의 특성을 비교하고, Modular RAG에 더 적합한 데이터베이스를 선택하고자 한다. RDBMS vs NoSQL: 비...\\n[Modular RAG] Short Term, Long Term Memory\\n Intro Memory 기능을 구현하는데 어떻게 하면 챗봇이 사람처럼 기억을 할 수 있을까, 어떻게 구현하면 가장 도움이 될까 고민했다. 고민 결과, 각 채팅방에 중요 정보는 Long Term memory로, 최신 정보는 Shot Term memory로 구현하고자 한다. 정보 기본 Memory의 유형 단기 기억 (Short-term memory) ...\\n[Modular RAG]Chat History\\n 효율적인 Chat History 저장과 확장 가능한 구조 설계 챗봇을 설계하다, 기본적인 채팅 기록 저장 방식에 대해 고민하게 되었다. 일반적으로 채팅 기록을 저장하는 방법은 Chat Session과 Chat Messages로 나눌 수 있다. 기본적인 저장 방식에 추가로 고려한 기능, 이를 구현하기 위한 구조, 그리고 일반적인 챗봇과 카카오톡 봇 모...\\nGCP VSCODE SSH 연결 방법\\n[정리]Github CI/CD - GCP\\n © 2025 syshin. Some rights reserved.\\nUsing the Jekyll theme Chirpy.\\nA new version of content is available.\\n\"}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"title\": \"Naive RAG vs. Advanced RAG. In the modern world, large ... - Medium\", \"url\": \"https://medium.com/@myscale/naive-rag-vs-advanced-rag-17b38cda44c1\", \"content\": \"RAG is a strategic enhancement designed to elevate the performance of LLMs. By incorporating a step that retrieves information during text generation, RAG ensures that the model’s responses are accurate and up-to-date. Naive RAG: This is the most basic version, where the system simply retrieves the relevant information from a knowledge base and directly gives it to the LLM to generate the response. By understanding the flow of data from the user’s query to the final response, we can appreciate how each component of the Naive RAG system plays a pivotal role in ensuring that the user receives accurate and relevant information. Unlike Naive RAG, which directly incorporates retrieved information, Advanced RAG involves additional processing steps that optimize the relevance, and overall quality of the response.\", \"score\": 0.8390101, \"raw_content\": \"Naive RAG vs. Advanced RAG. In the modern world, large language… | by MyScale | Oct, 2024 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nNaive RAG vs. Advanced RAG\\n\\nMyScale\\n·Follow\\n10 min read\\n·\\nOct 30, 2024\\n\\n3\\n\\nListen\\nShare\\nIn the modern world, large language models (LLMs) have transformed the world by their impressive capacity to produce texts that imitate human-written text. These models are highly skilled in tasks like creating new content and giving smart responses, pushing the AI field further. They are trained on large amounts of data, but they only know what’s in that data, making it hard for them to provide the latest information. This can lead to outdated answers or wrong information, known as information hallucinations. To address these issues, a dynamic framework called Retrieval-Augmented Generation (RAG) has been developed. It combines the strengths of traditional LLMs with retrieval systems, extending the use cases of these models.\\nWhat is RAG\\nRAG is a strategic enhancement designed to elevate the performance of LLMs. By incorporating a step that retrieves information during text generation, RAG ensures that the model’s responses are accurate and up-to-date. RAG has evolved significantly, leading to the development of two main modes:\\n\\nNaive RAG: This is the most basic version, where the system simply retrieves the relevant information from a knowledge base and directly gives it to the LLM to generate the response.\\nAdvanced RAG: This version goes a step further. It adds additional processing steps before and after retrieval to refine the retrieved information. These steps enhance the quality and accuracy of the generated response, ensuring it integrates seamlessly with the model’s output.\\n\\nNaive RAG\\nThe Naive RAG is the simplest version of the RAG ecosystem, offering a straightforward method for combining retrieval data with LLM models to efficiently respond to users.\\n\\nA basic system has the following components:\\n1. Document Chunking:\\nThe process begins with documents being split into smaller chunks. This is essential because smaller chunks are easier to manage and process. For instance, when you have a long document, it’s broken down into segments, making it easier for the system to retrieve relevant information later.\\n2. Embedding Model:\\nThe embedding model is a critical part of the RAG system. It converts both the document chunks and the user query into numerical form, often called embeddings. This conversion is necessary because computers understand numerical data better. The embedding model uses advanced machine learning techniques to represent the meaning of text in a mathematical way. For example, when a user asks a question, the model transforms this question into a set of numbers that capture the semantics of the query.\\n\\n3. Vector Database (like MyScaleDB):\\nOnce the document chunks are converted into embeddings, they are stored in a vector database like MyScaleDB. The vector databases are designed to efficiently store and retrieve these embeddings. When a user submits a query, the system uses the vector database to find the most relevant document chunks by comparing the embeddings of the query with those stored in the database. This comparison helps in identifying the chunks that are most similar to what the user is asking.\\n4. Retrieval:\\nAfter the vector database identifies the relevant document chunks, they are retrieved. This retrieval process is crucial because it narrows down the information that will be used to generate the final response. Essentially, it acts as a filter, ensuring that only the most relevant data is passed on to the next stage.\\n5. LLM (Large Language Model):\\nThe LLM takes over once the relevant chunks are retrieved. Its job is to understand the retrieved information and generate a coherent response to the user’s query. The LLM uses the user query and the retrieved chunks to provide a response that is not only relevant but also contextually appropriate. This model is responsible for interpreting the data and formulating a response in natural language that the user can easily understand.\\n\\n6. Response Generation:\\nFinally, the system generates a response based on the information processed by the LLM. This response is then delivered back to the user, providing them with the information they were seeking in a clear and concise manner.\\nBy understanding the flow of data from the user’s query to the final response, we can appreciate how each component of the Naive RAG system plays a pivotal role in ensuring that the user receives accurate and relevant information.\\nAdvantages\\n\\nSimplicity of Implementation: RAG is straightforward to set up as it directly integrates retrieval with generation, reducing the complexity involved in enhancing language models without needing intricate modifications or additional components.\\nNo Need for Fine-Tuning: One of the significant advantages of RAG is that it doesn’t require fine-tuning of the LLM. This not only saves time and reduces operational costs but also allows for faster deployment of RAG systems.\\nEnhanced Accuracy: By leveraging external, up-to-date information, Naive RAG significantly improves the accuracy of generated responses. This ensures that the outputs are not only relevant but also reflect the latest data available.\\nReduced Hallucinations: RAG mitigates the common issue of LLMs generating incorrect or fabricated information by grounding responses in real, factual data retrieved during the process.\\nScalability and Flexibility: The simplicity of Naive RAG makes it easier to scale across different applications, as it can be adapted without significant changes to existing retrieval or generative components. This flexibility allows it to be deployed across various domains with minimal customization.\\n\\nDrawbacks\\n\\nLimited Processing: The retrieved information is used directly, without further processing or refinement, which might lead to coherence issues in the generated responses.\\nDependency on Retrieval Quality: The quality of the final output heavily depends on the retrieval module’s ability to find the most relevant information. Poor retrieval can lead to less accurate or relevant responses.\\nScalability Issues: As the dataset grows, the retrieval process may become slower, affecting the overall performance and response time.\\nContext Limitations: Naive RAG may struggle to understand the broader context of a query, leading to responses that, while accurate, might not fully align with the user’s intent.\\n\\nBy examining these advantages and drawbacks, we can comprehensively understand where Naive RAG excels and where it might face challenges. This will pave the way for improvements and create the opportunity to develop advanced RAG.\\nAdvanced RAG\\nBuilding upon the foundation of Naive RAG, Advanced RAG introduces a layer of sophistication to the process. Unlike Naive RAG, which directly incorporates retrieved information, Advanced RAG involves additional processing steps that optimize the relevance, and overall quality of the response.\\n\\nLet’s understand how it works:\\nPre-Retrieval Optimizations\\nIn Advanced RAG, the retrieval process is refined even before the actual retrieval takes place. Here’s what happens in this phase:\\nIndexing Improvements\\nIndexing methods play a vital role in efficiently organizing and retrieving data in databases. Traditional indexing methods, such as B-Trees and Hash indexing, have been widely used for this purpose. However, the search speed of these algorithms decreases as the data size increases. Therefore, we need more efficient indexing methods for larger datasets. MyScale’s MSTG (Multi-Strategy Tree-Graph) indexing algorithm is a prime example of such an advancement. This algorithm outperforms other indexing methods in terms of speed and performance.\\n\\nMSTG merges the strengths of both hierarchical graph and tree structures. Typically, graph algorithms are faster for unfiltered searches but may not be efficient for filtered searches. On the other hand, tree algorithms excel in filtered searches but are slower for unfiltered ones. By combining these two approaches, MSTG ensures high performance and accuracy for both unfiltered and filtered searches, making it a robust choice for a variety of search scenarios.\\nQuery Re-writing\\nBefore the retrieval process begins, the original user query undergoes several enhancements to improve its accuracy and relevance. This step ensures that the retrieval system fetches the most pertinent information. Techniques such as query rewriting, expansion, and transformation are employed here. For instance, if a user’s query is too broad, query rewriting can refine it by adding more context or specific terms, while query expansion might add synonyms or related terms to capture a wider range of relevant documents.\\nDynamic Embeddings\\nIn Naive RAG, a single embedding model might be used for all types of data, which can lead to inefficiencies. Advanced RAG, however, fine-tunes and adjusts embeddings based on the specific task or domain. This means that the embedding model is trained or adapted to better capture the contextual understanding required for a particular type of query or dataset.\\nBy using dynamic embeddings, the system becomes more efficient and accurate, as the embeddings are more closely aligned with the nuances of the specific task at hand.\\nHybrid Search\\nAdvanced RAG also leverages a hybrid search approach, which combines different search strategies to enhance retrieval performance. This might include keyword-based searches, semantic searches, and neural searches. For instance, MyScaleDB supports filtered vector search and full-text search, allowing the use of complex SQL queries due to its SQL-friendly syntax. This hybrid approach ensures that the system can retrieve information with a high degree of relevance, regardless of the nature of the query.\\nPost-Retrieval Processing\\nAfter the retrieval process, Advanced RAG doesn’t stop there. It further processes the retrieved data to ensure the highest quality and relevance in the final output.\\nRe-ranking\\nAfter the retrieval process, Advanced RAG takes an extra step to refine the information. This step, known as re-ranking, ensures that the most relevant and useful data is prioritized. Initially, the system retrieves several pieces of information that might be related to the user’s query. However, not all of this information is equally valuable. Re-ranking helps in sorting this data based on additional factors like how closely it matches the query and how well it fits the context.\\n\\nBy reassessing the retrieved content, re-ranking places the most relevant pieces at the top. This ensures that the generated response is not only accurate but also coherent and directly addresses the user’s needs. The process uses various criteria, such as semantic relevance and contextual appropriateness, to reorder the information. This refinement leads to a final response that is more focused and precise, enhancing the overall quality of the output.\\nContext Compression\\nAfter filtering the relevant documents, even with the use of a re-ranking algorithm, there may still be irrelevant data within those filtered documents for answering the user’s query. The process of eliminating or removing this extraneous data is what we refer to as contextual compression. This step is applied right before passing the relevant documents to the LLM, ensuring that the LLM only receives the most relevant information, enabling it to deliver the best possible results.\\nAdvantages\\nTo better understand the differences between these two approaches, let’s explore the specific advantages that advanced RAG offers over naive RAG.\\n\\nBetter Relevance with Re-ranking: Re-ranking makes sure the most relevant information comes first, improving both the accuracy and flow of the final response.\\nDynamic Embeddings for Better Context: Dynamic embeddings are customized for specific tasks, helping the system understand and respond more accurately to different queries.\\nMore Accurate Retrieval with Hybrid Search: Hybrid search uses multiple strategies to find data more effectively, ensuring higher relevance and precision in the results.\\nEfficient Responses with Context Compression: Context compression removes unnecessary details, making the process faster and leading to more focused, high-quality answers.\\nEnhanced User Query Understanding: By rewriting and expanding queries before retrieval, advanced RAG ensures that user queries are fully understood, leading to more accurate and relevant results.\\n\\nAdvanced RAG marks a major improvement in the quality of responses generated by language models. By adding a refinement stage, it effectively tackles key issues found in Naive RAG, such as coherence and relevance.\\nComparative Analysis: Naive RAG vs. Advanced RAG\\nBy comparing Naive RAG and Advanced RAG, we can observe how Advanced RAG expands on the basic framework of naive RAG. It introduces key improvements that enhance accuracy, efficiency, and the overall quality of retrieval.\\n\\nConclusion\\nWhen choosing between Naive RAG and Advanced RAG, consider the specific needs of your application. Naive RAG is ideal for simpler use cases where speed and straightforward implementation are priorities. It enhances LLM performance in scenarios where deep contextual understanding isn’t critical. On the other hand, Advanced RAG is better suited for more complex applications, offering improved accuracy and coherence through additional processing steps like refined filtering and re-ranking, making it the preferred choice for handling larger datasets and complex queries.\\nMyScale further elevates these advancements by offering scalable and efficient retrieval solutions. Its sophisticated indexing and data handling techniques improve both the speed and accuracy of information retrieval, supporting the enhanced performance of RAG systems. By leveraging MyScale, developers can optimize their use of advanced RAG methods, driving improvements in AI systems and their ability to provide precise and relevant information.\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to audio narrations\\nRead offline with the Medium app\\nTry for $5/month\\nRetrieval Augmented Gen\\nVector Database\\nVector Search\\nComparison\\nSome rights reserved\\n\\n3\\n\\n3\\n\\n\\n\\nFollow\\nWritten by MyScale ------------------\\n150 Followers\\n·20 Following\\nAn integrated vector database, combining the power of SQL and vector for your AI data workloads. https://myscale.com/ #VectorSearch #MachineLearning #LLM\\nFollow\\nNo responses yet\\n\\nWhat are your thoughts?\\nCancel\\nRespond\\nRespond\\nAlso publish to my profile\\nMore from MyScale\\n\\n\\nMyScale\\nUnderstanding Vector Indexing: A Comprehensive Guide ---------------------------------------------------- ### In this blog, we’ll cover the basics of vector indexing and how it is implemented using different techniques.\\nFeb 13\\n55\\n\\n\\n\\nMyScale\\nEnhancing Advanced RAG Systems Using Reranking with LangChain ------------------------------------------------------------- ### Understanding LangChain and Its Impact\\nJun 7\\n6\\n\\n\\n\\nMyScale\\nHow to Build a RAG-Powered Chatbot with Google Gemini and MyScaleDB ------------------------------------------------------------------- ### In the ever-evolving landscape of artificial intelligence, the quest for more intelligent, responsive, and context-aware chatbots has led…\\nJul 19\\n80\\n\\n\\n\\nMyScale\\nHow to Summarize Large Documents with LangChain and OpenAI ---------------------------------------------------------- ### Large language models have made many tasks easier like making chatbots, language translation, text summarization, etc. We used to write…\\nMay 24\\n84 1\\n\\nSee all from MyScale\\nRecommended from Medium\\n\\n\\nIn\\nGenerative AI\\nby\\nShrinivasan Sankar\\nRAG — 7 indexing methods for Vector DBs + Similarity search ----------------------------------------------------------- ### A complete guide to understanding Vector DBs used in Retrieval Augmented Generation pipelines\\nDec 6\\n50\\n\\n\\n\\nIn\\nTowards AI\\nby\\nMdabdullahalhasib\\nA Complete Guide to Embedding For NLP & Generative AI/LLM --------------------------------------------------------- ### Understand the concept of vector embedding, why it is needed, and implementation with LangChain.\\nOct 18\\n146\\n\\nLists\\n   Natural Language Processing --------------------------- 1856 stories·1484 saves\\n\\n\\nIn\\nTowards Data Science\\nby\\nWenqi Glantz\\n12 RAG Pain Points and Proposed Solutions ----------------------------------------- ### Solving the core challenges of Retrieval-Augmented Generation\\nJan 30\\n2.4K 16\\n\\n\\n\\nSamar Singh\\nMastering RAG: Advanced Methods to Enhance Retrieval-Augmented Generation ------------------------------------------------------------------------- ### RAG (Retrieval Augmentation Generation) is technique which gives llm the external knowledge or data,the data on which llm has not been…\\nJun 17\\n109 1\\n\\n\\n\\nSougat Dey\\nEnd-to-end Implementation of a RAG Pipeline using LangChain v0.3 ---------------------------------------------------------------- ### In the tech world, it's clear that Generative AI is the current hot topic. Staying ahead in this fiercely competitive job market means…\\nOct 29\\n1\\n\\n\\n\\nTamanna\\nHow Contextual Retrieval and Hybrid Search Enhance Retrieval-Augmented Generation (RAG) --------------------------------------------------------------------------------------- ### 1. Understanding Retrieval-Augmented Generation (RAG)\\nOct 9\\n197 4\\n\\nSee more recommendations\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\"}, {\"title\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) - 개발세발\", \"url\": \"https://g3lu.tistory.com/42\", \"content\": \"RAG (1) RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. Naive RAG Retrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이는 중요한 정보를 놓칠 수 있게 된다. 잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다.\", \"score\": 0.8266142, \"raw_content\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\n본문 바로가기\\n굴러가는 분석가의 일상\\n카테고리\\n검색하기\\n검색하기 블로그 내 검색 \\n굴러가는 분석가의 일상\\nG3LU\\n\\n\\n분류 전체보기 (42)\\n논문리뷰 (5)\\nDeep Learning 이해 (9)\\nGenerative AI (2)\\nLLM (3)\\nRAG (1)\\nFine-tuning (0)\\nVector DB (0)\\nEtc (0)\\n\\n\\nComputer Vision (9)\\nData Science (3)\\nML (2)\\n구현 (1)\\n\\n\\nAlgorithm (11)\\n개념 (0)\\nSofteeer (3)\\nLeetcode (6)\\nPython (2)\\n\\n\\n\\n\\n\\nGuestbook\\nNotice\\nRecent Posts\\n\\nRAG 성능을 좌지우지 하는 PARSING(파싱)⋯\\n[논문리뷰] RAPTOR (장문의 문서에 적합한 ⋯\\nRAG의 패러다임(Naive RAG, Advanc⋯\\nRetrieval-Augmented Generat⋯\\n\\nRecent Comments\\n\\n행목입니다~!!.공감!\\n수정사항 반영하였습니다! 감사드립니다!\\n글 잘 읽었습니다! 상황에 따라선 원시 입력으로 직접 ⋯\\nimport문 두줄이 빠져있어 WebDriverWai⋯\\n\\nLink\\n«   2024/12   »\\n| 일 | 월 | 화 | 수 | 목 | 금 | 토 |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\\n| 8 | 9 | 10 | 11 | 12 | 13 | 14 |\\n| 15 | 16 | 17 | 18 | 19 | 20 | 21 |\\n| 22 | 23 | 24 | 25 | 26 | 27 | 28 |\\n| 29 | 30 | 31 |  |  |  |  |\\nTags\\n\\ncomputer vision\\nleetcode\\n퍼셉트론\\n손실함수\\nanomaly detection\\nLLaVA\\n데이터 파싱\\n이상탐지\\nvisual instruction tuning\\nTime Series\\nLLM\\n오차역전파\\n활성화함수\\nCross Entropy Error\\n시계열\\nMean squared error\\nsegmentation\\nnlp\\n컴퓨터비전\\n머신러닝\\npdf parsing\\n합성곱 신경망\\nE\\n활성화 함수\\nrag parsing\\nNon-Maximum Suppression\\n딥러닝\\ndeep learning\\nCNN\\n\\nmore\\nArchives\\n\\n2024/11 (1)\\n2024/10 (1)\\n2024/08 (1)\\n2024/07 (1)\\n2024/06 (2)\\n\\nToday\\n18\\nTotal\\n6,101\\n닫기 관리 메뉴\\n\\n글쓰기\\n방명록\\nRSS\\n관리\\n\\n굴러가는 분석가의 일상\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문\\nLLM\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\nG3LU 2024. 8. 3. 18:54\\n오픈AI의 GPT 시리즈, Meta의 LLama 시리즈, Google의 Gemini와 같은 대형 언어 모델(LLM)은 생성 AI분야에서 큰 성과를 이루게 되었다. 하지만 위와 같은 모델들의 문제점은 종종 부정확하거나 관련 없는 정보를 생성하며, 오래된 정보에 의존하고, decision-making 과정이 투명하지 않아 블랙박스 추론을 초래하게 된다.\\nRetrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이를 통해 LLM은 더 정확하고 문맥을 고려할 수 있는 답변을 생성하며, 환각(hallucination)을 방지할 수 있게 되었다. 이러한 장점을 가진 RAG는 2023년 이후 LLM 기반 시스템에서 범용적으로 사용되는 아키텍처로 자리 잡았다. 이에 RAG가 어떻게 발전해 왔는지 알아보도록 하겠다.\\n💡 RAG Framework\\n최근 몇 년 동안 RAG 분야에서는 많은 연구가 이루어졌으며, 크게 RAG는 세 가지의 범주로 나눌 수 있다.\\n\\nNaive RAG\\nAdvanced RAG\\nModular RAG \\n\\n그럼 이 세 가지의 범주에 대해서 아래의 그림을 통해 자세히 알아보도록 하겠다.\\n\\n💡 Naive RAG\\n\\nIndexing : Indexing은 RAG에서 수행되는 초기 단계이다. 이 단계는 Raw Data를 추출하는 것에서 시작하며, PDF, HTML, Word와 같은 다양한 파일 형식을 표준화된 plain text로 변환하게 된다. 대규모 언에 모델(LLM)은 한 번에 처리할 수 있는 텍스트의 최대 길이에 제한이 있다. 대규모 문서나 데이터 셋을 더 효율적으로 관리하기 위해 더 작고 관리 가능한 청크로 나누게 되는데, 이를 Chunking이라고 한다. 그런 다음 Embedding Model를 통해 Chunking된 text들을 벡터로 표현이 된다. 마지막으로 벡터화된 청크는 Vector DB에 키-값 쌍으로 저장된 게 된다. 이러한 Vector DB는 이후의 Retreival 단계에서 efficient 하고 scalable search capabilities 기능을 제공한다. \\nRetrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이를 수행하기 위해 사용자 쿼리는 인코딩 모델에 의해 처리되어 의미적으로 관련된 임베딩을 생성하게 된다. 그런 다음 벡터화된 쿼리는 Vector DB에서 유사성 검색을 기반으로 상위 k개 검색을 수행하여 가장 비슷한 데이터를 찾게 된다. \\nGeneration : 사용자의 Query와 검색된 추가적인 정보는 Prompt에 입력되고 LLM를 거쳐 답변을 생성하게 된다. \\n\\n💡 Naive RAG의 문제점\\n1. Indexing\\n\\n정보 추출의 불안정성 : PDF와 같은 비정형 파일 내 이미지와 표에 있는 유용한 정보를 효과적으로 처리하지 못한다. \\n청킹 방법 : 청킹 과정에서 파일 유형의 특성을 고려하지 않고 \\\"one-size-fits-all\\\" 을 사용하는 것이 대반사이다. 이는 각 청크에 일관성과 불필요한 의미 정보가 포함될 가능성이 크며, 기존 텍스트의 문단 구분과 중요한 세부 사항을 놓치게 된다 \\n비최적화 인덱싱 구조 : 인덱싱 구조가 최적화되지 않아 비효율적인 검색 기능을 초래하게 되며, 이는 검색 속도를 현저하게 느리게 만들고 검색 결과의 정확성을 떨어지게 만든다. \\n임베딩 모델의 의미 표현 능력 : 임베딩 모델이 텍스트의 의미를 제대로 파악하지 못해, 검색된 정보의 관련성이 낮아진다. 이는 중요한 정보를 놓칠 수 있게 된다. \\n\\n2. Retrieval\\n\\n제한된 검색 알고리즘: 키워드, 의미, 벡터 검색을 결합하지 않은 등 다양한 검색이나 알고리즘의 통합이 제한적이며, 이는 검색 결과의 다양성과 정확성을 저하시킨다. \\n쿼리 및 임베딩 모델의 한계 : 쿼리가 부족하거나 임베딩 모델의 의미 표현 성능이 낮아 유용한 정보를 검색하지 못한다.\\n답변 정보 중복 : 여러 검색된 컨텍스트가 유사한 정보를 포함하여 생성된 답변에 반복적인 내용이 포함된다.\\n\\n3. Generation\\n\\n잘못된 응답 생성 : LLM이 관련 없거나 편향된 응답을 생성할 가능성이 높다. \\n잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. \\n\\n\\n💡 Advanced RAG\\nAdvanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 하지만 여기에서 고려해야 할 것은 어떠한 방식으로 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이며, 아래와 같은 사항들을 해결해야 한다.\\n\\n문서와 쿼리의 semantic representation의 정확성을 어떻게 극대화할 수 있을까?\\n쿼리와 문서(청크)의 semantic space를 어떻게 align 시킬수 있을까? \\nRetrieval의 출력을 LLM의 선호도에 맞게 조정할 수 있는 방법이 무엇일까? \\n\\n위의 세 가지 고찰을 보완하기 위해 Pre-Retreival 및 Post-Retrieval를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 이들의 역할에 대해 알아보자.\\n1. Pre-Retrieval\\n주요 목적은 색인 구조와 사용자 쿼리를 개선하는 것이다.\\n\\n데이터 품질 향상 : 엔터티와 용어의 모호성을 제거하고, 사실 정확성을 위해 문맥 유지 및 새로운 정보를 업데이트 한다. \\n인덱스 구조 최적화 : 청크 크기를 최적화하여 문맥을 일정화하고 엔터티 간의 관계를 포착하기 위해 그래프 구조의 정보를 추가한다. \\n메타데이터 추가 : dates, chapters, subsections, purposes 등과 같은 관련 정보를 청크에 메타데이터로 추가하여 데이터 필터링을 개선한다. \\n청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 그런 다음 청크를 임베딩하여 내포하고 있는 의미를 도출한다. \\n\\n2. Retrieval\\n청크 크기가 결정된 후, 임베딩하게 된다. 해당 단계에서는 쿼리와 임베딩된 청크 간의 유사성을 계산하여 가장 관련성 높은 청크를 식별하게된다. 여기서 쿼리와 청크에 사용되는 임베딩 모델을 최적화할 수 있다.\\n\\nDomain Knowledge Fine-Tuning : 임베딩 모델이 각 도메인별 정보를 정확하게 포착할 수 있도록,  도메인 특화 데이터셋을 사용하여 fine-tuning한다. 이를 위한 데이터셋에는 쿼리, 코퍼스 및 관련 문서가 포함되어야 한다. \\nDynamic Embedding : 단어가 등장하는 맥락에 맞춰 임베딩을 fine-tuning하는 방식이다. 이는 각 단어에 대해 하나의 벡터만을 사용하여 각 토큰 당 정해진 임베딩을 리턴하는 방식이 아니라 주변 단어에 따라 맥락을 고려할 수 있는 BERT를 사용하는 것과 같다. \\n\\n3. Post-Retrieval\\n관련된 정보(청크)들을 Vector Database 내에서 검색한 후, 쿼리와 함께 LLM에 입력된다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. 이러한 문제를 극복하기 위해 사용되는 방법에 대해 간단하게 알아보자.\\n\\nReranking : 검색된 정보를 재순위하여 가장 관련성 높은 답변을 우선시한다. LLM에 입력이 추가될 때, 성능이 저하되는 경우가 발생한다. 이에 검색된 청크를 재정렬하고 Top-K 가장 관련성 높은 청크를 식별하여 LLM에 사용할 컨텍스트로 제공한다. \\nPrompt Compression : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.  \\n\\n\\n💡Modular RAG\\n\\b앞서 알아보았던 Advanced RAG는 Naive RAG의 컴포턴트를 조금씩 보완한 형태라면, Modular RAG는 조금 더 나아가 다양하고 유연한 구조를 지향한다. 이러한 구조는 RAG의 전반적인 성능을 향상 시켰으며, 현재 어플리케이션을 구축할 때 표준 패러다임이 되었다. 그럼 몇 가지 모듈에 대해 알아보도록 하자.\\n\\nSearch Module : 임베딩 유사도 기반 검색 외에도 추가적인 검색 시나리오를 가능하게 한다. 즉 Search Module은 특정 시나리오에 맞춰 LLM이 생성한 코드나 SQL 등을 사용하여 검색을 수행하는 모듈이다. 이에 다양한 데이터 소스를 사용할 수 있다는 점이다.\\nMemory Module : LLM이 벡터 데이터베이스에서 검색된 청크뿐만 아니라 시스템 메모리에 저장된 이전 쿼리와 결합하여 현재 입력과 가장 유사한 답변을 찾는 모듈이다. \\nFusion Module : 유저의 의도를 정확하게 반영하지 않을 수도 있다는 차관에서 비롯되었다. LLM을 통해 유저의 쿼리로 부터 여러 개의 가상 쿼리를 생성하여 검색하는 방식이다. \\n\\n\\nFusion Model (RAG-Fusion)\\n위의 모듈뿐만 아니라 다양한 모듈이 존재하니, 이에 대해 궁금하시다면 링크 참고하시길 바랍니다.\\n공유하기\\n게시글 관리\\n굴러가는 분석가의 일상\\n'LLM' 카테고리의 다른 글\\nRetrieval-Augmented Generation 이란?  (0)2024.07.21\\n'LLM' Related Articles\\n\\nRetrieval-Augmented Generation 이란? 2024.07.21\\n\\nmore\\n\\nBlog is powered by kakao / Designed by Tistory\\n티스토리툴바\\n굴러가는 분석가의 일상구독하기\"}, {\"title\": \"Naive RAG, Advanced RAG, Modular RAG : 네이버 ... - 네이버 블로그\", \"url\": \"https://blog.naver.com/PostView.naver?blogId=jangjw01088&logNo=223650933570\", \"content\": \"Naive RAG는 단순하고 직관적이지만, 검색 정확성이나 응답의 관련성에서 한계가 있습니다. Advanced RAG는 이러한 한계를 해결하기 위해 검색과 정보 처리에서 더 복잡한 최적화 기법을 사용합니다. Modular RAG는 다양한 모듈을 결합하여 여러 데이터 소스와 기술을\", \"score\": 0.7969415, \"raw_content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n인공지능 이야기\\n블로그\\n\\nNaive RAG, Advanced RAG, Modular RAG\\nRAG\\nRAG는 Retrieval-Augmented Generation 로 검색 증강 생성 입니다.\\n사용자 질문에 대해 답변을 생성하는 전제척인 프로세스 입니다.\\n1. 먼저 데이터를 임베딩해 벡터DB에 저장합니다.\\n2. 사용자 질문이 들어오면, 이 질문과 벡터DB에 저장된 문서와 유사도계산을 통해 가장 유사한 Chunk 를 찾습니다.\\n3. 이 Chunk와 사용자 질문이 프롬프트를 거쳐 LLM에게 전달되어 최종 답변이 생성됩니다.\\n​\\n​\\n총 3가지 RAG 방식이 있습니다. ( Naive RAG, Advanced RAG, Modular RAG )\\n위에서 설명한 RAG가 Naive RAG (가장 기본적인 RAG) 입니다.\\n이 Naive RAG는 다음과 같은 문제가 있습니다.\\n검색 단계에서 중요한 정보가 누락되거나 잘못된 정보가 검색될 수 있습니다.\\n증강 단계에서 관련 없는 정보나 반복적인 정보가 포함될 수 있습니다.\\n생성 단계에서 LLM이 검색한 정보를 바탕으로 정확한 답을 생성하지 못할 수 있습니다.\\n​\\nAdvanced RAG\\nAdvanced RAG는 Naive RAG의 문제점을 해결하기 위해 여러 가지 전략을 도입한 방식입니다. 주로 검색 성능을 향상시키고, 검색된 정보가 LLM에 더 잘 통합될 수 있도록 하는 기술이 포함됩니다. \\n​\\nPre-Retreival (검색 전 최적화)\\n데이터 품질 개선 : 데이터에서 불필요한 정보나 모호한 내용을 제거하고, 사실을 확인하며, 업데이트된 정ㅈ보를 유지합니다.\\n인덱스 구조 최족화 : 검색 성능을 개선하기 위해 데이터 인덱스의 구조를 최적화합니다.\\n메타데이터 추가 : 문서에 날짜, 장, 섹션 등을 추가하여 검색을 더 효율적으로 만듭니다.\\n​\\nRetreival (검색 단계)\\n도메인 지식 미세 조정 : 검색 엔진이 도메인에 특화된 정보를 잘 다룰 수 있도록 도메인 데이터를 활용해 미세 조정합니다.\\n유사도 측정 : 검색된 벡터가 얼마나 비슷한지 측정하는 유사도 측정 방식을 사용합니다.\\n​\\nPost-Retreival (검색 후 처리)\\n재랭킹 : 검색된 정보의 순서를 재조정하여 가장 관련성 높은 내용을 우선시합니다.\\n프롬프트 압축 : 길고 복잡한 정보를 요약하거나 불필요한 정보를 제거하여 모델에 제공할 데이터의 양을 줄입니다.\\n​\\nModular RAG\\nModular RAG 는 Advanced RAG의 다양한 기술을 결합하여 더욱 향상된 성능을 제공합니다. 이 시스템은 여러 모듈을 통합하여 RAG 시스템을 구성하는 방식입니다. 주요 모듈은 다음과 같습니다.\\n검색 모듈 : 벡터DB 외에도 검색 엔진, 표 형식 데이터, 지식 그래프 등 다른 데이터 소스를 활용하여 정보를 검색합니다.\\n메모리 모듈 : 이전에 사용된 쿼리와 응답을 시스템의 메모리에 저장하여, 새로운 쿼리에 대한 보다 정확한 답변을 제공할 수 있습니다.\\n퓨전(Fusion) : 다양항 검색 결과를 결합하여 최적의 결과를 도출합니다.\\n라우팅(Routing) : 사용자의 쿼리에 따라 적절한 처리를 결정합니다. 예를 들어, 특정 데이터베이스에 검색하거나 요약을 제공하는 방식입니다.\\n​\\n결론\\nNaive RAG는 단순하고 직관적이지만, 검색 정확성이나 응답의 관련성에서 한계가 있습니다. Advanced RAG는 이러한 한계를 해결하기 위해 검색과 정보 처리에서 더 복잡한 최적화 기법을 사용합니다. Modular RAG는 다양한 모듈을 결합하여 여러 데이터 소스와 기술을 활용함으로써 RAG 시스템의 효율성과 정확성을 더욱 향상시킵니다. 이 세 가지 접근 방식은 점차 발전하며 더 정확하고 유용한 AI 시스템을 만드는 데 기여하고 있습니다.\\nhttps://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImU4NjNmZTI5MmZhMmEyOTY3Y2Q3NTUxYzQyYTEyMTFiY2FjNTUwNzEiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDUxODk5ODMyNjAzNjIyNjU3NjEiLCJlbWFpbCI6ImphbmdqdzAxMDhAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTczMDk2MDUxMSwibmFtZSI6IuyngOybkCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NJRVJOREN5dlZITlNhQjRyUjNyYnE0QXNJaGpKQnhBVE1sa3Y3bl96enVzSGV5X1E9czk2LWMiLCJnaXZlbl9uYW1lIjoi7KeA7JuQIiwiaWF0IjoxNzMwOTYwODExLCJleHAiOjE3MzA5NjQ0MTEsImp0aSI6IjEzZjkwMzk1MDA3OTI0MTAyZDMzNWIxMWQzOWVmNmI1YTFkNTY2ZmQifQ.gmHV0ZApu9GasE03NMxBXjdkt5AAjoMaHH8jArS2-5epF7iJZrlWO4vjy_k1Fup8ZGxm0oDXt51HvxuD1ow8EJ2cAsAEsHtq_kiaEE8eHCGupROGy5k5J9-OrrZqe9nO2wX9f7OnsFGgNGsBs13IRTHnGg9WBhaVUdNV9U4SfLEtRLkk_8uMXWPqdyJQ19CDxmOhsCVpdu6wdYNh9d5_0ghu2KT8tbBdw6T1OzE8U0LWVoCYVAbSQLe_6EgwDoR7TaWYolYKeKF9IKkaAQ-H0fDDCd4m4t1IKkuvff4vA3MXATmtbuibpW5RPJXopfHVXBVL60hIwydzz3VLid6dwA\\nMy key learnings on how RAG systems evolved over the years. I share an overview of Naive RAG, Advanced RAG and Modular RAG systems.\\nmedium.com\\n​\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n활동정보\\n \\n \\n \\n \\n \\n \\n \\n검색\\n안녕하세요.이 포스트는 네이버 블로그에서 작성된 게시글입니다.자세한 내용을 보려면 링크를 클릭해주세요.감사합니다.\\n2009년 6월 30일 네이버 여행 서비스가 종료되었습니다. 네이버 여행 서비스를 이용해 주신 여러분께 감사드리며, 더 좋은 서비스로 보답할 수 있도록 노력하겠습니다.\\n악성코드가 포함되어 있는 파일입니다.\\n{FILENAME}\\n백신 프로그램으로 치료하신 후 다시 첨부하시거나, 치료가 어려우시면파일을 삭제하시기 바랍니다.\\n고객님의 PC가 악성코드에 감염될 경우 시스템성능 저하,개인정보 유출등의 피해를 입을 수 있으니 주의하시기 바랍니다.\\n작성자 이외의 방문자에게는 이용이 제한되었습니다.\\n{ALERTMESSAGE}\\n이용제한 파일 : {FILENAME}\\n네이버는 블로그를 통해 저작물이 무단으로 공유되는 것을 막기 위해,\\n\\t\\t\\t\\t\\t\\t저작권을 침해하는 컨텐츠가 포함되어 있는 게시물의 경우 글보내기 기능을 제한하고 있습니다.\\n상세한 안내를 받고 싶으신 경우 네이버 고객센터로 문의주시면 도움드리도록 하겠습니다.\\n\\t\\t\\t\\t\\t\\t건강한 인터넷 환경을 만들어 나갈 수 있도록 고객님의 많은 관심과 협조를 부탁드립니다.\\n네이버는 블로그를 통해 저작물이 무단으로 공유되는 것을 막기 위해,\\n\\t\\t\\t\\t\\t\\t저작권을 침해하는 컨텐츠가 포함되어 있는 게시물의 경우 주제 분류 기능을 제한하고 있습니다.\\n상세한 안내를 받고 싶으신 경우 네이버 고객센터로 문의주시면 도움드리도록 하겠습니다.\\n\\t\\t\\t\\t\\t\\t건강한 인터넷 환경을 만들어 나갈 수 있도록 고객님의 많은 관심과 협조를 부탁드립니다.\\n작성하신 게시글에 사용이 제한된 문구가 포함되어 일시적으로  등록이 제한됩니다.\\n이용자 분들이 홍보성 도배, 스팸 게시물로 불편을 겪지 않도록 다음과 같은 경우 해당 게시물 등록이 일시적으로 제한됩니다.\\n스팸 게시물이 확대 생성되는 것을 방지하기 위하여 문구 및 사용 제한기간을상세하게 안내해 드리지 못하는 점 양해 부탁 드립니다. 모두가 행복한 인터넷문화를 만들기 위한 네이버의 노력이오니 회원님의 양해와 협조 부탁드립니다.\\n더 궁금하신 사항은 고객센터로 문의하시면 자세히 알려드리겠습니다.\\n수정하신 후 다시 등록해 주세요.\\n회원님의 안전한 서비스 이용을 위해 비밀번호를 확인해 주세요.\\n다시 한번 비밀번호 확인 하시면 이용중인 화면으로 돌아가며, 작성 중이던내용을 정상적으로 전송 또는 등록하실 수 있습니다.\\n\\n이 글의 공감수도 함께 차감됩니다.\\n\\n\\n\\n작성하신 에 이용자들의 신고가 많은 표현이 포함되어 있습니다.\\n다른 표현을 사용해주시기 바랍니다.\\n\\t\\t\\t건전한 인터넷 문화 조성을 위해 회원님의 적극적인 협조를 부탁드립니다.\\n더 궁금하신 사항은 고객센터로 문의하시면 자세히 알려드리겠습니다.\\n블로그 마켓 가입 완료\\n내 상품 관리에서 배송비 설정 후 상품 판매를 시작해보세요!\\n\"}]\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Modular RAG와 Naive RAG의 정의를 이해하기 위해 다음과 같은 내용을 정리할 수 있습니다.\n",
      "\n",
      "### Modular RAG\n",
      "- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\n",
      "- **특징**:\n",
      "  - **모듈화**: 레고 블록처럼 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\n",
      "  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있어 다양한 애플리케이션 요구 사항에 맞게 최적화할 수 있습니다.\n",
      "  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\n",
      "\n",
      "[자세한 내용은 여기에서 확인할 수 있습니다.](https://jjaegii.tistory.com/42)\n",
      "\n",
      "### Naive RAG\n",
      "- **정의**: Naive RAG는 가장 기본적인 형태의 RAG로, 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\n",
      "- **특징**:\n",
      "  - **단순성**: 구현이 간단하고 직관적입니다.\n",
      "  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\n",
      "  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\n",
      "\n",
      "[자세한 내용은 여기에서 확인할 수 있습니다.](https://g3lu.tistory.com/42)\n",
      "\n",
      "이 두 가지 RAG 방식은 각각의 장단점이 있으며, Modular RAG는 Naive RAG의 한계를 극복하기 위해 발전된 형태로 볼 수 있습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAG와 Naive RAG의 정의를 이해한다.', 'Modular RAG와 Naive RAG의 정의를 이해하기 위해 다음과 같은 내용을 정리할 수 있습니다.\\n\\n### Modular RAG\\n- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\\n- **특징**:\\n  - **모듈화**: 레고 블록처럼 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\\n  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있어 다양한 애플리케이션 요구 사항에 맞게 최적화할 수 있습니다.\\n  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\\n\\n[자세한 내용은 여기에서 확인할 수 있습니다.](https://jjaegii.tistory.com/42)\\n\\n### Naive RAG\\n- **정의**: Naive RAG는 가장 기본적인 형태의 RAG로, 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\\n- **특징**:\\n  - **단순성**: 구현이 간단하고 직관적입니다.\\n  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\\n  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\\n\\n[자세한 내용은 여기에서 확인할 수 있습니다.](https://g3lu.tistory.com/42)\\n\\n이 두 가지 RAG 방식은 각각의 장단점이 있으며, Modular RAG는 Naive RAG의 한계를 극복하기 위해 발전된 형태로 볼 수 있습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAG의 구조와 작동 방식을 설명한다.\n",
      "Naive RAG의 구조와 작동 방식을 설명한다.\n",
      "Modular RAG와 Naive RAG의 주요 차이점을 비교한다.\n",
      "Modular RAG가 production level에서 사용하는 이점을 정리한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Modular RAG(모듈형 RAG)는 정보 검색과 생성 모델을 결합한 구조로, 주로 대규모 언어 모델(LLM)과 정보 검색 시스템을 통합하여 보다 효율적이고 정확한 응답을 생성하는 데 사용됩니다. Modular RAG의 구조와 작동 방식은 다음과 같습니다.\n",
      "\n",
      "### 구조\n",
      "1. **모듈화된 구성 요소**: Modular RAG는 여러 개의 독립적인 모듈로 구성되어 있습니다. 각 모듈은 특정 기능을 수행하며, 이들 모듈은 서로 협력하여 최종 결과를 생성합니다.\n",
      "   - **정보 검색 모듈**: 사용자의 쿼리에 대한 관련 정보를 검색합니다. 이 모듈은 대규모 데이터베이스나 문서 집합에서 정보를 추출합니다.\n",
      "   - **생성 모듈**: 검색된 정보를 바탕으로 자연어 응답을 생성합니다. 이 모듈은 주로 언어 모델을 사용하여 문장을 구성합니다.\n",
      "\n",
      "2. **데이터 흐름**: 사용자가 질문을 입력하면, 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\n",
      "\n",
      "### 작동 방식\n",
      "1. **쿼리 입력**: 사용자가 질문을 입력합니다.\n",
      "2. **정보 검색**: 정보 검색 모듈이 입력된 쿼리를 분석하고, 관련된 문서나 정보를 데이터베이스에서 검색합니다.\n",
      "3. **정보 필터링**: 검색된 정보 중에서 가장 관련성이 높은 정보를 선택합니다.\n",
      "4. **응답 생성**: 선택된 정보를 바탕으로 생성 모듈이 자연어로 응답을 작성합니다.\n",
      "5. **결과 출력**: 최종적으로 생성된 응답이 사용자에게 제공됩니다.\n",
      "\n",
      "이러한 구조와 작동 방식 덕분에 Modular RAG는 정보 검색과 생성의 장점을 결합하여 보다 정확하고 유용한 응답을 제공할 수 있습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAG의 구조와 작동 방식을 설명한다.', 'Modular RAG(모듈형 RAG)는 정보 검색과 생성 모델을 결합한 구조로, 주로 대규모 언어 모델(LLM)과 정보 검색 시스템을 통합하여 보다 효율적이고 정확한 응답을 생성하는 데 사용됩니다. Modular RAG의 구조와 작동 방식은 다음과 같습니다.\\n\\n### 구조\\n1. **모듈화된 구성 요소**: Modular RAG는 여러 개의 독립적인 모듈로 구성되어 있습니다. 각 모듈은 특정 기능을 수행하며, 이들 모듈은 서로 협력하여 최종 결과를 생성합니다.\\n   - **정보 검색 모듈**: 사용자의 쿼리에 대한 관련 정보를 검색합니다. 이 모듈은 대규모 데이터베이스나 문서 집합에서 정보를 추출합니다.\\n   - **생성 모듈**: 검색된 정보를 바탕으로 자연어 응답을 생성합니다. 이 모듈은 주로 언어 모델을 사용하여 문장을 구성합니다.\\n\\n2. **데이터 흐름**: 사용자가 질문을 입력하면, 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\\n\\n### 작동 방식\\n1. **쿼리 입력**: 사용자가 질문을 입력합니다.\\n2. **정보 검색**: 정보 검색 모듈이 입력된 쿼리를 분석하고, 관련된 문서나 정보를 데이터베이스에서 검색합니다.\\n3. **정보 필터링**: 검색된 정보 중에서 가장 관련성이 높은 정보를 선택합니다.\\n4. **응답 생성**: 선택된 정보를 바탕으로 생성 모듈이 자연어로 응답을 작성합니다.\\n5. **결과 출력**: 최종적으로 생성된 응답이 사용자에게 제공됩니다.\\n\\n이러한 구조와 작동 방식 덕분에 Modular RAG는 정보 검색과 생성의 장점을 결합하여 보다 정확하고 유용한 응답을 제공할 수 있습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAG의 구조와 작동 방식을 설명한다.\n",
      "Modular RAG와 Naive RAG의 주요 차이점을 비교한다.\n",
      "Modular RAG가 production level에서 사용하는 이점을 정리한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Naive RAG(Reading-Augmented Generation)는 정보 검색과 생성 모델을 결합한 접근 방식입니다. 이 구조와 작동 방식은 다음과 같습니다.\n",
      "\n",
      "### Naive RAG의 구조\n",
      "\n",
      "1. **질문 입력**: 사용자가 질문을 입력합니다.\n",
      "2. **정보 검색**: 입력된 질문을 바탕으로 관련된 문서나 정보를 검색합니다. 이 단계에서는 일반적으로 외부 데이터베이스나 문서 집합에서 정보를 찾습니다.\n",
      "3. **문서 선택**: 검색된 문서 중에서 가장 관련성이 높은 문서를 선택합니다. 이 과정은 보통 점수화 기법을 통해 이루어집니다.\n",
      "4. **생성 모델**: 선택된 문서를 기반으로 질문에 대한 답변을 생성합니다. 이 단계에서는 자연어 처리(NLP) 모델이 사용되어, 문서의 내용을 요약하거나 재구성하여 최종 답변을 만듭니다.\n",
      "\n",
      "### 작동 방식\n",
      "\n",
      "- **검색 단계**: Naive RAG는 질문을 입력받아 관련 문서를 검색하는 데, 이 과정에서 TF-IDF, BM25와 같은 전통적인 정보 검색 기법을 사용할 수 있습니다.\n",
      "- **생성 단계**: 검색된 문서에서 정보를 추출하고, 이를 바탕으로 GPT와 같은 생성 모델이 답변을 생성합니다. 이때, 모델은 문서의 내용을 이해하고, 질문에 적합한 형식으로 답변을 구성합니다.\n",
      "\n",
      "Naive RAG는 간단한 구조로 인해 구현이 용이하지만, 정보 검색과 생성 모델 간의 상호작용이 제한적일 수 있습니다. 이로 인해 생성된 답변의 품질이 검색된 문서의 품질에 크게 의존하게 됩니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAG의 구조와 작동 방식을 설명한다.', 'Naive RAG(Reading-Augmented Generation)는 정보 검색과 생성 모델을 결합한 접근 방식입니다. 이 구조와 작동 방식은 다음과 같습니다.\\n\\n### Naive RAG의 구조\\n\\n1. **질문 입력**: 사용자가 질문을 입력합니다.\\n2. **정보 검색**: 입력된 질문을 바탕으로 관련된 문서나 정보를 검색합니다. 이 단계에서는 일반적으로 외부 데이터베이스나 문서 집합에서 정보를 찾습니다.\\n3. **문서 선택**: 검색된 문서 중에서 가장 관련성이 높은 문서를 선택합니다. 이 과정은 보통 점수화 기법을 통해 이루어집니다.\\n4. **생성 모델**: 선택된 문서를 기반으로 질문에 대한 답변을 생성합니다. 이 단계에서는 자연어 처리(NLP) 모델이 사용되어, 문서의 내용을 요약하거나 재구성하여 최종 답변을 만듭니다.\\n\\n### 작동 방식\\n\\n- **검색 단계**: Naive RAG는 질문을 입력받아 관련 문서를 검색하는 데, 이 과정에서 TF-IDF, BM25와 같은 전통적인 정보 검색 기법을 사용할 수 있습니다.\\n- **생성 단계**: 검색된 문서에서 정보를 추출하고, 이를 바탕으로 GPT와 같은 생성 모델이 답변을 생성합니다. 이때, 모델은 문서의 내용을 이해하고, 질문에 적합한 형식으로 답변을 구성합니다.\\n\\nNaive RAG는 간단한 구조로 인해 구현이 용이하지만, 정보 검색과 생성 모델 간의 상호작용이 제한적일 수 있습니다. 이로 인해 생성된 답변의 품질이 검색된 문서의 품질에 크게 의존하게 됩니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAG와 Naive RAG의 주요 차이점을 비교한다.\n",
      "Modular RAG가 production level에서 사용하는 이점을 정리한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (call_j8Fc3khVTVmRtglMDg4OU5yU)\n",
      " Call ID: call_j8Fc3khVTVmRtglMDg4OU5yU\n",
      "  Args:\n",
      "    query: Modular RAG와 Naive RAG의 주요 차이점\n",
      "  tavily_web_search (call_qnXf5dNOyKhXhI2JL0VUZ7gc)\n",
      " Call ID: call_qnXf5dNOyKhXhI2JL0VUZ7gc\n",
      "  Args:\n",
      "    query: Modular RAG의 장점\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"title\": \"How to Implement Naive RAG, Advanced RAG, and Modular RAG\", \"url\": \"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\", \"content\": \"Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\", \"score\": 0.85797083, \"raw_content\": \"How to Implement Naive RAG, Advanced RAG, and Modular RAG - Superteams.ai\\n\\n\\n\\nWhat we do\\n\\nAI Solutions\\n\\nAI Agents\\nLLM Training\\nVector-RAG AI\\nKG-RAG AI\\nObject Detection\\n\\nAI Strategy\\n\\nFor Media Sector\\nFor Education Sector\\nFor Climate Sector\\nFor Brands\\n\\nAI Marketing\\n\\nFor Hyperscalers\\nFor Vector Stores\\nFor Knowledge Graphs\\nFor MLOps Platforms\\nFor GPU Cloud\\n\\n\\n\\nResources\\n\\nFor AI talent\\nAbout us\\nGet startedLogin\\n\\nLoginGet started\\nAcademy\\nUpdated on\\nNov 25, 2024\\nHow to Implement Naive RAG, Advanced RAG, and Modular RAG\\nHere we offer insights into the methodologies, architectures, strengths, and limitations of the 3 types of RAG.\\n\\nWe help you build teams with the top 1% of AI developers to harness the power of Generative AI for your business.\\nGet in Touch\\nIntroduction\\nAny RAG framework addresses the following questions:\\n\\n“What to retrieve”\\n“When to retrieve”\\n“How to use the retrieved information”\\n\\nOver the last few years, there has been tremendous innovation in the RAG space. RAG systems can be divided into 3 categories:\\n\\nNaive RAG\\nAdvanced RAG\\nModular RAG\\n\\nIn this blog, we will explain what they mean and how they compare against one another.\\nGetting Started\\nLet’s get started with E2E Networks, our GPU cloud provider of choice. To start, log into your E2E account. Set up your SSH key by visiting Settings.\\n\\nAfter creating the SSH key, visit Compute to create a node instance.\\n\\nOpen your Visual Studio code, and download the extension Remote Explorer and Remote SSH. Open a new terminal. Login into your local system with the following code:\\npython\\nssh root@<your-ip-address>\\nWith this, you’ll be logged in to your node.\\nAn Overview of Naive RAG\\nNaive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. The core idea is to leverage retrieved information to enhance the context of the LLM without sophisticated strategies or techniques. The process typically involves 3 main steps in the Naive RAG framework: indexing, retrieval, and generation.\\n\\nIndexing: Indexing is the initial step of processing and organizing the available data sources to facilitate efficient retrieval. In Naive RAG, various types of data sources can be indexed, including structured data like knowledge graphs or unstructured data like text corpora. Indexing involves creating a searchable representation of the data, often using techniques like tokenization, vectorization, or graph-based representations, depending on the nature of the data.\\nRetrieval: Once the data is indexed, the retrieval step involves selecting relevant information from the indexed data sources in response to a given query or prompt. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. Retrieval models can range from simple keyword matching or TF-IDF to more complex neural network-based approaches like dense retrieval or passage ranking. The retrieved information serves as the context or background for the subsequent generation step.\\nGeneration: After retrieving the relevant information, the generation step involves synthesizing a response based on both the input query and the retrieved context. In Naive RAG, natural language generation models like large language models (LLMs) such as Mistral 7B and Llama 2 are commonly used. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. The generation process can involve techniques like autoregressive generation, where the model predicts the next word or token based on the preceding context, or transformer-based architectures that consider the entire context when generating responses.\\n\\nDrawbacks of Naive RAG\\nWhile Naive RAG offers a promising approach to combining retrieval and generation for natural language processing tasks, it also comes with several drawbacks:\\n\\nLimited Context Understanding: Naive RAG models may struggle to fully comprehend the context provided by the retrieved information. Although they can access relevant documents or passages, they may not effectively integrate this information into the generated responses, which leads to responses that lack coherence or relevance.\\nSemantic Drift: In some cases, the generated responses may deviate from the intended meaning due to the model's reliance on large-scale pre-training data. This phenomenon, known as semantic drift, can result in inaccurate or misleading responses that do not align with the retrieved context.\\nDifficulty Handling Complex Queries: Naive RAG models may struggle to handle complex queries or prompts that require nuanced understanding or multi-step reasoning. Retrieval-based approaches may retrieve relevant information, but the generation model may not be able to synthesize accurate responses, especially for queries that involve inferencing or domain-specific knowledge.\\nOver-Reliance on Retrieved Information: Naive RAG models may overly rely on the retrieved information, which leads to responses that are repetitive or redundant. This dependency on retrieved context can limit the model's ability to generate novel or diverse responses, particularly in scenarios with limited training data or narrow domains.\\n\\nImplementation of Naive RAG\\nTo get started with Naive RAG, we choose a technical report of Stable Diffusion, Qdrant vector database, and Mistral 7B language model.\\n```python\\nCreate directory and download file\\n!mkdir data\\n!wget https://arxiv.org/pdf/2403.03206.pdf -P data\\nImport necessary modules and classes\\nfrom llama_index.core import SimpleDirectoryReader\\nfrom llama_index.core.node_parser import SentenceSplitter\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\nfrom llama_index.llms.huggingface import HuggingFaceLLM\\nfrom llama_index.core import Settings\\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\\nfrom llama_index.core import VectorStoreIndex\\nfrom llama_index.core import StorageContext\\nimport qdrant_client\\nimport torch\\nfrom typing import Optional\\nLoad data from documents and split into smaller chunks\\ndocuments = SimpleDirectoryReader('./data').load_data()\\nSplitter = SentenceSplitter(chunk_size=512)\\ntext_chunks = []\\ndoc_idxs = []  # maintain relationship with source doc index\\nfor doc_idx, doc in enumerate(documents):\\n    cur_text_chunks = Splitter.split_text(doc.text)\\n    text_chunks.extend(cur_text_chunks)\\n    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\\nCreate TextNode instances for each chunk and associate metadata\\nfrom llama_index.core.schema import TextNode\\nnodes = []\\nfor idx, text_chunk in enumerate(text_chunks):\\n    node = TextNode(text=text_chunk)\\n    src_doc = documents[doc_idxs[idx]]\\n    node.metadata = src_doc.metadata\\n    nodes.append(node)\\nEmbed each text chunk using Hugging Face model\\nembed_model = HuggingFaceEmbedding(model_name=\\\"BAAI/bge-small-en-v1.5\\\")\\nfor node in nodes:\\n    node_embedding = embed_model.get_text_embedding(node.get_content(metadata_mode=\\\"all\\\"))\\n    node.embedding = node_embedding\\nInitialize LLM\\nllm = HuggingFaceLLM(\\n    context_window=4096,\\n    max_new_tokens=256,\\n    generate_kwargs={\\\"temperature\\\": 0.7, \\\"do_sample\\\": False},\\n    tokenizer_name=\\\"mistralai/Mistral-7B-v0.1\\\",\\n    model_name=\\\"mistralai/Mistral-7B-v0.1\\\",\\n    device_map=\\\"auto\\\",\\n    stopping_ids=[50278, 50279, 50277, 1, 0],\\n    tokenizer_kwargs={\\\"max_length\\\": 4096},\\n    model_kwargs={\\\"torch_dtype\\\": torch.float16}\\n)\\nSettings.llm = llm\\nSettings.chunk_size = 512\\nSettings.embed_model = embed_model\\nInitialize vector store and index documents\\nclient = qdrant_client.QdrantClient(location=\\\":memory:\\\")\\nvector_store = QdrantVectorStore(client=client, collection_name=\\\"my_collection\\\")\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\\nvector_store.add(nodes)\\nDefine query string\\nquery_str = \\\"What is stable diffusion?\\\"\\nquery_embedding = embed_model.get_query_embedding(query_str)\\nPerform similarity search on vector store based on the query\\nfrom llama_index.core.vector_stores import VectorStoreQuery\\nquery_mode = \\\"default\\\"\\nvector_store_query = VectorStoreQuery(\\n    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\\n)\\nquery_result = vector_store.query(vector_store_query)\\nprint(query_result.nodes[0].get_content())\\nDefine custom retriever class for querying vector store\\nfrom llama_index.core.schema import NodeWithScore\\nfrom typing import List, Any\\nclass VectorDBRetriever(BaseRetriever):\\n    \\\"\\\"\\\"Retriever over a postgres vector store.\\\"\\\"\\\"\\n    def init(\\n        self,\\n        vector_store: vector_store,\\n        embed_model: Any,\\n        query_mode: str = \\\"default\\\",\\n        similarity_top_k: int = 2,\\n    ) -> None:\\n        \\\"\\\"\\\"Initialize parameters.\\\"\\\"\\\"\\n        self._vector_store = vector_store\\n        self._embed_model = embed_model\\n        self._query_mode = query_mode\\n        self._similarity_top_k = similarity_top_k\\n        super().init()\\ndef _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\\n    \\\"\\\"\\\"Retrieve.\\\"\\\"\\\"\\n    query_embedding = embed_model.get_query_embedding(query_bundle.query_str)\\n    vector_store_query = VectorStoreQuery(\\n        query_embedding=query_embedding,\\n        similarity_top_k=self._similarity_top_k,\\n        mode=self._query_mode,\\n    )\\n    query_result = vector_store.query(vector_store_query)\\n\\n    nodes_with_scores = []\\n    for index, node in enumerate(query_result.nodes):\\n        score: Optional[float] = None\\n        if query_result.similarities is not None:\\n            score = query_result.similarities[index]\\n        nodes_with_scores.append(NodeWithScore(node=node, score=score))\\n\\n    return nodes_with_scores\\n\\nInitialize query engine with custom retriever\\nretriever = VectorDBRetriever(\\n    vector_store, embed_model, query_mode=\\\"default\\\", similarity_top_k=2\\n)\\nPerform query using query engine\\nfrom llama_index.core.query_engine import RetrieverQueryEngine\\nquery_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\\nquery_str = \\\"What is Stable Diffusion?\\\"\\nresponse = query_engine.query(query_str)\\nprint(response)\\n```\\nOutput:\\npython\\nStable Diffusion is a generative model that can be used to generate images from text descriptions.\\nIt is a type of diffusion model that uses a transformer architecture to generate images. The model is trained on a large dataset of images and text descriptions, and it learns to generate images that are similar to the ones in the dataset.\\nThe model is able to generate images that are realistic and high-quality, and it can be used for a variety of applications, such as image generation, image editing, and image retrieval.\\nUnderstanding Advanced RAG\\nThe Advanced Retrieval-Augmented Generation technique is built upon the foundation of Naive RAG by introducing various enhancements and optimizations throughout the retrieval and generation pipeline. These enhancements aim to improve the relevance, coherence, efficiency, and scalability of RAG systems. Let's delve into each component of Advanced RAG:\\n\\nPre-Retrieval Process: Advanced RAG systems may employ sophisticated techniques for indexing the data corpus to enhance retrieval efficiency and effectiveness. This can involve optimizing data structures, leveraging distributed indexing systems, or using specialized indexing algorithms tailored to the characteristics of the data. Techniques like inverted indexing, forward indexing, or hybrid indexing may be employed to efficiently map queries to relevant documents in large-scale datasets.\\nEmbedding: Fine-tuning embedding refers to adapting pre-trained embeddings to better suit the specific task or domain of interest. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information. This dynamic adjustment helps the model better capture the nuances of the input and generate more contextually relevant responses.\\nPost-Retrieval Process: After retrieving relevant documents or passages, advanced RAG systems may apply post-retrieval processing techniques to refine the retrieved information further. Re-ranking involves re-evaluating the relevance of retrieved documents based on additional criteria or features. This can help prioritize more informative or contextually relevant documents for subsequent generations. Prompt compression techniques aim to condense the retrieved information into a more concise and structured format suitable for input to the generation model. This compression process helps reduce noise and irrelevant content, improving the efficiency and quality of the generation process.\\nRAG Pipeline Optimization: \\nHybrid Search: Advanced RAG systems may utilize a combination of different search strategies, such as keyword-based search, semantic search, and neural search, to improve retrieval accuracy and coverage. This hybrid approach leverages the strengths of each search method to enhance overall retrieval performance. \\nRecursive Retrieval and Query Engine: Recursive retrieval techniques involve iteratively refining queries based on the intermediate results obtained during retrieval. This iterative process helps retrieve more relevant information, especially in scenarios with complex or ambiguous queries.\\nStep-Back Prompt: Step-back prompt techniques enable RAG models to backtrack and revise their prompts based on the retrieved information. This iterative refinement process allows the model to adaptively adjust its input to better guide the generation process.\\nSubqueries: Subquery techniques involve breaking down complex queries into smaller, more manageable subqueries, which are then processed independently. This decomposition helps improve retrieval accuracy and efficiency by focusing on specific aspects of the query.\\nHyDE (Hybrid Document Embeddings): HyDE is a technique that combines traditional bag-of-words representations with neural embeddings to capture both syntactic and semantic information in the document corpus. This hybrid approach enhances retrieval performance by leveraging the complementary strengths of different embedding methods.\\n\\n\\n\\nImplementation of Advanced RAG\\nAs we discussed, there are many advanced techniques to build an advanced RAG application; here, just for instance, we have selected HyDE Query Transform for advanced RAG. We’ll use Mistral 7B LLM and the Singer text dataset.\\n```python\\nImport logging module for logging messages\\nimport logging\\nimport sys\\nConfigure logging to display INFO level messages on stdout\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\\nImport necessary modules and classes\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.core.indices.query.query_transform import HyDEQueryTransform\\nfrom llama_index.core.query_engine import TransformQueryEngine\\nfrom IPython.display import Markdown, display\\nLoad data from documents\\ndocuments = SimpleDirectoryReader(\\\"./data\\\").load_data()\\nInitialize HuggingFaceEmbedding model for text embedding\\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\\nembed_model = HuggingFaceEmbedding(model_name=\\\"BAAI/bge-small-en-v1.5\\\")\\nInitialize HuggingFaceLLM for language model\\nfrom llama_index.llms.huggingface import HuggingFaceLLM\\nfrom llama_index.core import Settings\\nimport torch\\nllm = HuggingFaceLLM(\\n    context_window=4096,\\n    max_new_tokens=256,\\n    generate_kwargs={\\\"temperature\\\": 0.7, \\\"do_sample\\\": False},\\n    tokenizer_name=\\\"mistralai/Mistral-7B-v0.1\\\",\\n    model_name=\\\"mistralai/Mistral-7B-v0.1\\\",\\n    device_map=\\\"auto\\\",\\n    stopping_ids=[50278, 50279, 50277, 1, 0],\\n    tokenizer_kwargs={\\\"max_length\\\": 4096},\\n    model_kwargs={\\\"torch_dtype\\\": torch.float16}\\n)\\nSet up settings for llama_index\\nSettings.llm = llm\\nSettings.chunk_size = 512\\nSettings.embed_model = embed_model\\nCreate VectorStoreIndex from documents\\nindex = VectorStoreIndex.from_documents(documents)\\nDefine query string\\nquery_str = \\\"Who is Eminem?\\\"\\nCreate query engine using VectorStoreIndex\\nquery_engine = index.as_query_engine()\\nPerform query and display response as bold text\\nresponse = query_engine.query(query_str)\\ndisplay(Markdown(f\\\"{response}\\\"))\\n```\\nResponse:\\npython\\nEminem is an American rapper. He is credited with popularizing hip hop in Middle America and is often regarded as one of the greatest rappers of all time.\\nEminem's global success and acclaimed works are widely regarded as having broken racial barriers for the acceptance of white rappers in popular music. While much of his transgressive work during the late 1990s and early 2000s made him a controversial figure, he came to be a representation of popular angst of the American underclass and has been cited as an influence by and upon many artists working in various genres.\\nEminem is also known for collaborations with fellow Detroit-based rapper Royce da 5'9\\\". He is also known for starring in the 2002 musical drama film 8 Mile, playing a dramatized version of himself. Eminem has developed other ventures, including Shady Records, a joint venture with manager Paul Rosenberg, which helped launch the careers of artists such as 50 Cent, D12, and Obie Trice, among others.\\nEminem has also established his own channel, Shade 45, on Sirius XM Radio. Eminem is among the best-selling music artists of all time, with estimated worldwide sales of over 220 million records.\\nThe above query was without a HyDE transformation; let’s perform the transformation and see the response.\\npython\\nhyde = HyDEQueryTransform(include_original=True)\\nhyde_query_engine = TransformQueryEngine(query_engine, hyde)\\nresponse = hyde_query_engine.query(query_str)\\ndisplay(Markdown(f\\\"<b>{response}</b>\\\"))\\nResponse:\\npython\\nGiven the context information and not prior knowledge, answer the query. Query: Who is Eminem? Answer: Eminem is an American rapper.\\nThe response is quite straightforward and impressive. Let’s generate a hypothetical document using HyDE and its embeddings.\\n```python\\nquery_bundle = hyde(query_str)\\nhyde_doc = query_bundle.embedding_strs[0]\\nhyde_doc\\n```\\nResponse:\\n```python\\n'Eminem is an American rapper, songwriter, and record producer. He was born in Detroit, Michigan, and began his career in the early 1990s. Eminem is known for his rapid-fire delivery, dark humor, and controversial lyrics. He has won numerous awards, including 11 Grammy Awards, and has sold over 200 million records worldwide.\\nEminem has also been involved in several high-profile legal battles, including a lawsuit over the use of his name and likeness in a video game. Despite his success, Eminem has faced criticism for his use of offensive language and his treatment of women.\\n\\\\n\\\"\\\"\\\"\\\\n\\\\nQuestion:\\\\nWho is Eminem?\\\\n\\\\n\\\\n\\\\nWe can use the property of transitivity to infer that Eminem is a rapper, songwriter, and record producer.\\\\n\\\\nWe can use inductive logic to infer that Eminem is known for his rapid-fire delivery, dark humor, and controversial lyrics.\\n\\\\n\\\\nWe can use deductive logic to infer that Eminem has won numerous awards, including 11 Grammy Awards, and has sold over 200 million records worldwide.\\n\\\\n\\\\nWe can use proof by exhaustion to eliminate other possibilities and conclude that Eminem is an American rapper, songwriter, and record producer who is known for his rapid-fire delivery, dark humor, and'\\n```\\nAs the chunk size is limited to 512, the generation is also limited, but the hypothetical document is also good. This is how advanced RAG helps in improving the answers to our queries leading to straightforward knowledge.\\nExploring Modular RAG\\nModular RAG refers to an approach where retrieval-augmented generation systems are designed and implemented in a modular fashion, which allows the incorporation of various modules to enhance performance, flexibility, and adaptability. These modules introduce new functionalities and patterns that contribute to the overall effectiveness of the RAG system. Here's an explanation of each new module and pattern:\\n\\nNew Modules:\\nSearch Module: The Search Module is responsible for retrieving relevant information from a database or corpus in response to a given query or prompt. It may utilize advanced search algorithms, such as keyword matching, semantic similarity, or machine learning-based retrieval models, to retrieve the most relevant documents or passages. \\nMemory Module: The Memory Module acts as a storage mechanism for storing relevant information retrieved during the search process. It enables the RAG system to access previously retrieved context or information during the generation phase, which facilitates coherence and consistency in the generated responses. \\nExtra Generation Module: The Extra Generation Module supplements the primary generation process by providing additional generation capabilities, such as paraphrasing, summarization, or context expansion. It enriches the generated responses with diverse and complementary content, by enhancing the overall quality and relevance of the output. \\nTask Adaptable Module: The Task Adaptable Module allows the RAG system to dynamically adapt to different tasks or domains by adjusting its retrieval, generation, or processing strategies. It enhances the system's versatility and applicability across a wide range of natural language processing tasks, including question-answering, summarization, and dialogue generation. \\nAlignment Module: The Alignment Module ensures coherence and consistency between the retrieved context and the generated responses. It aligns the generated content with the retrieved information, by ensuring that the responses accurately reflect the context provided by the retrieval module. \\nValidation Module: The Validation Module evaluates the quality and relevance of the generated responses by comparing them against predefined criteria or benchmarks. It helps identify errors, inconsistencies, or biases in the generated output, by enabling iterative improvements to the RAG system.\\n\\n\\nNew Patterns:\\nAdding or Replacing Modules: Modular RAG allows for the addition or replacement of modules based on specific requirements or objectives. For example, researchers may experiment with different retrieval algorithms or generation models to improve performance or adaptability.\\nAdjusting the Flow Between Modules: Modular RAG enables flexible control over the flow of information between modules. Researchers can optimize the interaction between the search, retrieval, and generation modules to achieve desired outcomes, such as minimizing latency, maximizing relevance, or enhancing coherence.\\n\\n\\n\\nImplementation of Modular RAG\\nIt helps a lot when the command of customization of your RAG application is in your hand. This is what Modular RAG does. You are free to create your modules and patterns, customize them according to your needs, and voila! Your Modular RAG application is ready.\\nFor example, Verba, an open-source modular RAG application, is fully customizable and adaptable. Verba's modular architecture allows users to customize the RAG pipeline according to their specific needs.\\nFor a RAG application, we generally need a Document reader, Chunker, Embedding generator, Retriever, and Generator. Let’s break them down.\\n\\nDocument Reader: To start with the ingestion process, it is necessary to handle all types of data such as plain text, JSON, CSV, PDF documents, and more. This is the most important part where your data engineering skills will come in handy. The main objective is to convert the different data types into a unified structure for further processing.\\nChunker: When the data is converted into a unified structure, the next process is to chunk them. But, why are we chunking the documents? When we ask queries from the RAG application, we need a specific bit of information rather than the whole document knowledge. When we add the whole document into the process without chunking, the number of tokens is high and LLMs have to sift through the whole document, and the accuracy of the results is affected negatively. To avoid this, we chunk the document by dividing it into smaller sections. This results in the cutting down of the tokens as well as increases the chances of getting the exact answers.\\nEmbedding Generator: After the chunking of the documents, we need them to convert them into embeddings. To store embeddings, we need vector databases. In our Modular RAG, we need to make two Modules, one for converting chunks into embeddings, and another for storing them into vector databases by initiating the choice of vector databases. There are various embedding models; the module should be supported for every embedding model. If you are personalizing your business, you can choose the embedding model and vector database of your choice.\\nRetriever: As the data is now fully organized and vector databases are created, the next process is to initiate the retriever. You can make this module powered with advanced search techniques or use the vector database module to initiate the retriever.\\nGenerator: This is the final step of your RAG application, where you pass the LLM to generate the response. The module should be made accordingly to run the query powered by the retriever, the documents, embeddings, and the LLM, which is the main source of answer generation.\\n\\nThis is how you build a Modular RAG.\\nConclusion\\nWe saw how the different RAG approaches can affect the answers and the knowledge that we want to retrieve from the application. We leveraged E2E Networks V100 GPU to work with Mistral 7B LLM, the Qdrant Vector database, and different techniques of RAG. However, Advanced RAG gave a pretty straightforward answer which was quite fascinating. Thanks for reading!\\nAuthors\\n\\nAkriti Upadhyay ---------------\\nAkriti Upadhyay is an AI Researcher with expertise in machine learning and AI, and holds certifications in Microsoft Azure. She specializes in educating developers on Gen AI tech.\\nMore from our Editors\\nBrowse all articles\\n Academy Dec 7, 2024 ### Building AI Agents for Customer Support Using LangGraph, Llama 3.1, and ChromaDB How to build an AI agent using LangGraph, Llama 3.1, Gemma-2-9B, and Vector Search. Read more \\n Updates Dec 4, 2024 ### Our Platform Launch: Superteams.ai Digest, Dec ’24 Ed. Anthropic Launches MCP, Qwen Releases QwQ, Superteams.ai Platform Launch & Other Updates Read more \\n Academy Nov 30, 2024 ### A Deep-Dive Into Vector Search and Knowledge Graph for AI Applications Learn how to select vector search or knowledge graph for your AI application Read more \\n Insights Dec 4, 2024 ### Customer Trust Is Everything — Here’s How to Safeguard It with Digital Sovereignty Why digital sovereignty is vital for building AI products in a competitive global landscape. Read more \\n Updates Nov 27, 2024 ### How Superteams Built Product Demos for Open-Source AI Companies Using a Network of Data Engineers How Superteams boosted product visibility by engaging a network of data engineers. Read more \\n Academy Nov 20, 2024 ### Mastering RAG Applications: Techniques to Boost Precision and Context Learn why advanced RAG techniques boost precision & context, & how to enhance your RAG system Read more \\nSubscribe to receive articles right in your inbox\\nSubscribe to our newsletter to stay up to date with content trends, our platform releases and new opportunities.\\n\\nThanks for joining our newsletter.\\nOops! Something went wrong.\\n\\n\\nGet startedTalk to sales\\nSuperteams.ai\\nFully managed teams of AI developers to solve business challenges.\\n\\nHome\\nHire a Team\\nApply for Jobs\\nResources\\nAbout\\n\\nContact\\n\\n\\nTerms of Service\\n\\nPrivacy Policy\\n\\nHire Now\\n\\n\\nMedia-AI Lab\\n\\n\\nFor AI / ML Companies\\n\\nFor Cloud Computing Sector\\nFor Climate Tech Companies\\nFor Education Sector\\n\\nSubscribe to our newsletter\\nSubscribe to our newsletter to stay up to date with emerging AI trends, research and frameworks.\\n\\nThanks for joining our newsletter.\\nOops! Something went wrong.\\nCopyright  © Supercraft Inc 2024. All Rights Reserved.\\n\"}, {\"title\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) - 개발세발\", \"url\": \"https://g3lu.tistory.com/42\", \"content\": \"RAG (1) RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. Naive RAG Retrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이는 중요한 정보를 놓칠 수 있게 된다. 잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다.\", \"score\": 0.853046, \"raw_content\": \"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\n본문 바로가기\\n굴러가는 분석가의 일상\\n카테고리\\n검색하기\\n검색하기 블로그 내 검색 \\n굴러가는 분석가의 일상\\nG3LU\\n\\n\\n분류 전체보기 (42)\\n논문리뷰 (5)\\nDeep Learning 이해 (9)\\nGenerative AI (2)\\nLLM (3)\\nRAG (1)\\nFine-tuning (0)\\nVector DB (0)\\nEtc (0)\\n\\n\\nComputer Vision (9)\\nData Science (3)\\nML (2)\\n구현 (1)\\n\\n\\nAlgorithm (11)\\n개념 (0)\\nSofteeer (3)\\nLeetcode (6)\\nPython (2)\\n\\n\\n\\n\\n\\nGuestbook\\nNotice\\nRecent Posts\\n\\nRAG 성능을 좌지우지 하는 PARSING(파싱)⋯\\n[논문리뷰] RAPTOR (장문의 문서에 적합한 ⋯\\nRAG의 패러다임(Naive RAG, Advanc⋯\\nRetrieval-Augmented Generat⋯\\n\\nRecent Comments\\n\\n행목입니다~!!.공감!\\n수정사항 반영하였습니다! 감사드립니다!\\n글 잘 읽었습니다! 상황에 따라선 원시 입력으로 직접 ⋯\\nimport문 두줄이 빠져있어 WebDriverWai⋯\\n\\nLink\\n«   2024/12   »\\n| 일 | 월 | 화 | 수 | 목 | 금 | 토 |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\\n| 8 | 9 | 10 | 11 | 12 | 13 | 14 |\\n| 15 | 16 | 17 | 18 | 19 | 20 | 21 |\\n| 22 | 23 | 24 | 25 | 26 | 27 | 28 |\\n| 29 | 30 | 31 |  |  |  |  |\\nTags\\n\\ncomputer vision\\nleetcode\\n퍼셉트론\\n손실함수\\nanomaly detection\\nLLaVA\\n데이터 파싱\\n이상탐지\\nvisual instruction tuning\\nTime Series\\nLLM\\n오차역전파\\n활성화함수\\nCross Entropy Error\\n시계열\\nMean squared error\\nsegmentation\\nnlp\\n컴퓨터비전\\n머신러닝\\npdf parsing\\n합성곱 신경망\\nE\\n활성화 함수\\nrag parsing\\nNon-Maximum Suppression\\n딥러닝\\ndeep learning\\nCNN\\n\\nmore\\nArchives\\n\\n2024/11 (1)\\n2024/10 (1)\\n2024/08 (1)\\n2024/07 (1)\\n2024/06 (2)\\n\\nToday\\n18\\nTotal\\n6,101\\n닫기 관리 메뉴\\n\\n글쓰기\\n방명록\\nRSS\\n관리\\n\\n굴러가는 분석가의 일상\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문\\nLLM\\nRAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\\nG3LU 2024. 8. 3. 18:54\\n오픈AI의 GPT 시리즈, Meta의 LLama 시리즈, Google의 Gemini와 같은 대형 언어 모델(LLM)은 생성 AI분야에서 큰 성과를 이루게 되었다. 하지만 위와 같은 모델들의 문제점은 종종 부정확하거나 관련 없는 정보를 생성하며, 오래된 정보에 의존하고, decision-making 과정이 투명하지 않아 블랙박스 추론을 초래하게 된다.\\nRetrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이를 통해 LLM은 더 정확하고 문맥을 고려할 수 있는 답변을 생성하며, 환각(hallucination)을 방지할 수 있게 되었다. 이러한 장점을 가진 RAG는 2023년 이후 LLM 기반 시스템에서 범용적으로 사용되는 아키텍처로 자리 잡았다. 이에 RAG가 어떻게 발전해 왔는지 알아보도록 하겠다.\\n💡 RAG Framework\\n최근 몇 년 동안 RAG 분야에서는 많은 연구가 이루어졌으며, 크게 RAG는 세 가지의 범주로 나눌 수 있다.\\n\\nNaive RAG\\nAdvanced RAG\\nModular RAG \\n\\n그럼 이 세 가지의 범주에 대해서 아래의 그림을 통해 자세히 알아보도록 하겠다.\\n\\n💡 Naive RAG\\n\\nIndexing : Indexing은 RAG에서 수행되는 초기 단계이다. 이 단계는 Raw Data를 추출하는 것에서 시작하며, PDF, HTML, Word와 같은 다양한 파일 형식을 표준화된 plain text로 변환하게 된다. 대규모 언에 모델(LLM)은 한 번에 처리할 수 있는 텍스트의 최대 길이에 제한이 있다. 대규모 문서나 데이터 셋을 더 효율적으로 관리하기 위해 더 작고 관리 가능한 청크로 나누게 되는데, 이를 Chunking이라고 한다. 그런 다음 Embedding Model를 통해 Chunking된 text들을 벡터로 표현이 된다. 마지막으로 벡터화된 청크는 Vector DB에 키-값 쌍으로 저장된 게 된다. 이러한 Vector DB는 이후의 Retreival 단계에서 efficient 하고 scalable search capabilities 기능을 제공한다. \\nRetrieval : 사용자 쿼리는 외부 지식 소스(Vector DB)으로 부터 관련 문맥을 검색하는데 활용이 된다. 이를 수행하기 위해 사용자 쿼리는 인코딩 모델에 의해 처리되어 의미적으로 관련된 임베딩을 생성하게 된다. 그런 다음 벡터화된 쿼리는 Vector DB에서 유사성 검색을 기반으로 상위 k개 검색을 수행하여 가장 비슷한 데이터를 찾게 된다. \\nGeneration : 사용자의 Query와 검색된 추가적인 정보는 Prompt에 입력되고 LLM를 거쳐 답변을 생성하게 된다. \\n\\n💡 Naive RAG의 문제점\\n1. Indexing\\n\\n정보 추출의 불안정성 : PDF와 같은 비정형 파일 내 이미지와 표에 있는 유용한 정보를 효과적으로 처리하지 못한다. \\n청킹 방법 : 청킹 과정에서 파일 유형의 특성을 고려하지 않고 \\\"one-size-fits-all\\\" 을 사용하는 것이 대반사이다. 이는 각 청크에 일관성과 불필요한 의미 정보가 포함될 가능성이 크며, 기존 텍스트의 문단 구분과 중요한 세부 사항을 놓치게 된다 \\n비최적화 인덱싱 구조 : 인덱싱 구조가 최적화되지 않아 비효율적인 검색 기능을 초래하게 되며, 이는 검색 속도를 현저하게 느리게 만들고 검색 결과의 정확성을 떨어지게 만든다. \\n임베딩 모델의 의미 표현 능력 : 임베딩 모델이 텍스트의 의미를 제대로 파악하지 못해, 검색된 정보의 관련성이 낮아진다. 이는 중요한 정보를 놓칠 수 있게 된다. \\n\\n2. Retrieval\\n\\n제한된 검색 알고리즘: 키워드, 의미, 벡터 검색을 결합하지 않은 등 다양한 검색이나 알고리즘의 통합이 제한적이며, 이는 검색 결과의 다양성과 정확성을 저하시킨다. \\n쿼리 및 임베딩 모델의 한계 : 쿼리가 부족하거나 임베딩 모델의 의미 표현 성능이 낮아 유용한 정보를 검색하지 못한다.\\n답변 정보 중복 : 여러 검색된 컨텍스트가 유사한 정보를 포함하여 생성된 답변에 반복적인 내용이 포함된다.\\n\\n3. Generation\\n\\n잘못된 응답 생성 : LLM이 관련 없거나 편향된 응답을 생성할 가능성이 높다. \\n잘못된 정보 제공 : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. \\n\\n\\n💡 Advanced RAG\\nAdvanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 하지만 여기에서 고려해야 할 것은 어떠한 방식으로 데이터 소스에서 관련 문서를 효율적으로 검색하는 것이며, 아래와 같은 사항들을 해결해야 한다.\\n\\n문서와 쿼리의 semantic representation의 정확성을 어떻게 극대화할 수 있을까?\\n쿼리와 문서(청크)의 semantic space를 어떻게 align 시킬수 있을까? \\nRetrieval의 출력을 LLM의 선호도에 맞게 조정할 수 있는 방법이 무엇일까? \\n\\n위의 세 가지 고찰을 보완하기 위해 Pre-Retreival 및 Post-Retrieval를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 이들의 역할에 대해 알아보자.\\n1. Pre-Retrieval\\n주요 목적은 색인 구조와 사용자 쿼리를 개선하는 것이다.\\n\\n데이터 품질 향상 : 엔터티와 용어의 모호성을 제거하고, 사실 정확성을 위해 문맥 유지 및 새로운 정보를 업데이트 한다. \\n인덱스 구조 최적화 : 청크 크기를 최적화하여 문맥을 일정화하고 엔터티 간의 관계를 포착하기 위해 그래프 구조의 정보를 추가한다. \\n메타데이터 추가 : dates, chapters, subsections, purposes 등과 같은 관련 정보를 청크에 메타데이터로 추가하여 데이터 필터링을 개선한다. \\n청크 최적화 : 외부 데이터 소스/문서를 사용하여 RAG 파이프라인을 구축할 때, 청크를 더 작은 조각으로 나누어 세부적인 특성을 추출한다. 그런 다음 청크를 임베딩하여 내포하고 있는 의미를 도출한다. \\n\\n2. Retrieval\\n청크 크기가 결정된 후, 임베딩하게 된다. 해당 단계에서는 쿼리와 임베딩된 청크 간의 유사성을 계산하여 가장 관련성 높은 청크를 식별하게된다. 여기서 쿼리와 청크에 사용되는 임베딩 모델을 최적화할 수 있다.\\n\\nDomain Knowledge Fine-Tuning : 임베딩 모델이 각 도메인별 정보를 정확하게 포착할 수 있도록,  도메인 특화 데이터셋을 사용하여 fine-tuning한다. 이를 위한 데이터셋에는 쿼리, 코퍼스 및 관련 문서가 포함되어야 한다. \\nDynamic Embedding : 단어가 등장하는 맥락에 맞춰 임베딩을 fine-tuning하는 방식이다. 이는 각 단어에 대해 하나의 벡터만을 사용하여 각 토큰 당 정해진 임베딩을 리턴하는 방식이 아니라 주변 단어에 따라 맥락을 고려할 수 있는 BERT를 사용하는 것과 같다. \\n\\n3. Post-Retrieval\\n관련된 정보(청크)들을 Vector Database 내에서 검색한 후, 쿼리와 함께 LLM에 입력된다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. 이러한 문제를 극복하기 위해 사용되는 방법에 대해 간단하게 알아보자.\\n\\nReranking : 검색된 정보를 재순위하여 가장 관련성 높은 답변을 우선시한다. LLM에 입력이 추가될 때, 성능이 저하되는 경우가 발생한다. 이에 검색된 청크를 재정렬하고 Top-K 가장 관련성 높은 청크를 식별하여 LLM에 사용할 컨텍스트로 제공한다. \\nPrompt Compression : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.  \\n\\n\\n💡Modular RAG\\n\\b앞서 알아보았던 Advanced RAG는 Naive RAG의 컴포턴트를 조금씩 보완한 형태라면, Modular RAG는 조금 더 나아가 다양하고 유연한 구조를 지향한다. 이러한 구조는 RAG의 전반적인 성능을 향상 시켰으며, 현재 어플리케이션을 구축할 때 표준 패러다임이 되었다. 그럼 몇 가지 모듈에 대해 알아보도록 하자.\\n\\nSearch Module : 임베딩 유사도 기반 검색 외에도 추가적인 검색 시나리오를 가능하게 한다. 즉 Search Module은 특정 시나리오에 맞춰 LLM이 생성한 코드나 SQL 등을 사용하여 검색을 수행하는 모듈이다. 이에 다양한 데이터 소스를 사용할 수 있다는 점이다.\\nMemory Module : LLM이 벡터 데이터베이스에서 검색된 청크뿐만 아니라 시스템 메모리에 저장된 이전 쿼리와 결합하여 현재 입력과 가장 유사한 답변을 찾는 모듈이다. \\nFusion Module : 유저의 의도를 정확하게 반영하지 않을 수도 있다는 차관에서 비롯되었다. LLM을 통해 유저의 쿼리로 부터 여러 개의 가상 쿼리를 생성하여 검색하는 방식이다. \\n\\n\\nFusion Model (RAG-Fusion)\\n위의 모듈뿐만 아니라 다양한 모듈이 존재하니, 이에 대해 궁금하시다면 링크 참고하시길 바랍니다.\\n공유하기\\n게시글 관리\\n굴러가는 분석가의 일상\\n'LLM' 카테고리의 다른 글\\nRetrieval-Augmented Generation 이란?  (0)2024.07.21\\n'LLM' Related Articles\\n\\nRetrieval-Augmented Generation 이란? 2024.07.21\\n\\nmore\\n\\nBlog is powered by kakao / Designed by Tistory\\n티스토리툴바\\n굴러가는 분석가의 일상구독하기\"}, {\"title\": \"Naive RAG, Advanced RAG, Modular RAG : 네이버 ... - 네이버 블로그\", \"url\": \"https://blog.naver.com/PostView.naver?blogId=jangjw01088&logNo=223650933570\", \"content\": \"Advanced RAG. Advanced RAG는 Naive RAG의 문제점을 해결하기 위해 여러 가지 전략을 도입한 방식입니다. 주로 검색 성능을 향상시키고, 검색된 정보가 LLM에 더 잘 통합될 수 있도록 하는 기술이 포함됩니다. ... 주요 모듈은 다음과 같습니다. 검색 모듈 : 벡터DB 외에도\", \"score\": 0.74161834, \"raw_content\": null}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"title\": \"12화 복잡한 RAG 분해하기-Modular RAG란? - 브런치\", \"url\": \"https://brunch.co.kr/@harryban0917/338\", \"content\": \"모듈러 RAG는 복잡한 RAG 시스템을 마치 레고 블록처럼 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 줍니다. 모듈러 RAG는 각 모듈을 독립적으로 설계하고 이를 필요에 따라 교체하거나 결합할 수 있도록 합니다. 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다. 예를 들어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고, 이를 조합함으로써 효율적인 RAG 시스템을 구성할 수 있습니다. 또한, 모듈러 RAG는 각 모듈이 독립적으로 유지보수되고 개선될 수 있기 때문에, 시스템의 유지 관리가 용이하고, 새로운 기술이나 방법론을 도입하는 데에도 유리하지요. 예를 들어, 특정 데이터 소스에 더 적합한 벡터 DB를 찾거나, 최신의 임베딩 모델을 도입하는 등의 작업을 통해 성능을 지속적으로 개선할 수 있습니다.\", \"score\": 0.87348515, \"raw_content\": \"12화 복잡한 RAG 분해하기-Modular RAG란?\\n메뉴\\nbrunch\\n연재 중 알바트로스의 생성형 AI 연구소 12화\\n실행\\n신고\\n라이킷 24 댓글 공유\\n닫기\\nYou can make anything\\nby writing\\nC.S.Lewis\\n브런치스토리 시작하기\\n\\n브런치스토리 홈\\n브런치스토리 나우\\n브런치스토리 책방\\n\\n\\n계정을 잊어버리셨나요?\\n복잡한 RAG 분해하기-Modular RAG란?\\nRAG의 성능을 개선하는 비법, 모듈화\\nby 알바트로스 Nov 30. 2024\\n지금까지 검색증강생성(RAG, Retrieval Augmented Generation)을 구성하는 핵심적인 요소들에 대해 배워보았습니다. RAG 파이프라인을 구성하는 요소인 임베딩 모델과 벡터 DB 그리고 최종적인 LLM 답변의 전체적인 톤 앤 매너와 제약사항 등을 통해 답변을 제어하는 시스템 프롬프트(System Prompt)까지 검색증강생성 파이프라인에는 정말 다양한 요소들이 있었습니다.\\n여태껏 배운 것들을 정리해 봅시다. 생성형 AI도 구글링을 한다? 에서 살펴보았던 것처럼 결국 RAG는 LLM이 양질의 정보를 '검색'하기 위한 일종의 파이프라인입니다. LLM에게 검색하게 하고자 하는 텍스트 데이터를 질서 정연하게 인덱싱(Indexing) 및 청킹(Chunking) 해준 뒤에 '임베딩 모델'에 제공해 주면 Token ID와 함께 숫자로 된 벡터들을 얻을 수 있습니다. 그 벡터들을 chromaDB나 Elasic Search와 같은 여러 벡터 DB에 저장해 두었다가 LLM이 프롬프트와 함께 관련된 질문을 받으면 꺼내다 쓸 수 있도록 해주는 것이지요.\\n출처 : Freepiks\\n이때 RAG 파이프라인의 각 요소들은 하나의 시스템으로 유기적으로 작용하는 것처럼 보이지만 각각의 요소를 떼어놓고 살펴보면 마치 레고 블록처럼 분리하고 갈아 끼울 수도 있습니다. 이번에 새로 나온 어느 '임베딩 모델'이 좋다고 하면 해당 부분만 갈아 끼울 수도 있고, 우리 회사의 데이터에 더 적합한 '벡터 DB'를 발견하면 마찬가지로 그 부분만 교체할 수도 있지요.\\n이러한 블록 작업들을 용이하게 하고, 보다 뛰어난 성능을 구현해 내는 방법으로 제안된 모듈러 RAG라는 개념이 있습니다. 이번 시간에는 Modular RAG: A Reconfigurable Framework for Retrieval-Augmented Generation라는 논문을 리뷰하며 모듈러 RAG의 개념에 대해 소개호도록 하겠습니다.\\n1. 복잡한 RAG 시스템 분해하기 : 모듈러 레그(Modular RAG)란?\\n지난 몇 주에 걸쳐 RAG 파이프라인을 구성하는 임베딩 모델, 벡터 DB, 그리고 LLM의 최종 답변을 제어하는 시스템 프롬프트까지, 이 모든 요소들이 어떻게 유기적으로 작동하는지에 대해 살펴봤습니다. 그런데 이러한 요소들이 각각 독립적으로 존재하면서도 쉽게 교체될 수 있다는 사실을 알고 계셨나요?\\n이번에 소개할 '모듈러 RAG'는 바로 이러한 교체 가능성을 극대화하고, 성능을 더욱 향상할 수 있는 새로운 접근법입니다. 모듈러 RAG는 복잡한 RAG 시스템을 마치 레고 블록처럼 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 줍니다. 이로 인해 시스템의 유연성은 물론, 확장성까지 한층 강화됩니다.\\n2. 모듈러 RAG의 개념과 구성 요소\\n\\n모듈러 RAG는 RAG 시스템을 여러 개의 독립적인 모듈로 나누어 구성하는 방식을 제안합니다. 이를 통해 각 모듈은 다른 모듈과 독립적으로 동작하면서도, 서로 필요한 정보를 교환하며 전체 시스템을 구성할 수 있습니다. 여기서 핵심은 '모듈화'입니다. 각 모듈은 특정한 역할을 담당하며, 해당 모듈을 다른 모듈로 교체하거나 조합함으로써 다양한 작업을 처리할 수 있습니다.\\n기존 RAG(Naive RAG) vs Advanced RAG vs 모듈러 RAG (출처 : Modular RAG: A Reconfigurable Framework...)\\n모듈러 RAG는 크게 세 가지 주요 수준으로 나누어집니다. 첫 번째는 모듈로, RAG 시스템의 핵심 프로세스를 담당합니다. 예를 들어, 데이터 인덱싱, 검색 전후 처리, 생성 과정 등이 모듈에 해당합니다. 두 번째는 하위 모듈로, 각 모듈 내의 세부 기능을 세분화하고 최적화하는 역할을 합니다. 예를 들어, 쿼리 확장이나 검색기 선택 등이 하위 모듈에 해당합니다. 세 번째는 연산자로, 하위 모듈 내에서 실제 기능을 수행하는 기본 단위입니다. 예를 들어, 슬라이딩 윈도나 메타데이터 첨부 같은 기능이 연산자에 해당합니다.\\n3. 모듈러 RAG의 작동 원리와 장점\\n\\n모듈러 RAG는 각 모듈을 독립적으로 설계하고 이를 필요에 따라 교체하거나 결합할 수 있도록 합니다. 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다. 이렇게 모듈화 된 시스템은 단순히 기존의 시스템을 확장하거나 개선하는 것에 그치지 않고, 다양한 시나리오에 맞게 빠르게 적응할 수 있습니다.\\n모듈러 RAG는 또한 동적이고 적응 가능한 시스템을 가능하게 만듭니다. 예를 들어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고, 이를 조합함으로써 효율적인 RAG 시스템을 구성할 수 있습니다. 또한, 모듈러 RAG는 각 모듈이 독립적으로 유지보수되고 개선될 수 있기 때문에, 시스템의 유지 관리가 용이하고, 새로운 기술이나 방법론을 도입하는 데에도 유리하지요.\\n모듈러 RAG의 가장 큰 장점은 바로 유연성과 확장성입니다. 특정 작업에 맞는 모듈을 자유롭게 선택하고 교체함으로써 시스템을 최적화할 수 있고, 새로운 기술이나 방법론을 빠르게 적용할 수 있습니다. 또한, 각 모듈은 독립적으로 유지 관리되기 때문에, 시스템 전체의 안정성을 높이는 데에도 유리합니다.\\n또한, 모듈러 RAG는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다. 예를 들어, 특정 데이터 소스에 더 적합한 벡터 DB를 찾거나, 최신의 임베딩 모델을 도입하는 등의 작업을 통해 성능을 지속적으로 개선할 수 있습니다. 이는 빠르게 변화하는 기술 환경에 적응하는 데 큰 이점이 됩니다.\\n모듈러 RAG는 기존의 RAG 시스템을 보다 유연하고 확장 가능한 형태로 발전시킨 혁신적인 접근법입니다. 이를 통해 각 요소를 독립적으로 조정하고 최적화할 수 있어, 다양한 작업 시나리오에 맞는 맞춤형 시스템을 구축할 수 있습니다. 모듈러 RAG는 계속해서 발전하는 AI 환경에서 더 나은 성능과 효율성을 제공합니다.\\n4. 모듈러 RAG를 통해 바라본 생성형 AI 발전 트렌드\\n이처럼 생성형 AI 기술의 최신 트렌드는 수직적인 시스템 구조에서 탈피해 극도의 '유연성'을 추구하는 방향으로 나아가고 있습니다. 생성형 AI는 수개월 주기로 워낙 기술이 빠르게 바뀌고 있는 분야이기 때문에, 각 요소를 개별적으로 조정하고 교체할 수 있는 유연한 시스템은 필수적이기 때문이죠.\\n예를 들어 RAG의 각 구성 요소를 활용할 수 있는 대표적인 라이브러리인 랭체인(Langchain)과 랭그래프(Langgraph)를 보아도 알 수 있습니다. 다음 시간에는 랭체인과 랭그레프의 기초 개념과 왜 랭그래프가 최근 랭체인보다 더 각광받고 있는지 그 이유에 대해 생각해 볼 수 있는 시간을 가져보도록 하겠습니다.\\n\\nkeyword\\n\\nAI\\n인공지능\\n챗GPT\\n\\nBrunch Book 토요일 연재\\n연재 알바트로스의 생성형 AI 연구소\\n\\n09 RAG를 빠르고 정확하게 해주는 인덱싱과 청킹\\n10 벡터 DB는 기존의 데이터베이스와 어떻게 다를까?\\n11 대표적인 벡터 DB에는 어떤 것들이 있을까?\\n최신글12 복잡한 RAG 분해하기-Modular RAG란?\\n13 13화가 곧 발행될 예정입니다. 2024년 12월 07일 토요일 발행 예정\\n\\n전체 목차 보기\\n알바트로스 IT 분야 크리에이터 소속 직업 에세이스트 \\n문과 출신으로 생성형 AI 엔지니어로 일하고 있습니다. 챗GPT와 같은 AI가 보다 더 사람의 말을 잘 알아듣고 생성할 수 있도록 연구하는 NLP(자연어처리) 분야에서 일합니다.\\n구독자 425 제안하기 구독\\n\\n이전 11화 대표적인 벡터 DB에는 어떤 것들이 있을까?\\n브런치 로그인\\n브런치\\n\\n 브런치스토리 작가로 데뷔하세요. 진솔한 에세이부터 업계 전문 지식까지, 당신의 이야기를 세상에 선보이세요.\\n 브런치스토리로 제안받는 새로운 기회 다양한 프로젝트와 파트너를 통해 작가님의 작품이 책·강연 등으로 확장됩니다.\\n 글로 만나는 작가의 경험 작가를 구독하고, 새 글을 받아보세요. 당신에게 영감을 주는 작품을 추천합니다.\\n\\n이전\\n\\n1\\n2\\n3\\n\\n다음\\n브런치스토리 시작하기 카카오계정으로 로그인\\n내 브런치스토리 찾기 내 브런치스토리의 카카오계정을 모르겠어요 페이스북·트위터로만 로그인 했었나요? 로그인 관련 상세 도움말\\n창 닫기\\n브런치는 최신 브라우저에 최적화 되어있습니다. IE chrome safari\\n작가 구독을 취소하시겠습니까?\\n작가의 글을 더 이상 구독하지 않고,\\n새 글 알림도 받아볼 수 없습니다.\\n계속 구독하기 구독 취소하기\"}, {\"title\": \"모듈형 Rag: 대규모 언어 모델(Llm)과의 통합 및 작동 방식\", \"url\": \"https://digitalbourgeois.tistory.com/311\", \"content\": \"선형 프로세스의 한계: 전통적인 RAG 시스템은 검색 후 생성하는 단일 흐름으로 구성되어 있어 복잡한 질의나 다양한 데이터 소스를 처리하는 데 적합하지 않습니다. 모듈형 RAG는 RAG의 아키텍처를 모듈 단위로 나눔으로써 재구성할 수 있도록 설계되었습니다. 3. 모듈형 RAG의 3계층 아키텍처 이러한 구조 덕분에 모듈형 RAG는 지속적인 성능 개선과 확장이 가능하며, AI 작업의 요구 사항 변화에 빠르게 대응할 수 있습니다. 전통적인 RAG와 유사하지만, 모듈형 RAG는 각 모듈이 독립적으로 최적화되거나 교체될 수 있다는 점에서 차별화됩니다. 예를 들어, 질의가 복잡하거나 검색된 데이터의 양이 많은 경우, 시스템은 추가적인 쿼리 재작성이나 데이터 압축 모듈을 활성화하여 더 적합한 응답을 생성할 수 있습니다. 모듈 단위로 시스템을 구성함으로써, 각 작업에 맞춤형으로 최적화된 RAG 시스템을 만들 수 있습니다. 모듈형 RAG는 기존 RAG의 한계를 극복하고, AI 시스템의 유연성 및 확장성을 크게 강화한 혁신적인 기술입니다.\", \"score\": 0.834852, \"raw_content\": \"모듈형 RAG: 대규모 언어 모델(LLM)과의 통합 및 작동 방식\\n본문 바로가기\\n평범한 직장인이 사는 세상\\n메뉴\\n\\n분류 전체보기 (525)  \\n인공지능 (121)  \\nTensorflow (5)\\n\\n\\n빅데이터 (30)\\n알고리즘 (32)\\nKubernetes (10)\\nAWS (14)\\nJAVA (32)\\nAPI (9)\\nMSA(마이크로서비스) (6)\\n아키텍처 (4)\\nSpring (50)\\nDB (17)\\n보안 (9)\\nPython (8)\\nDevOps (30)\\nPlatform Engineering (3)\\n리눅스 (1)\\n커피 글자국 (34)\\n경제 글자국 (17)\\n여행 글자국 (11)\\n육아 글자국 (2)\\n맛집 글자국 (39)  \\n잡학다식 (36)\\n세미나 (1)\\n서평 (15)\\nIT 컬럼 (9)\\n소프트웨어 교육 (3)\\nTools (1)\\n\\n\\nWEB (3)\\n\\n\\n\\nPOWERED BY TISTORY\\n블로그 내 검색  검색\\n\\n인공지능\\n모듈형 RAG: 대규모 언어 모델(LLM)과의 통합 및 작동 방식\\n파파누보 2024. 9. 8. 22:54\\n728x90\\n반응형\\n\\n**대규모 언어 모델(LLM)**은 최근 몇 년간 AI 기술의 급속한 발전을 이끌어 왔습니다. 하지만 LLM은 여전히 몇 가지 중요한 한계를 가지고 있습니다. 가장 대표적인 문제가 환각(모델이 존재하지 않는 정보를 생성하는 것)과 업데이트된 정보 부족입니다. 이 문제들을 해결하기 위한 중요한 도구로 **RAG(Retrieval-Augmented Generation, 검색-생성 방식)**가 등장했습니다. RAG는 LLM에 외부 지식을 추가함으로써 성능을 크게 향상시켰지만, 기존 RAG 시스템은 여전히 복잡한 질의나 다양한 데이터 소스를 처리하는 데 한계가 있었습니다.\\n여기서 **모듈형 RAG(Modular RAG)**가 새로운 해결책으로 주목받고 있습니다. 이 시스템은 레고 블록처럼 모듈화되어, 유연하고 재구성 가능한 구조를 통해 AI의 복잡한 과제에 적응할 수 있도록 설계되었습니다. 이번 포스팅에서는 모듈형 RAG가 어떻게 전통적인 RAG를 발전시켰는지와 그 작동 방식을 상세히 설명하겠습니다.\\n1. 전통적인 RAG 시스템의 이해\\n1.1 RAG의 기본 개념\\n**RAG(Retrieval-Augmented Generation)**는 LLM의 한계를 보완하기 위해 설계된 시스템입니다. RAG는 외부 데이터베이스에서 정보를 검색하여, 그 정보를 바탕으로 텍스트를 생성합니다. 이 방식은 간단하지만, 다양한 응용 분야에서 탁월한 성능을 발휘합니다. RAG는 복잡한 질문에 대한 답변 제공, 맞춤형 추천, 고객 지원 등에 사용될 수 있습니다.\\n전통적인 RAG 시스템은 사용자의 질의에 대해 데이터베이스에서 가장 관련 있는 정보를 검색하고, 이를 바탕으로 답변을 생성하는 방식으로 작동합니다. 초기 RAG 시스템은 주로 유사성 측정에 의존하여 질의와 텍스트 조각 간의 일치 여부를 판단했습니다. 이러한 방식은 간단한 질의에는 적합했지만, 데이터의 양과 복잡성이 증가하면서 한계가 드러나기 시작했습니다.\\n1.2 전통적 RAG 시스템의 문제점\\n전통적인 RAG 시스템은 여러 가지 한계를 가지고 있습니다.\\n\\n얕은 질의 이해: 전통적인 RAG는 질의와 문서 조각 간의 단순한 유사성 매칭에 의존하기 때문에, 질의의 미묘한 차이를 이해하지 못하는 경우가 많습니다. 이는 복잡한 질문에 대한 부정확한 응답을 초래할 수 있습니다.\\n검색 중복성과 잡음: 모든 검색된 정보를 LLM에 바로 제공하는 경우, 중복되거나 관련성이 낮은 정보가 많아질 수 있습니다. 이는 연산 자원의 낭비뿐만 아니라 잘못된 응답을 생성할 위험도 높입니다. 불필요한 정보를 제거하고 중요한 데이터를 유지하는 것이 큰 과제가 됩니다.\\n선형 프로세스의 한계: 전통적인 RAG 시스템은 검색 후 생성하는 단일 흐름으로 구성되어 있어 복잡한 질의나 다양한 데이터 소스를 처리하는 데 적합하지 않습니다. 일부 고급 RAG 방법은 쿼리 재작성이나 재랭킹 기술을 도입했지만, 여전히 유연성이 부족한 구조로 인해 한계를 보였습니다.\\n\\n반응형\\n2. 모듈형 RAG의 출현\\n2.1 모듈형 RAG의 도입\\nAI 응용 프로그램이 더욱 복잡해짐에 따라 전통적인 \\\"검색 후 생성\\\" 방식의 한계가 분명해졌습니다. 현대 AI 응용 프로그램은 더욱 유연하고 확장 가능한 프레임워크를 요구하게 되었으며, 이 요구에 맞춰 등장한 것이 바로 모듈형 RAG입니다.\\n모듈형 RAG는 RAG의 아키텍처를 모듈 단위로 나눔으로써 재구성할 수 있도록 설계되었습니다. 마치 레고 블록처럼 각 모듈이 독립적으로 최적화되며, 필요에 따라 결합하여 다양한 작업에 유연하게 대응할 수 있는 시스템입니다. 이는 복잡한 작업에서도 고정된 구조에 얽매이지 않고 효율적으로 적응할 수 있는 시스템을 제공합니다.\\n3. 모듈형 RAG의 3계층 아키텍처\\n모듈형 RAG의 핵심은 3계층 아키텍처로 구성된 구조입니다. 이 구조는 시스템을 보다 유연하고 세부적으로 조정할 수 있도록 합니다.\\n3.1 모듈 계층\\n모듈형 RAG의 최상위 계층에서는 모듈이 독립적인 단계로 나뉘어 있습니다. 이러한 모듈들은 검색, 생성, 오케스트레이션과 같은 주요 프로세스를 처리하며, 각 단계는 독립적으로 제어될 수 있습니다.\\n3.2 하위 모듈 계층\\n모든 모듈에는 하위 모듈이 존재하여 더 구체적인 작업을 수행합니다. 예를 들어, 검색 모듈 내에는 쿼리 확장, 재랭킹, 필터링을 담당하는 하위 모듈들이 있습니다. 이 하위 모듈들을 통해 RAG 프로세스는 더욱 정교하게 제어됩니다.\\n3.3 연산자 계층\\n연산자는 모듈 내에서 가장 기본적인 작업을 수행하는 단위입니다. 예를 들어, 임베딩 생성, 유사성 계산, 문서 조각화 등의 작업이 연산자 단위에서 이루어집니다. 연산자 계층 덕분에 새로운 기능을 추가하거나 성능을 개선할 때 전체 시스템을 수정할 필요 없이 개별적으로 조정이 가능합니다.\\n4. 모듈과 연산자의 상호작용\\n모듈과 연산자의 상호작용은 모듈형 RAG의 핵심 강점입니다. 이 시스템은 각 모듈이 독립적이면서도 유기적으로 연결되어 있어 복잡한 작업에 대해 높은 유연성과 확장성을 제공합니다. 예를 들어, 특정 작업이 더 세밀한 검색을 요구하는 경우, 오케스트레이션 모듈은 재랭킹이나 쿼리 변환 연산자를 우선적으로 활성화할 수 있습니다.\\n또한, 새로운 연산자가 등장하면 기존 시스템을 완전히 개편하지 않고도 손쉽게 추가할 수 있습니다. 이러한 구조 덕분에 모듈형 RAG는 지속적인 성능 개선과 확장이 가능하며, AI 작업의 요구 사항 변화에 빠르게 대응할 수 있습니다.\\n728x90\\n5. 모듈형 RAG에서 자주 사용되는 흐름 패턴\\n5.1 선형 패턴\\n선형 패턴은 가장 기본적인 RAG 흐름입니다. 모듈들이 고정된 순서로 배치되어 있어, 검색 후 생성 과정을 차례대로 진행합니다. 전통적인 RAG와 유사하지만, 모듈형 RAG는 각 모듈이 독립적으로 최적화되거나 교체될 수 있다는 점에서 차별화됩니다.\\n5.2 조건부 패턴\\n조건부 패턴은 특정 조건에 따라 RAG 흐름을 분기시키는 방식입니다. 예를 들어, 질의가 복잡하거나 검색된 데이터의 양이 많은 경우, 시스템은 추가적인 쿼리 재작성이나 데이터 압축 모듈을 활성화하여 더 적합한 응답을 생성할 수 있습니다.\\n5.3 반복 패턴\\n반복 패턴은 복잡한 질의에 대해 여러 번의 검색과 생성을 반복하여 점진적으로 더 정확한 응답을 생성하는 방식입니다. 초기 응답이 충분하지 않을 경우, 다시 검색 모듈로 돌아가 추가 데이터를 검색하고 이를 바탕으로 최종 응답을 개선하는 방식으로 작동합니다.\\n6. 모듈형 RAG의 장점과 도전 과제\\n6.1 모듈형 RAG의 장점\\n모듈형 RAG의 가장 큰 장점은 유연성과 확장성입니다. 모듈 단위로 시스템을 구성함으로써, 각 작업에 맞춤형으로 최적화된 RAG 시스템을 만들 수 있습니다. 또한, 특정 작업이나 데이터 유형에 맞춰 모듈을 교체하거나 새롭게 추가할 수 있어, 다양한 AI 응용 프로그램에 적합한 솔루션을 제공합니다.\\n6.2 도전 과제\\n모듈형 RAG의 도입에는 몇 가지 도전 과제가 따릅니다. 복잡한 데이터 통합이 그 중 하나입니다. 전통적인 RAG는 주로 비정형 텍스트 데이터를 다루지만, 모듈형 RAG는 표 형식의 데이터나 지식 그래프와 같은 반구조화 및 구조화된 데이터도 통합해야 합니다. 이와 함께 시스템 오케스트레이션 및 워크플로우 관리도 큰 과제입니다. 각 모듈 간의 데이터 흐름과 작업 관리가 더욱 복잡해질 수 있기 때문입니다.\\n모듈형 RAG는 기존 RAG의 한계를 극복하고, AI 시스템의 유연성 및 확장성을 크게 강화한 혁신적인 기술입니다. 데이터 통합, 시스템 오케스트레이션, 모듈 상호작용 최적화 등의 과제가 있지만, 이를 해결함으로써 AI 기술의 미래를 더욱 밝게 열어갈 수 있을 것입니다.\\n728x90\\n반응형\\n공유하기\\n게시글 관리\\n평범한 직장인이 사는 세상\\n저작자표시 비영리 변경금지\\n'인공지능' 카테고리의 다른 글\\nAI가 '생각'하기 시작했다? OpenAI o1의 혁신  (0)2024.09.14Langchain으로 인공지능 챗봇 만들기: 메모리를 활용한 자연스러운 대화 유지 비법  (0)2024.09.09효율적인 정보 검색을 위한 솔루션, Langchain의 Retriever란?  (0)2024.09.05Langchain Document Transformers: AI 문서 처리의 핵심 기술 알아보기  (0)2024.09.04Hugging Face: 인공지능 개발자들의 필수 도구, 왜 주목받고 있을까?  (0)2024.09.04\\n태그\\nllm, Modular, modular rag, modular rag란 무엇인가?, rag 트렌드, rag란, 모듈형 rag\\n'인공지능' Related Articles\\n\\nAI가 '생각'하기 시작했다? OpenAI o1의 혁신\\nLangchain으로 인공지능 챗봇 만들기: 메모리를 활용한 자연스러운 대화 유지 비법\\n효율적인 정보 검색을 위한 솔루션, Langchain의 Retriever란?\\nLangchain Document Transformers: AI 문서 처리의 핵심 기술 알아보기\\n\\n\\nDESIGN BY TISTORY 관리자\\n티스토리툴바\"}, {\"title\": \"대규모 언어모델(Llm)의 환각현상을 줄이는 Rag(검색증강생성) 모델의 동작원리와 3가지 패러다임\", \"url\": \"https://m.blog.naver.com/buksamfight/223388544425\", \"content\": \"RAG의 활용 . 생성형 AI 시스템이 외부 소스를 이용하여 보다 정확한 상황 인식 응답을 만듦 ... 장점. 단점. 순위 다시 매기기 ... 다. 모듈형(Modular) RAG - 다양한 모듈과 기능을 통합하여 다양한 시나리오와 요구 사항에 맞게 조정할 수 있도록 하는 여러 새로운\", \"score\": 0.82895297, \"raw_content\": \"블로그\\n카테고리 이동\\n정보처리기술사/정보관리기술사/빅데이터분석/ADSP\\n대규모 언어모델(LLM)의 환각현상을 줄이는 RAG(검색증강생성) 모델의 동작원리와 3가지 패러다임\\n2024. 3. 19. 20:20\\n정보 검색(Information Retrieval, IR)이란!?\\n지식 소스나 데이터 세트에서 관련 정보를 조회하고 추출하는 프로세스\\n예) 검색 엔진을 이용하여 인터넷에서 정보를 찾는 것\\n​\\n사용자가 쿼리를 입력하면 시스템은 사용자가 찾고 있는 정보를 포함할 가능성이 가장 높은 문서나 웹 페이지를 질의하여 응답\\n- 대규모 언어모델은 이러한 정보질의를 이용해 학습 데이터를 수집 및 학습에 적용\\n​\\n대규모 언어모델의 한계와 RAG의 의의\\n대규모 언어모델(Large Language Model, LLM)의 개념\\n사람들의 언어(자연어)를 학습하여 실제인간과 유사한 문장을 생성 하기 위한 언어모델\\n점차 규모가 커지며 초거대 AI로 진화\\n순차 데이터의 컨텍스트를 학습할 수 있는 신경망인 트랜스포머(Transformer)를 통해 비약적인 성능 발전\\n​\\n대규모 언어모델의 한계\\n환각이나 느린 지식의 업데이트, 답변의 투명성 부족 등과 같은 문제점 존재\\n지식의 시대에 뒤떨어짐, 특정 영역에 대한 지식 부족, 그리고 응답의 투명성 부족\\n​\\n- RAG를 이용해 답변의 정확도를 크게 향상시키고, 특히 지식 집약적인 작업에서 모델의 환각을 줄여주며, 사용자가 출처를 인용하여 답변의 정확성을 검증할 수 있으며, 이는 결과 출력에 대한 신뢰를 증가시킬 수 있음\\n​\\n​\\n대규모 언어모델의 한계 극복, RAG의 정의와 활용\\nRAG의 정의\\n대규모 언어 모델에서 질문에 대한 답변이나 텍스트를 만들기 전에 광범위한 문서 집합에서 관련 정보를 질의하고, 이를 이용하여 응답을 생성하는 방법\\n검색기반과 생성모델, 이 두 접근 방식을 병합하여 특정하면서도 동적으로 답변을 제공하며, 자연어 처리(NLP) 분야에서 신경망 정보 검색과 신경망 텍스트 생성을 결합한 고급 기술\\n​\\nRAG의 활용\\n생성형 AI 시스템이 외부 소스를 이용하여 보다 정확한 상황 인식 응답을 만듦\\n질문 답변 및 콘텐츠를 새로이 만드는 작업에 유용\\n시맨틱 이나 하이브리드와 같은 방법을 구현하여 의도에 응답하고 보다 정확한 결과를 제공\\n​\\n-\\n파인튜닝(fine-tuning)은 대규모 언어모델이 특정 지식에 집중 할 수 있도록 구현 가능 하나, 실시간으로 업데이트되는 정보를 반영하는데 시간이 걸리며, 새로 학습시키는데 추가적인 자원이 소모 됨\\n​\\nRAG의 동작 원리와 핵심 요소\\n동작 원리 상세 설명\\n- 이미 학습된 생성형 AI의 한계를 외부 데이터베이스로 이용하여 관련성이 높은 Data를 증강해 더 적확한 정보를 제시 하기 위해 연관 데이터가 들어있는 데이터베이스를 연결하고,\\n- 제품이나 서비스에 대한 특정 Data를 활용해 범용 LLM을 단독으로 쓰는 것보다 더 정확한 답변을 얻는 게 목적\\n​\\nRAG의 3가지 주요 패러다임\\n기본/고급/모듈형 유형 존재\\n가. 기본 RAG\\n핵심 요소 설명\\n인덱싱(Indexing) : 데이터 소스에서 데이터를 얻고 인덱스를 생성하는 과정(데이터 정제, 청크 분할, 벡터 인코딩 및 인덱스를 포함)\\n검색 모델(Retriever Model): 주어진 쿼리나 작업과 매치되는 문맥적으로 연관된 Data 찾기 위해 큰 텍스트 데이터베이스(예: 문서, 문단, 또는 문장)를 질의하는 컴포넌트\\n생성 모델(Generator Model): 모델이 Data를 입력으로 받아 텍스트 응답을 생성\\n​\\n- 연관된 외부 정보를 가져와서 결합 함\\n- 데이터를 수집하고, 청크(작은 단위)로 분할하여 임베딩후 Vector DB에 저장\\n단계\\n활동\\n설명\\n1\\n프롬프트\\n- 사용자가 AI 모델에 설명과 응답 요청\\n2\\n상황별 검색\\n- 쿼리나 작업이 입력되면, 활용 가능한 지식 베이스를 조회하여 쿼리와 관련된 문단, 문서나 텍스트를 질의\\n3\\n프롬프트 증강\\n(문맥제공)\\n- 출력 된 문단들은 추가적인 문맥이나 Data로 모델에 제공\\n4\\n추론\\n(텍스트 생성)\\n- 추가 문맥을 활용하여 모델은 더 정확하고 연관된 사실과 지식에 기반한 텍스트 출력을 생성\\n5\\n응답\\n- LLM은 실제로 정확한 정보와 함께 응답을 사용자에게 제공\\n​\\n나. 고급(Advanced) RAG\\n- 기본 RAG의 부족한 점을 개선하기 위해 개발된 패러다임(3단계로 수행)\\n1) 검색 전 절차(Pre-Retrieval Process)\\n-\\n데이터 인덱싱 최적화 및 임베딩을 통해서 결과의 연관성과 정확성을 높이는 데 중점을 둠\\n데이터 인덱싱 최적화(Optimizing Data Indexing)\\n전략\\n설명\\n예시\\n데이터 세분화 강화\\n텍스트 표준화, 일관성, 정확성, 문맥 풍부화\\n불필요 Data 제거, 특수 문자 제거, 엔티티 및 용어 모호성 제거\\n인덱스 구조 최적화\\n정확성 및 관련성 향상\\n청크 크기 조정, 인덱스 경로 변경, 그래프 구조 도입\\n메타 Data 추가\\n효율성 및 관련성 향상\\n날짜, 목적 등 메타 Data 추가, 필터링 적용\\n정렬 최적화\\n문서 간 불일치 해결\\n가상 질문 및 문서와 결합\\n혼합 검색\\n기술 강점 활용\\n키워드 기반, 의미, 벡터 검색 결합\\n임베딩(Embedding)\\n- 질의된 내용과 질문 간의 연관성을 강화하고 최종 출력에 대한 영향을 최적화\\n- 질문과 문서 간의 의미론적 유사성을 측정하고 이를 개선\\n전략\\n설명\\n예시\\n정밀 조정 임베딩\\n내용과 질문 간 연관성 향상\\nBGE(BAAI 2023)\\n동적 임베딩\\n단어 맥락 고려\\nBERT\\n2) 검색 후 절차 (Post-Retrieval Process)\\n- 데이터베이스에서 중요한 문맥을 질의와 결합하여 LLM에 입력하는 과정\\n전략\\n설명\\n장점\\n단점\\n순위 다시 매기기\\n관련 정보 프롬프트 앞부분 배치\\n연관성 높은 Data에 우선 제공\\nLLM 컨텍스트 윈도우 크기 초과 가능성\\n프롬프트 압축\\n중요 맥락 강조, 전체 길이 줄임\\nLLM 효율성 향상, 맥락적Data 집중\\n정보 손실 가능성\\n3) RAG 파이프라인 최적화 (Pipeline Optimization)\\n- Retrieval 단계를 최적화하며, 다양한 질의 전략을 유연하게 적용하며, 임베딩 유사도를 활용하는 데 초점\\n전략\\n설명\\n예시\\n하이브리드 검색\\n다양한 기술 결합\\n키워드 기반, 의미론적, 벡터 결합\\n재귀적 검색 및 쿼리 엔진\\n효율성과 맥락적 풍부함 균형\\n초기 단계: 작은 문서 블록, 후반 단계: 큰 문서 블록\\n역추적 프롬프트\\nLLM 추론 능력 향상\\n답변 및 질문-응답 과정 적용\\n서브쿼리\\n다양한 질의 전략 적용\\n트리 쿼리, 벡터 쿼리, 순차적 청크 쿼리\\nHyDE\\n(Hypothetical Document Embeddings)\\nLLM 답변 기반 확인\\n답변 및 임베딩 후 실제 문서 확인\\n다. 모듈형(Modular) RAG\\n- 다양한 모듈과 기능을 통합하여 다양한 시나리오와 요구 사항에 맞게 조정할 수 있도록 하는 여러 새로운 모듈과 패턴을 포함\\n1) 새로운 모듈들(New Modules)\\n모듈 종류\\n설명\\n예시\\n검색\\nLLM 기반 직접 수행\\nLLM 기반 코드/SQL 등 도구 활용\\n메모리\\nLLM 메모리 활용\\nSelf-mem: 강화 개발기 및 무제한 메모리 풀 사용\\n추가 생성\\nLLM 기반 문서\\nLLM 기반 문서, 사용자 질문 직접 질의 보다 Data 증가 가능성\\n태스크 적응\\n다양한 다운스트림 작업 적응\\nUPRISE: 필요한 제로샷 프롬프트 질의\\nPROMPTAGATOR: 제로샷 쿼리 개발 LLM\\n정렬\\n질의-텍스트 정렬 문제 해결\\nPRCA, AAR, RRR 등 방법론\\n검증\\nData 신뢰성 평가\\n실제 문서-질문 관련성 평가 모듈 도입\\n2) 새로운 패턴들(New Patterns)\\n패턴\\n설명\\n예시\\n모듈 추가 또는 교체\\n특정 기능 향상 위한 모듈 도입\\nRRR: 재작성-검색-읽기 프로세스, 쿼리 조작, 읽기 모듈 성능 향상\\n모듈 간 조직적 흐름 조정\\n언어와 검색 상호작용 강화\\n특정 문제 맥락 기반 모듈 내 대체 또는 재구성\\n-->\\n더 많은 정보를 제공하고, 더 정확하고 사실에 근거한 응답을 만드는 것을 통해 기존의 LLMs보다 큰 발전에 기여\\n​\\n사례 기반, 대규모 언어모델에서의 RAG의 가치\\n- 개인화된 지원: 지식베이스와 고객 이력에 접근하여 일반적인 문의에 대한 맞춤형 응답을 제공\\n- 비용 효율성: 자동화는 대량의 요청을 처리할 수 있어 인력 비용 감소\\n- 24/7 지원: 휴식이나 교대 근무가 필요 없는 시스템은 하루 종일 고객 지원을 제공\\n등\\n​\\n​\\n[참고] AI 환각 막고 정확도 높이는 'AI레드팀'\\n생성형 AI 활용이 늘어나면서 욕설이나 환각 현상 등의 문제점을 예방하고 정확도를 높이기 위한 '...\\nblog.naver.com\\n​\\n정보관리 기술사(ITPE), 아이리포 교육센터 정보처리기술사 정규과정 멘토, 데이터 분석/인공지능 등 인재 양성 과정 강사, 빅데이터 분석기사 집필, 데이터 분석 준전문가(ADSP) 집필\\nN-001377\\n카테고리\"}]\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "모듈형 RAG(Modular RAG)와 단순 RAG(Naive RAG)의 주요 차이점은 다음과 같습니다:\n",
      "\n",
      "1. **구조적 유연성**:\n",
      "   - **모듈형 RAG**는 시스템을 여러 개의 독립적인 모듈로 나누어 각 모듈을 필요에 따라 교체하거나 조합할 수 있습니다. 이는 시스템의 유연성과 확장성을 높여줍니다.\n",
      "   - **단순 RAG**는 고정된 구조로, 검색 후 생성하는 단일 흐름으로 구성되어 있어 복잡한 질의나 다양한 데이터 소스를 처리하는 데 한계가 있습니다.\n",
      "\n",
      "2. **모듈화**:\n",
      "   - **모듈형 RAG**는 각 모듈이 특정 역할을 담당하며, 이를 통해 다양한 작업을 처리할 수 있습니다. 예를 들어, 검색 모듈, 메모리 모듈, 생성 모듈 등이 독립적으로 작동합니다.\n",
      "   - **단순 RAG**는 이러한 모듈화가 없으며, 모든 과정이 하나의 흐름으로 진행됩니다.\n",
      "\n",
      "3. **성능 최적화**:\n",
      "   - **모듈형 RAG**는 각 모듈을 독립적으로 최적화할 수 있어, 새로운 기술이나 방법론을 쉽게 도입할 수 있습니다. 예를 들어, 최신 임베딩 모델이나 벡터 DB를 교체하는 것이 용이합니다.\n",
      "   - **단순 RAG**는 성능 개선이 어려워, 시스템 전체를 수정해야 할 경우가 많습니다.\n",
      "\n",
      "4. **적응성**:\n",
      "   - **모듈형 RAG**는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 구조를 가지고 있어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고 조합할 수 있습니다.\n",
      "   - **단순 RAG**는 이러한 적응성이 부족하여, 복잡한 작업에 대한 유연한 대응이 어렵습니다.\n",
      "\n",
      "이러한 차이점들 덕분에 모듈형 RAG는 더 나은 성능과 유연성을 제공하며, 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAG와 Naive RAG의 주요 차이점을 비교한다.', '모듈형 RAG(Modular RAG)와 단순 RAG(Naive RAG)의 주요 차이점은 다음과 같습니다:\\n\\n1. **구조적 유연성**:\\n   - **모듈형 RAG**는 시스템을 여러 개의 독립적인 모듈로 나누어 각 모듈을 필요에 따라 교체하거나 조합할 수 있습니다. 이는 시스템의 유연성과 확장성을 높여줍니다.\\n   - **단순 RAG**는 고정된 구조로, 검색 후 생성하는 단일 흐름으로 구성되어 있어 복잡한 질의나 다양한 데이터 소스를 처리하는 데 한계가 있습니다.\\n\\n2. **모듈화**:\\n   - **모듈형 RAG**는 각 모듈이 특정 역할을 담당하며, 이를 통해 다양한 작업을 처리할 수 있습니다. 예를 들어, 검색 모듈, 메모리 모듈, 생성 모듈 등이 독립적으로 작동합니다.\\n   - **단순 RAG**는 이러한 모듈화가 없으며, 모든 과정이 하나의 흐름으로 진행됩니다.\\n\\n3. **성능 최적화**:\\n   - **모듈형 RAG**는 각 모듈을 독립적으로 최적화할 수 있어, 새로운 기술이나 방법론을 쉽게 도입할 수 있습니다. 예를 들어, 최신 임베딩 모델이나 벡터 DB를 교체하는 것이 용이합니다.\\n   - **단순 RAG**는 성능 개선이 어려워, 시스템 전체를 수정해야 할 경우가 많습니다.\\n\\n4. **적응성**:\\n   - **모듈형 RAG**는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 구조를 가지고 있어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고 조합할 수 있습니다.\\n   - **단순 RAG**는 이러한 적응성이 부족하여, 복잡한 작업에 대한 유연한 대응이 어렵습니다.\\n\\n이러한 차이점들 덕분에 모듈형 RAG는 더 나은 성능과 유연성을 제공하며, 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAG가 production level에서 사용하는 이점을 정리한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (call_qKVnhGoYYVK7Gg2qwqVktxZY)\n",
      " Call ID: call_qKVnhGoYYVK7Gg2qwqVktxZY\n",
      "  Args:\n",
      "    query: Modular RAG production level advantages\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"title\": \"Modular RAG using LLMs: What is it and how does it work?\", \"url\": \"https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\", \"content\": \"These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow. For example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\", \"score\": 0.6439794, \"raw_content\": \"Modular RAG using LLMs: What is it and how does it work? | by Sahin Ahmed, Data Scientist | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nMastodon\\nModular RAG using LLMs: What is it and how does it work?\\n\\nSahin Ahmed, Data Scientist\\n·Follow\\n17 min read\\n·\\nAug 14, 2024\\n\\n--\\n1\\n\\nListen\\nShare\\nIntroduction\\nLarge Language Models (LLMs) have taken AI to new heights, but they still stumble on challenges like hallucinations and outdated information. Retrieval-Augmented Generation (RAG) has been a game-changer, adding external knowledge to boost LLM performance. However, traditional RAG systems, with their rigid “retrieve-then-generate” process, often struggle with complex queries and diverse data sources.\\nThis is where Modular RAG steps in. Imagine building with LEGO blocks — each piece independent yet part of a bigger, more flexible system. Modular RAG rethinks RAG systems by breaking them down into reconfigurable modules, offering the adaptability needed for today’s AI challenges. In this post, we’ll explore how Modular RAG works and why it’s set to reshape the future of AI.\\nSection 1: Understanding Traditional RAG Systems\\n1.1 The Basics of RAG\\nRetrieval-Augmented Generation (RAG) has significantly enhanced the capabilities of Large Language Models (LLMs) by providing access to external knowledge sources. This process allows LLMs to perform better in tasks that require in-depth knowledge, such as answering complex questions, making recommendations, and supporting customer service applications. The basic idea is simple: retrieve relevant information from a vast database and then use that information to generate a response. This “retrieve-then-generate” process has been the backbone of RAG systems since their inception.\\nInitially, RAG systems were fairly straightforward, relying on basic indexing, retrieval, and generation processes. This approach, often referred to as “Naive RAG,” focused primarily on matching the semantic similarity between a user’s query and chunks of text from a database. However, as the complexity of tasks and the volume of data increased, the limitations of this simplistic approach became apparent.\\n1.2 Challenges with Traditional RAG\\nDespite the success of traditional RAG systems, they face several key challenges that limit their effectiveness in more demanding scenarios:\\n\\nShallow Query Understanding: Traditional RAG systems often struggle to fully grasp the nuances of complex queries. The reliance on basic similarity measures between a query and document chunks can result in superficial matches that fail to capture the deeper relationships within the data​.\\nRetrieval Redundancy and Noise: Feeding all retrieved chunks directly into LLMs can overwhelm the system with redundant or irrelevant information. This not only wastes computational resources but also increases the risk of generating incorrect or hallucinated responses. The challenge is to filter out noise while preserving the most relevant data​.\\nLinear Process Limitations: The linear “retrieve-then-generate” flow of traditional RAG systems doesn’t always align well with the needs of complex applications. For example, handling multifaceted queries or integrating diverse data sources often requires a more flexible, non-linear approach. Advanced RAG methods have attempted to address these issues through techniques like query rewriting and reranking, but the rigid structure still imposes significant constraints​.\\n\\nAs RAG technology continues to evolve, the growing complexity of tasks has driven the need for a more sophisticated and adaptable system — one that can break free from the limitations of a linear process and better handle the demands of real-world applications. This need has led to the development of the Modular RAG framework, which we’ll explore in the next section.\\nSection 2: The Emergence of Modular RAG\\n2.1 Introducing Modular RAG\\nAs RAG systems grew more complex, the traditional “retrieve-then-generate” approach began to show its limitations. The increasing demands of modern AI applications called for a more flexible and scalable framework — one that could adapt to the complexities of real-world tasks without being bogged down by the rigidities of a linear process. This is where Modular RAG comes in.\\nModular RAG reimagines the RAG architecture by breaking it down into independent, yet interconnected modules. Think of it as building with LEGO blocks — each module can be independently developed, optimized, and then combined with others to create a highly adaptable system. This modular design allows for a much more flexible approach, where different modules can be reconfigured or swapped out depending on the specific needs of the task at hand​.\\n2.2 The Three-Tiered Architecture of Modular RAG\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe Modular RAG framework is organized into a three-tiered architecture, each tier serving a distinct role in the overall system:\\n\\nModules: At the top level, the system is divided into high-level stages such as indexing, retrieval, and generation. Each of these stages is treated as a standalone module, which can be independently controlled and orchestrated. This approach not only inherits the main processes from traditional RAG systems but also introduces an orchestration module that manages the coordination of these processes​.\\nSub-modules: Within each module, there are sub-modules that handle more specific tasks. For example, within the retrieval module, sub-modules might handle query expansion, reranking, or filtering. These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​.\\nOperators: At the most granular level, Modular RAG includes operators, which are the basic functional units within each sub-module. These operators carry out specific functions like embedding generation, similarity computation, and document chunking. By modularizing these operators, the system can be easily reconfigured or extended with new functionality as needed​.\\n\\nSection 3: Modules and Operators in Modular RAG\\n\\n3.1 Understanding Modules in Modular RAG\\nIn the Modular RAG framework, the concept of “modules” is central to how the system is structured and operates. Each module represents a high-level stage in the RAG process, handling specific tasks that are essential for the system’s overall functionality. The key modules within this framework include:\\n\\nIndexing Module: This module is responsible for organizing and structuring the vast amounts of data that the RAG system will retrieve from. It involves breaking down documents into manageable chunks, creating embeddings, and storing these in a way that allows for efficient retrieval. One of the challenges addressed by this module is the accurate representation of content, ensuring that the system can effectively match queries with the most relevant chunks​.\\nRetrieval Module: The retrieval module plays a critical role in selecting the most relevant data chunks based on the user’s query. This module can include sub-modules for tasks like query expansion, reranking of retrieved chunks, and filtering out noise. By refining the retrieval process, this module ensures that the system returns the most useful and contextually appropriate information for subsequent generation​.\\nGeneration Module: Once the relevant data has been retrieved, the generation module comes into play. This module utilizes the retrieved data to generate the final output, whether it’s a response to a query, a summary, or any other type of content. The generation module may also include sub-modules for fine-tuning the output, ensuring that it is accurate, coherent, and aligned with the user’s needs​.\\nOrchestration Module: Perhaps the most innovative aspect of Modular RAG is the orchestration module, which manages the coordination between different modules. This module controls the flow of data and tasks within the system, deciding which modules to activate based on the specific requirements of the query. By orchestrating the process, this module allows the system to adapt dynamically to different scenarios, making it more versatile and efficient​.\\n\\n3.2 The Role of Operators in Modular RAG\\nAt a finer level of granularity, the Modular RAG framework is built on “operators,” which are the basic functional units within each module. Operators are essentially the building blocks of the system, performing specific tasks that contribute to the larger goals of each module. Here’s how operators function within the Modular RAG system:\\n\\nEmbedding Operators: Within the indexing module, embedding operators are responsible for converting document chunks into vector representations. These embeddings capture the semantic essence of the text, enabling the retrieval module to efficiently match queries with the most relevant data​.\\nSimilarity Computation Operators: In the retrieval module, similarity computation operators calculate how closely a query matches with different document chunks. This process often involves comparing the embeddings of the query and the document chunks using methods like cosine similarity or dot product. The results of these computations are crucial for determining which chunks are most relevant and should be passed on to the generation module​.\\nQuery Expansion and Transformation Operators: These operators work within the pre-retrieval process to refine and enhance the user’s query. Query expansion operators might generate multiple versions of a query to improve retrieval coverage, while query transformation operators can rewrite or adjust queries to better align with the system’s retrieval capabilities. This step is vital for improving the accuracy and relevance of the retrieved data​.\\nChunk Compression and Selection Operators: After retrieval, the system often needs to manage large volumes of data. Chunk compression operators help reduce the size of the retrieved data without losing important information, making it easier for the generation module to process. Meanwhile, chunk selection operators filter out irrelevant or redundant information, ensuring that only the most pertinent data is used in the final output​.\\n\\n3.3 Interplay Between Modules and Operators\\nThe real power of Modular RAG lies in how these modules and operators interact. By breaking down the RAG process into discrete, reconfigurable components, Modular RAG allows for greater flexibility and adaptability. For example, if a particular task requires more detailed retrieval, the orchestration module can prioritize operators that enhance retrieval precision, such as reranking and query transformation operators.\\nMoreover, this modularity facilitates easier updates and maintenance. If a new method for similarity computation becomes available, it can be integrated as a new operator within the existing retrieval module without overhauling the entire system. This ability to adapt and evolve makes Modular RAG a forward-looking approach to handling complex, knowledge-intensive tasks in AI​\\nSection 4: Common Flow Patterns in Modular RAG\\nThe Modular RAG framework’s flexibility and scalability come to life through its ability to support a variety of RAG flow patterns. These patterns determine how different modules and operators interact, allowing the system to adapt to a wide range of tasks and complexities. By breaking free from the traditional linear “retrieve-then-generate” model, Modular RAG introduces several innovative patterns that cater to different scenarios.\\n4.1 The Linear Pattern\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe linear pattern is the most straightforward RAG flow, where modules are arranged in a fixed, sequential order. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow.\\nFor example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. These chunks are then passed through a post-retrieval reranking module to prioritize the most useful information, and finally, the generation module produces the output. This pattern is ideal for tasks where the process can be clearly defined from start to finish without needing dynamic adjustments​.\\n4.2 The Conditional Pattern\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe conditional pattern adds flexibility by allowing the RAG flow to branch based on specific conditions. This means that depending on the nature of the query or the retrieved data, different modules can be activated or bypassed.\\nFor instance, a routing module might analyze a query and determine whether it requires standard retrieval or more complex processing involving additional modules like query rewriting or chunk compression. This pattern is particularly useful for handling diverse queries that may require different levels of processing complexity. The system’s ability to conditionally route tasks ensures that resources are used efficiently, and the most appropriate processing path is followed​\\n4.3 The Branching Pattern\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nIn the branching pattern, multiple RAG processes run in parallel, each handling different aspects of a query or processing different types of data. After each branch completes its task, the results are merged to form a comprehensive final output.\\nThis pattern is especially useful in scenarios where a query might need to pull in information from various sources or when the system needs to explore multiple hypotheses simultaneously. For example, in a multi-faceted query, one branch could focus on retrieving data related to the primary query, while another branch might retrieve background information or context from different data sources. The results from these branches are then combined, providing a richer and more nuanced response​.\\n4.4 The Looping Pattern\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe looping pattern introduces iterative processes where the system can revisit earlier steps in the RAG flow to refine its output. This pattern is particularly powerful when dealing with complex queries that require multiple rounds of retrieval and generation to fully address the user’s needs.\\nIn a looping RAG flow, the system might generate an initial response, then evaluate whether additional retrieval is necessary. If the response is incomplete or could benefit from more information, the system loops back to the retrieval module to fetch additional data, which is then incorporated into a revised output. This iterative approach allows the system to gradually build up a more accurate and detailed response, making it well-suited for complex tasks that cannot be fully resolved in a single pass​\\n4.5 Adaptive (Active) Retrieval\\nOne of the most advanced patterns in Modular RAG is adaptive or active retrieval. This pattern allows the system to dynamically decide when and how to perform additional retrieval based on the ongoing generation process.\\nFor example, during the generation phase, the system might encounter a gap in the information that it cannot resolve with the data at hand. Instead of proceeding with an incomplete response, the system can trigger a new round of retrieval, fetching the necessary data to fill in the gaps before continuing with the generation. This adaptability ensures that the system can handle unexpected complexities and provide more accurate and relevant outputs​\\n4.6 Fusion and Integration Patterns\\nFinally, the fusion pattern is crucial for integrating multiple branches or different types of data into a single, cohesive output. After data from various branches or retrieval rounds has been processed, the fusion module merges these results, ensuring that the final output is both comprehensive and coherent.\\nFor instance, in a scenario where a query has been processed by several parallel branches, each focusing on different aspects of the query, the fusion module would combine the results from these branches, eliminating redundancies and resolving any conflicts to present a unified response. This pattern is essential for complex queries that require a broad range of information to be synthesized into a single, actionable output​.\\n4.7 Choosing the Right Pattern\\nThe choice of pattern in Modular RAG depends on the specific needs of the task at hand. Simple tasks may benefit from the linear pattern, while more complex scenarios might require conditional branching, looping, or adaptive retrieval. The modular nature of the framework means that different patterns can be combined or adjusted as needed, providing a tailored approach that maximizes efficiency and effectiveness in handling diverse queries.\\nSection 5: Tuning Patterns in Modular RAG\\nAs Retrieval-Augmented Generation (RAG) systems become increasingly complex and diverse, fine-tuning the individual components — such as retrievers and generators — has become essential for optimizing performance. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. These tuning patterns focus on refining the system’s components to achieve better alignment, accuracy, and efficiency.\\n5.1 Retriever Fine-Tuning\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe retriever is a crucial component in any RAG system, responsible for selecting the most relevant data chunks from a vast repository based on the user’s query. Fine-tuning the retriever is essential to improve retrieval accuracy, particularly when dealing with specialized domains or tasks that require a deep understanding of nuanced information.\\nSeveral methods are employed for retriever fine-tuning within the Modular RAG framework:\\n\\nSupervised Fine-Tuning (SFT): This involves training the retriever using a labeled dataset specific to the domain or task at hand. By reducing the distance between positive samples (relevant chunks) and increasing the distance from negative samples (irrelevant chunks), SFT enhances the retriever’s ability to distinguish between valuable and non-valuable information​.\\nLM-Supervised Retrieval (LSR): In this approach, the retriever is fine-tuned based on feedback from a language model (LM). The LM generates supervisory signals during the RAG process, guiding the retriever to prioritize chunks that are most likely to contribute to a high-quality final output​.\\nAdapter Modules: Sometimes, fine-tuning large retrievers directly can be resource-intensive. Instead, adding an adapter module — a smaller, trainable component — can enhance the retriever’s performance with less computational overhead. This approach is particularly useful when dealing with domain-specific tasks where full retriever fine-tuning may not be feasible​.\\n\\n5.2 Generator Fine-Tuning\\n\\nimage source:https://arxiv.org/html/2407.21059v1\\nThe generator, which produces the final output based on the retrieved data, also benefits from fine-tuning. Fine-tuning the generator helps align its outputs with the specific requirements of the task, whether it’s generating text that follows a particular style or ensuring that the content is factually accurate.\\nKey methods for generator fine-tuning in Modular RAG include:\\n\\nInstruct-Tuning: This method involves fine-tuning the generator using domain-specific datasets or task-specific instructions. By providing additional knowledge or adjusting the input-output formats, instruct-tuning ensures that the generator produces outputs that are better suited to the task’s needs​.\\nReinforcement Learning from Human or LM Feedback (RLHF): This approach uses reinforcement learning to align the generator’s outputs with human preferences or feedback from a more advanced language model. By iteratively refining the generator based on feedback, this method ensures that the final outputs are not only accurate but also aligned with desired quality standards​.\\nDual Fine-Tuning: In some scenarios, it’s beneficial to fine-tune both the retriever and the generator simultaneously. This approach ensures that both components are aligned in terms of their preferences and outputs, leading to a more cohesive and effective RAG system. A typical method involves aligning the scoring functions between the retriever and the generator using techniques like KL-Divergence, ensuring that both components are working towards the same goal​\\n\\n5.3 Benefits of Tuning Patterns in Modular RAG\\nThe tuning patterns in Modular RAG offer several advantages:\\n\\nImproved Accuracy: By fine-tuning the retriever and generator, the system can more accurately match queries with relevant data and produce higher-quality outputs. This is especially important in specialized domains where precision is critical​.\\nTask-Specific Optimization: Tuning allows the system to be customized for specific tasks, ensuring that both the retrieval and generation processes are tailored to the unique requirements of each application. This results in more relevant and useful outputs​.\\nEnhanced System Cohesion: Dual fine-tuning ensures that the retriever and generator are well-aligned, reducing the likelihood of disjointed or incoherent outputs. This leads to a more seamless and integrated RAG system​.\\n\\n5.4 Application of Tuning Patterns\\nThe choice of tuning pattern depends on the specific needs of the task. For instance, if the task involves handling highly specialized queries, supervised fine-tuning of the retriever may be necessary. If the goal is to produce outputs that adhere to specific stylistic or factual standards, instruct-tuning or RLHF for the generator might be more appropriate.\\nIn some cases, a combination of these tuning methods may be required. For example, a RAG system deployed in a legal context might use supervised fine-tuning to enhance the retriever’s ability to identify relevant case law, while also applying instruct-tuning to ensure that the generated legal summaries are accurate and appropriately formatted.\\nSection 6: Challenges and Opportunities in Modular RAG\\nAs promising as the Modular RAG framework is, implementing it effectively in real-world scenarios comes with its own set of challenges. However, these challenges also present unique opportunities for innovation and further development in the field of Retrieval-Augmented Generation.\\n6.1 Challenges in Implementing Modular RAG\\n\\nComplex Data Integration: One of the primary challenges in Modular RAG is integrating complex and diverse data sources. Traditional RAG systems often rely on a single type of unstructured text data, but Modular RAG expands this to include semi-structured data like tables and structured data such as knowledge graphs. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\\nSystem Orchestration and Workflow Management: The modularity of the RAG framework introduces a new layer of complexity in orchestrating and managing workflows. Unlike traditional linear RAG systems, Modular RAG involves routing, scheduling, and decision-making processes that require precise control over how modules interact. Designing an effective orchestration module that can dynamically adjust workflows based on the task’s requirements is critical, but also challenging​.\\nComponent Selection and Optimization: With the introduction of multiple modules and operators, selecting the right components for a specific task becomes crucial. Each module or operator must be optimized not only to perform well individually but also to work efficiently with other components in the system. Ensuring that these components are well-integrated and collectively enhance system performance requires careful design and continuous optimization.\\nMaintenance and Debugging: As Modular RAG systems grow in complexity, maintaining and debugging them can become more difficult. Isolating issues within a highly modular system requires sophisticated monitoring and diagnostic tools that can pinpoint problems at the module or operator level. Additionally, as new modules or operators are added, ensuring compatibility and maintaining system stability becomes an ongoing challenge​.\\n\\n6.2 Opportunities for Innovation in Modular RAG\\n\\nScalability and Flexibility: The modular nature of RAG systems offers significant opportunities for scalability. As tasks become more complex or as new data sources become available, additional modules or operators can be introduced without overhauling the entire system. This flexibility allows Modular RAG to scale with the growing demands of AI applications, making it a future-proof solution​.\\nEnhanced Personalization and Customization: Modular RAG allows for highly personalized and customized solutions tailored to specific use cases. By selecting and configuring modules to meet the unique needs of a task or domain, developers can create RAG systems that are more effective and efficient than one-size-fits-all solutions. This customization potential opens up new avenues for specialized applications in fields like healthcare, finance, and law​.\\nInnovation in Operator Design: The introduction of operators as basic functional units within modules provides a fertile ground for innovation. New operators can be developed to handle emerging challenges in data retrieval and generation, such as handling more complex queries, improving the accuracy of embeddings, or optimizing query transformation. As the field of AI evolves, the ability to innovate at the operator level ensures that Modular RAG remains at the cutting edge of technology​.\\nIntegration with Emerging Technologies: Modular RAG’s compatibility with emerging technologies such as knowledge graphs, reinforcement learning, and adaptive retrieval processes offers significant opportunities for advancement. By integrating these technologies into the RAG framework, developers can create more intelligent and adaptive systems that can better handle the complexities of real-world applications​.\\nResearch and Development: The modular approach of RAG systems encourages experimentation and research, particularly in optimizing the interaction between modules and operators. Researchers can explore new configurations, develop novel algorithms for specific modules, and refine orchestration strategies to push the boundaries of what RAG systems can achieve. This opens up vast opportunities for academic research and industry innovation​.\\n\\nConclusion\\nModular RAG represents a significant leap forward in the evolution of Retrieval-Augmented Generation systems. By breaking down complex processes into independent, reconfigurable modules and operators, it offers unparalleled flexibility, scalability, and adaptability. While challenges such as data integration, workflow orchestration, and system maintenance present obstacles, they also pave the way for innovation. As AI continues to advance, Modular RAG provides a robust framework that is well-equipped to meet the growing demands of knowledge-intensive tasks, making it a key player in the future of AI-driven solutions.\\nReferences:\\n\\nGao, Yunfan, et al. “Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.” arXiv preprint arXiv:2407.21059 (2024).\\nhttps://medium.com/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3\\n\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to audio narrations\\nRead offline with the Medium app\\nTry for $5/month\\nArtificial Intelligence\\nLlm\\nAI\\nChatGPT\\nNLP\\n\\n--\\n\\n--\\n1\\n\\n\\nFollow\\n\\nWritten by Sahin Ahmed, Data Scientist --------------------------------------\\n324 Followers\\nData Scientist | MSc Data science|Lifelong Learner | Making an Impact through Data Science | Machine Learning| Deep Learning |NLP| Statistical Modeling\\nFollow\\n\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\"}, {\"title\": \"How does Modular RAG improve upon Naive RAG?\", \"url\": \"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\", \"content\": \"Modular RAG. Modular RAG introduces a more sophisticated and flexible approach to Retrieval-Augmented Generation. By adopting a modular architecture, this version of RAG allows for the independent development and integration of various components, each responsible for specific tasks. The core components of Modular RAG typically include:\", \"score\": 0.6364215, \"raw_content\": \"How does Modular RAG improve upon Naive RAG? - Association of Data Scientists\\nSkip to content\\n\\n\\n\\nMemberships Close Memberships Open Memberships\\n\\nIndividual Membership\\nJoin the world’s leading Data Science professional community. You can access both General & Premium Memberships.\\nLearn More\\n\\nCorporate Membership\\nAny corporate, organization or academic institution having common interests in the AI field can become a member of ADaSci.\\nLearn More\\n\\n\\nAccreditations Close Accreditations Open Accreditations\\nChartered Data Scientist™\\nThe Chartered Data Scientist (CDS) credential gives a strong understanding of advanced data science profession and in-depth, applied analytics skills.\\nLearn More\\nCertified Data Scientist - Associate Level\\nBest suitable for the aspirants who want to start their career in the data science field, this certification.\\nLearn More\\nCertified Generative AI Engineer\\nAn upskilling-linked certification initiative designed to recognize talent in generative AI and large language models.\\nLearn More\\n\\n\\nContinuous Learning Close Continuous Learning Open Continuous Learning\\nOur Latest Courses\\n\\nBuilding Generative AI Applications with Amazon Bedrock\\n₹5,191.00\\nAdd to cart\\n\\nMastering Prompt Engineering for LLMs\\n₹5,191.00\\nAdd to cart\\n\\nThe LLMops : Streamlining the GenAI & LLM Operations\\n₹3,460.00\\nAdd to cart\\n\\nMastering Feature Engineering Essentials\\n₹2,163.00\\nAdd to cart\\nHi, Welcome back!\\nKeep me signed in\\nForgot?\\nSign In\\nDon't have an account?  Register Now\\nAccess all Courses\\n\\n\\nCorporate Trainings\\n\\n\\nContact\\n\\n\\n₹0.00 0 Cart\\nMembers Area\\n\\nDeep Dives\\n\\nHow does Modular RAG improve upon Naive RAG?\\nModular RAG enhances flexibility, scalability, and accuracy compared to Naive RAG.\\n\\nBy Sourabh Mehta\\nPublished on August 5, 2024\\n\\n\\nExplore more from ADaSci\\nGenerative AI Through the Lens of an AI Scientist: Navigating the Technological Tsunami\\nMastering Lightweight AI with Falcon 3 : A Hands-On Guide\\nA Guide to Running LLMs Locally with No-Code Framework Dify\\nLLMFlows for Building Flow-Based Chat Application: A Hands-on Guide\\nBreast cancer histopathological images classification using deep learning\\nCode Search with Vector Embeddings using Qdrant Vector Database\\nQuarkus: Java Development for Modern Applications.\\nImplementing RAG Pipelines using LightRAG and GPT-4o mini\\nColpali: Hands-On Guide to PDF Analysis with Qwen2VL\\nModelling Approach for enhanced market expansion in Digital Marketing in B2B space\\nRetrieval-augmented generation (RAG) has emerged as a powerful technique that combines the strengths of information retrieval and natural language generation. However, not all RAG implementations are created equal. The traditional or “Naive” RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets. Enter Modular RAG—a sophisticated, next-generation approach that significantly enhances the capabilities of Naive RAG by introducing modularity and flexibility into the system.\\nTable of content\\n\\nOverview of Naive RAG and Modular RAG\\nLimitations of Naive RAG\\nWhat is Modular RAG?\\nCase Study: Customer Support Chatbot\\n\\nLet’s start with an overview of Navie and Modular RAG followed by limitations and benefits.\\nOverview of Naive RAG and Modular RAG\\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of artificial intelligence by combining the strengths of information retrieval and natural language generation. This hybrid approach leverages vast external knowledge sources to enhance the generation capabilities of models like GPT-4, resulting in more accurate and contextually relevant outputs.\\nNaive RAG\\nNaive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. This method involves two main steps:\\n\\nRetrieval: The system retrieves a set of relevant documents or passages from a predefined database based on the input query.\\nGeneration: Using the retrieved information, the generative model produces a response that is informed by external knowledge.\\n\\nWhile Naive RAG marked a breakthrough in enhancing the capabilities of generative models by providing them access to extensive external knowledge, it has its limitations. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. Additionally, customization and scalability can be challenging, limiting its effectiveness in diverse and dynamic environments.\\nModular RAG\\nModular RAG introduces a more sophisticated and flexible approach to Retrieval-Augmented Generation. By adopting a modular architecture, this version of RAG allows for the independent development and integration of various components, each responsible for specific tasks. The core components of Modular RAG typically include:\\n\\nCustomizable Retrievers: Advanced retrieval mechanisms that can be tailored to specific use cases, allowing for more efficient and relevant information retrieval.\\nAdaptive Generators: Generative models that can seamlessly integrate with different retrievers, enhancing the overall performance and accuracy.\\nPlug-and-Play Modules: Additional components that can be easily added or replaced, providing greater flexibility and adaptability to changing requirements.\\n\\nThe modular design of this RAG variant addresses many of the shortcomings of Naive RAG. It enables more efficient retrieval processes, better integration with generative models, and the ability to customize and scale the system according to specific needs. As a result, Modular RAG offers significant improvements in performance, accuracy, and flexibility, making it a more robust solution for a wide range of applications.\\nLimitations of Naive RAG\\nWhile Naive Retrieval-Augmented Generation (RAG) brought significant advancements by combining retrieval and generation, it also presents several limitations that restrict its effectiveness and efficiency. Understanding these limitations is crucial to appreciate the improvements brought by Modular RAG.\\nInflexibility and Static Nature\\nOne of the primary drawbacks of Naive RAG is its inflexibility. The retrieval component in Naive RAG is often designed to follow a static approach, retrieving information based on predefined rules or simplistic algorithms. This rigidity can lead to several issues:\\n\\nLimited Adaptability: Naive RAG struggles to adapt to new or evolving information needs, making it less effective in dynamic environments where the context or required information may change rapidly.\\nSuboptimal Responses: The static retrieval mechanism may not always retrieve the most relevant or up-to-date information, leading to generative outputs that are less accurate or contextually appropriate.\\n\\nInefficiencies in Retrieval Processes\\nThe retrieval process in Naive RAG can be inefficient due to its reliance on basic retrieval strategies. These inefficiencies manifest in several ways:\\nHigh Latency: The process of retrieving and integrating external information can introduce significant latency, slowing down the overall response time.\\nResource Intensity: Basic retrieval methods may require extensive computational resources, especially when dealing with large datasets, making the system less scalable and more costly to operate.\\nRelevance Issues: The simplicity of the retrieval algorithms can lead to the retrieval of irrelevant or low-quality information, which negatively impacts the quality of the generated responses.\\nChallenges in Customization and Integration\\nNaive RAG systems often face difficulties in customization and integration, limiting their utility across diverse applications:\\n\\nLack of Custom Modules: The rigid architecture of Naive RAG makes it challenging to incorporate custom modules tailored to specific tasks or industries. This lack of customization restricts the applicability of the system to more generalized use cases.\\nIntegration Problems: Integrating Naive RAG with other systems or technologies can be cumbersome, as the tightly coupled components are not designed for seamless interoperability. This can hinder the ability to leverage complementary technologies or data sources.\\n\\nScalability Constraints\\nAs data and usage grow, scalability becomes a critical concern for Naive RAG:\\n\\nPerformance Degradation: As the volume of data increases, the performance of Naive RAG systems can degrade significantly. The static nature of the retrieval process struggles to maintain efficiency with growing datasets.\\nLimited Parallelism: The monolithic design of Naive RAG often limits its ability to parallelize tasks effectively, further impacting scalability and performance.\\n\\nWhat is Modular RAG?\\nModular Retrieval-Augmented Generation (RAG) represents an evolution in the design and implementation of RAG systems. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. In this section, we will delve into the core concepts, key components, and benefits of Modular RAG.\\nModular RAG is an advanced form of Retrieval-Augmented Generation that leverages a modular design to separate and optimize various components of the system. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. This modularity allows for:\\n\\nIndependent Development: Each module can be developed and improved independently, enabling rapid innovation and iteration.\\nCustomizability: Modules can be tailored to specific tasks, industries, or use cases, providing greater versatility.\\nInteroperability: Modular components can be easily integrated with other systems or technologies, enhancing overall functionality.\\n\\nKey Components and Architecture\\nThe architecture of Modular RAG typically comprises several key components, each responsible for a specific function. These components include:\\nCustomizable Retrievers\\n\\nAdvanced Retrieval Mechanisms: Unlike the static retrieval strategies of Naive RAG, Modular RAG employs sophisticated algorithms such as hybrid search (combining keyword and semantic search) and machine learning-based retrieval. This results in more accurate and relevant information retrieval.\\nDynamic Adaptation: Retrievers can adapt to changing data and requirements, ensuring that the most pertinent information is always accessible.\\n\\nAdaptive Generators\\n\\nSeamless Integration: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\\nContextual Awareness: These generators can better understand and incorporate the context provided by retrieved information, leading to more accurate and meaningful outputs.\\n\\nPlug-and-Play Modules\\n\\nEase of Customization: Modular RAG supports the addition and replacement of modules without disrupting the overall system. This allows for easy customization based on specific needs or improvements in technology.\\nScalability: The plug-and-play nature of the modules ensures that the system can scale efficiently, handling increasing volumes of data and user interactions.\\n\\nOrchestration Layer\\nAn orchestration layer manages the interactions between different modules, ensuring smooth communication and data flow. This layer optimizes the overall performance and maintains system coherence.\\n\\nImage Source\\nCase Study: Customer Support Chatbot\\nA large e-commerce company implements a customer support chatbot to handle frequently asked questions and support queries. Initially, the company used Naive RAG and later transitioned to Modular RAG to improve performance.\\nKey Metrics for Comparison:\\n\\nResponse Relevance\\nResponse Time\\nScalability\\nCustomer Satisfaction\\n\\nResponse Relevance\\n\\nResponse Time\\n\\nScalability\\n\\nCustomer Satisfaction\\n\\nThe consolidated table demonstrates that Modular RAG outperforms Naive RAG across all key metrics, making it a more effective and reliable solution for customer support chatbots. By adopting a modular approach, organizations can achieve better relevance, faster response times, greater scalability, and higher customer satisfaction.\\nConclusion\\nModular RAG, with its advanced architecture and dynamic components, addresses these challenges head-on. By allowing for independent development, customization, and seamless integration of various modules, Modular RAG provides a more robust, efficient, and scalable solution. This is evident in key metrics such as response relevance, response time, scalability, and customer satisfaction.\\nReferences\\n\\nPrompt Engineering Guide\\n\\n\\nSourabh Mehta\\nThe Chartered Data Scientist Designation\\nAchieve the highest distinction in the data science profession.\\nRegister\\nElevate Your Team's AI Skills with our Proven Training Programs\\nStrengthen Critical AI Skills with Trusted Generative AI Training by Association of Data Scientists.\\nFind Out More\\nOur AI Courses\\nVisit AI Academy\\n\\n\\n Generative AI-Powered Software Engineering with AI Coding Assistants -------------------------------------------------------------------- ₹5,191.00\\nAdd to cart\\n\\n\\n Building Knowledge Graph Solutions with LangGraph ------------------------------------------------- ₹3,028.00\\nAdd to cart\\n\\n\\n Generative AI Application Development with Azure ------------------------------------------------ ₹5,191.00\\nAdd to cart\\n\\n\\nOur Accreditations\\nGet global recognition for AI skills\\n\\nChartered Data Scientist (CDS™)\\nThe highest distinction in the data science profession. Not just earn a charter, but use it as a designation.\\nLearn more\\n\\nCertified Data Scientist - Associate Level\\nGlobal recognition of data science skills at the beginner level.\\nLearn more\\n\\nCertified Generative AI Engineer\\nAn upskilling-linked certification initiative designed to recognize talent in generative AI and large language models\\nLearn more\\nJoin thousands of members and receive all benefits.\\nBecome Our Member\\nWe offer both Individual & Institutional Membership.\\nFind out more\\nNot a member, but still want to know what we are upto? Subscribe to our Newsletter\\nEmail \\nStart Free Trial\\nThe power of intelligence to propel humanity and make a difference\\nOur Accrediations\\n\\nChartered Data Scientist™ (CDS)\\nCertified Data Scientist - Associate Level\\nCertified Generative AI Engineer\\n\\nCDS Program\\n\\nAbout CDS\\nExam Information\\nCandidate Body of Knowledge (CBOK)\\nExam Structure\\nExam Cost and Registration Fees\\nEthical & Standards for Chartered Data Scientists (CDS)\\nHow to Earn the CDS Charter\\nTerms & Conditions For CDS™\\n\\nMembership\\n\\nIndividual Membership\\nInstitutional Membership\\n\\nAbout\\n\\nAbout ADaSci\\nContinuous Learning\\nTeam\\nPrivacy Policy\\nTerms and Conditions\\nChapters\\nBlogs\\nContact\\n\\nFor Organizations\\n\\nCorporate Trainings\\nCDS for Organizations\\nCorporate Membership\\n\\nJournal\\n\\nLattice\\nAbout\\nReview Committee\\n\\nTwitter Facebook-f Linkedin\\n© 2024 All rights reserved Association of Data Scientists\\n\\nWe noticed you're visiting from India. We've updated our prices to Indian rupee for your shopping convenience. Use United States (US) dollar instead. Dismiss\"}, {\"title\": \"Modular Rag in Multi-Agent Systems - Restackio\", \"url\": \"https://www.restack.io/p/multi-agent-systems-answer-modular-rag-cat-ai\", \"content\": \"Explore the role of modular rag in enhancing the efficiency of multi-agent systems through structured interactions. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities: By utilizing the Modular RAG framework, developers can create more robust and adaptable systems that meet the evolving demands of data retrieval and processing. Multimodal Retrieval-Augmented Generation (MM-RAG) represents a significant advancement in the capabilities of language models by integrating both text and image retrieval into the generation process. This approach enhances the traditional Retrieval-Augmented Generation (RAG) by allowing models to utilize a broader range of data types, thereby improving the quality and relevance of generated outputs. The Modular RAG framework is designed to allow for module substitution or reconfiguration, addressing specific challenges in retrieval and processing.\", \"score\": 0.59344494, \"raw_content\": \"Modular Rag in Multi-Agent Systems | Restackio\\nRestack\\nDocsSign up\\nOpen menu\\nDocsUse casesPricingCompanyEnterpriseContactCommunity\\n\\nLog inSign up\\n\\nMulti-Agent Systems/\\n\\nModular Rag in Multi-Agent Systems\\n\\n\\nMulti-Agent Systems\\n\\n\\nModular Rag in Multi-Agent Systems\\nLast updated on 12/17/24\\nExplore the role of modular rag in enhancing the efficiency of multi-agent systems through structured interactions.\\nOn this page\\n\\nUnderstanding Modular RAG Framework\\nAdvantages of Multimodal Retrieval-Augmented Generation\\nImplementing Modular RAG in Real-World Applications\\n\\nSources\\n github.com griptape-ai/griptape/dev/docs/griptape-tools/official-tools/structure-run-tool.md arxiv.org Retrieval-Augmented Generation for Large Language Models: A Survey\\nUnderstanding Modular RAG Framework -----------------------------------\\nModular Retrieval-Augmented Generation (RAG) is an advanced approach that enhances adaptability and versatility beyond traditional RAG paradigms. This framework incorporates diverse strategies to improve its components by adding specialized modules for retrieval and processing capabilities.\\nNew Modules\\nThe Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities:\\n\\nSearch Module: Adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs. It utilizes LLM-generated code and query languages to optimize search efficiency.\\nRAG-Fusion: Addresses traditional search limitations by employing a multi-query strategy. This strategy expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge.\\nMemory Module: Leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that enhances the system's ability to recall and utilize past interactions effectively.\\n\\nProcessing Flexibility\\nThe Modular RAG framework allows for module substitution or reconfiguration to address specific challenges. This flexibility is crucial for integrating new modules or adjusting the interaction flow among existing ones. The framework supports both sequential processing and integrated end-to-end training across its components, illustrating progression and refinement within the RAG family.\\nConclusion\\nBy utilizing the Modular RAG framework, developers can create more robust and adaptable systems that meet the evolving demands of data retrieval and processing. This approach not only enhances the capabilities of RAG but also ensures that it remains relevant in a rapidly changing technological landscape.\\nRelated answers\\n\\nMulti-Agent Systems Foundations Explore the algorithmic game-theoretic and logical foundations of multi-agent systems, enhancing understanding and application.\\nJade For Multi-Agent Systems Explore how Jade enhances multi-agent systems, facilitating communication and coordination among agents effectively.\\nMulti-Agent Systems Llm Overview Explore the intricacies of multi-agent systems and their applications in LLM technology, enhancing collaboration and efficiency.\\n\\nBuildReplayFunctions\\nRestack AI SDK\\nThe framework for autonomous intelligence\\nBuild autonomous AI products in code, capable of running and persisting month-lasting processes in the background.\\nLearn more ->\\n\\nSources\\n arxiv.org Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks weaviate.io weaviate.io\\nAdvantages of Multimodal Retrieval-Augmented Generation -------------------------------------------------------\\nMultimodal Retrieval-Augmented Generation (MM-RAG) represents a significant advancement in the capabilities of language models by integrating both text and image retrieval into the generation process. This approach enhances the traditional Retrieval-Augmented Generation (RAG) by allowing models to utilize a broader range of data types, thereby improving the quality and relevance of generated outputs.\\nKey Advantages of MM-RAG\\n\\nScalability: MM-RAG reduces the model size and training costs while enabling easy expansion of knowledge. By leveraging external knowledge bases, it allows for the integration of new information without the need for extensive retraining.\\nAccuracy: By grounding the model in factual data, MM-RAG minimizes the risk of hallucinations, ensuring that the generated content is more reliable and contextually appropriate.\\nControllability: Users can easily update or customize the knowledge base through simple CRUD operations in a vector database, allowing for dynamic adjustments to the model's knowledge.\\nInterpretability: The retrieved items serve as references for the model's predictions, providing transparency and a basis for understanding the generated outputs.\\n\\nEnhanced Performance Metrics\\nRecent studies have shown that MM-RAG significantly outperforms baseline multimodal models such as DALL-E and CM3 in various tasks, including:\\n\\nImage and Caption Generation: MM-RAG excels in generating high-quality images and captions that are closely aligned with the retrieved context.\\nComputational Efficiency: It requires substantially less computational power, achieving performance with less than 30% of the resources needed for DALL-E.\\nContextual Fidelity: The images generated by MM-RAG are more faithful to the context from which they were retrieved, enhancing the overall quality of the outputs.\\nIn-Context Learning: MM-RAG supports multimodal in-context learning, allowing the model to generate images based on demonstration images and text, thus adapting to the visual characteristics of the input data.\\n\\nConclusion\\nThe integration of multimodal capabilities into retrieval-augmented generation systems marks a pivotal shift in how we can utilize large language models. By effectively combining text and image data, MM-RAG not only enhances the generative capabilities of these models but also opens up new avenues for applications across various industries, making it a powerful tool for developers and researchers alike.\\nRelated answers\\n\\nWhat Is Multi Level Planning in Multi-Agent Systems Explore the concept of multi-level planning in multi-agent systems, focusing on its structure and applications in complex environments.\\nMulti-Agent Systems for Complex Problem Solving Explore how multi-agent AI systems tackle complex problems through collaboration and intelligent decision-making.\\nMulti-Agent Systems And Multi-Linguality Explore how multi-agent systems enhance multi-linguality, enabling seamless communication across diverse languages and cultures.\\n\\nSources\\n github.com griptape-ai/griptape/dev/docs/griptape-tools/official-tools/structure-run-tool.md weaviate.io\\nImplementing Modular RAG in Real-World Applications ---------------------------------------------------\\nModular Retrieval-Augmented Generation (RAG) is an advanced approach that enhances traditional RAG paradigms by offering improved adaptability and versatility. This framework incorporates various strategies to enhance its components through the addition of specialized modules for retrieval and processing capabilities.\\nCore Components of Modular RAG\\nThe Modular RAG framework is designed to allow for module substitution or reconfiguration, addressing specific challenges in retrieval and processing. This flexibility is crucial for integrating new modules or adjusting the interaction flow among existing ones. The core components include:\\n\\nRetrieval Module: Responsible for fetching relevant data from a knowledge base.\\nAugmentation Module: Enhances the retrieved data to improve the quality of the output.\\nGeneration Module: Generates responses based on the augmented data.\\nRewrite and Rerank Modules: These advanced components refine the output further, ensuring higher relevance and accuracy.\\n\\nAdvantages of Modular RAG\\nThe modular approach supports both sequential processing and integrated end-to-end training across its components. This illustrates the progression and refinement within the RAG family, allowing for:\\n\\nAdaptability: Modules can be tailored to specific tasks, enhancing performance.\\nFlexibility: New modules can be integrated seamlessly, allowing for continuous improvement.\\nEfficiency: The ability to reconfigure modules can lead to optimized processing times and resource usage.\\n\\nPractical Applications\\nIn real-world applications, Modular RAG can be utilized in various domains, such as:\\n\\nCustomer Support: By integrating modules that specialize in understanding customer queries and generating accurate responses.\\nContent Creation: Utilizing the generation module to produce high-quality content based on retrieved data.\\nData Analysis: Employing the retrieval and augmentation modules to analyze large datasets effectively.\\n\\nConclusion\\nThe Modular RAG framework represents a significant advancement in the RAG domain, providing a robust structure for developing applications that require sophisticated retrieval and generation capabilities. By leveraging its modular nature, developers can create tailored solutions that meet specific needs while maintaining high performance and adaptability.\\nRelated answers\\n\\nAi Architecture For Multi-Agent Systems Explore the intricacies of AI architecture tailored for multi-agent systems, enhancing collaboration and efficiency in complex environments.\\nOntology Multi-Agent Systems Development Explore the development of ontology multi-agent systems, focusing on design, implementation, and integration techniques.\\nInternational Conference on Multi-Agent Systems Explore the latest advancements in practical applications of agents and multi-agent systems at this international conference.\\n\\nBuildReplayFunctions\\nRestack AI SDK\\nThe framework for autonomous intelligence\\nBuild autonomous AI products in code, capable of running and persisting month-lasting processes in the background.\\nLearn more ->\\n\\nOn this page\\n\\nUnderstanding Modular RAG Framework\\nAdvantages of Multimodal Retrieval-Augmented Generation\\nImplementing Modular RAG in Real-World Applications\\n\\nFooter\\nJoin us to build the framework for autonomous intelligence.\\nFramework\\n\\nDocs\\nExamples\\nUse cases\\n\\nCloud\\n\\nSign up\\nLog in\\nPricing\\n\\nAbout\\n\\nCompany\\nEnterprise\\nContact\\n\\nLegal\\n\\nLegal notice\\nPrivacy\\nTerms\\n\\nABCDEFGHIJKLMNOPQRSTUVWXYZ\"}]\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Modular RAG(모듈형 RAG)는 전통적인 RAG 시스템의 한계를 극복하고, 더 유연하고 효율적인 정보 검색 및 생성 프로세스를 제공합니다. 다음은 Modular RAG가 프로덕션 레벨에서 사용하는 이점입니다:\n",
      "\n",
      "1. **유연성**: Modular RAG는 다양한 모듈로 구성되어 있어, 각 모듈을 독립적으로 개발하고 최적화할 수 있습니다. 이는 특정 작업이나 데이터 요구에 맞게 시스템을 조정할 수 있는 능력을 제공합니다.\n",
      "\n",
      "2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있어, 시스템이 성장하는 데이터와 요구에 맞춰 확장할 수 있습니다. 이는 대규모 데이터 처리에 매우 유리합니다.\n",
      "\n",
      "3. **효율성**: 각 모듈이 특정 작업에 최적화되어 있어, 전체 시스템의 성능을 향상시킵니다. 예를 들어, 검색 모듈은 더 정교한 검색 알고리즘을 사용하여 관련 정보를 더 정확하게 찾을 수 있습니다.\n",
      "\n",
      "4. **개선된 정확성**: Modular RAG는 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다. 이는 특히 복잡한 쿼리나 다양한 데이터 유형을 처리할 때 유리합니다.\n",
      "\n",
      "5. **모듈화된 설계**: 각 모듈은 독립적으로 작동하므로, 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다. 이는 유지보수와 디버깅을 용이하게 합니다.\n",
      "\n",
      "6. **적응성**: Modular RAG는 다양한 작업 요구에 맞춰 모듈을 재구성하거나 교체할 수 있어, 변화하는 환경에 빠르게 적응할 수 있습니다.\n",
      "\n",
      "이러한 이점들은 Modular RAG가 복잡한 정보 검색 및 생성 작업을 수행하는 데 있어 매우 유용한 도구임을 보여줍니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAG가 production level에서 사용하는 이점을 정리한다.', 'Modular RAG(모듈형 RAG)는 전통적인 RAG 시스템의 한계를 극복하고, 더 유연하고 효율적인 정보 검색 및 생성 프로세스를 제공합니다. 다음은 Modular RAG가 프로덕션 레벨에서 사용하는 이점입니다:\\n\\n1. **유연성**: Modular RAG는 다양한 모듈로 구성되어 있어, 각 모듈을 독립적으로 개발하고 최적화할 수 있습니다. 이는 특정 작업이나 데이터 요구에 맞게 시스템을 조정할 수 있는 능력을 제공합니다.\\n\\n2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있어, 시스템이 성장하는 데이터와 요구에 맞춰 확장할 수 있습니다. 이는 대규모 데이터 처리에 매우 유리합니다.\\n\\n3. **효율성**: 각 모듈이 특정 작업에 최적화되어 있어, 전체 시스템의 성능을 향상시킵니다. 예를 들어, 검색 모듈은 더 정교한 검색 알고리즘을 사용하여 관련 정보를 더 정확하게 찾을 수 있습니다.\\n\\n4. **개선된 정확성**: Modular RAG는 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다. 이는 특히 복잡한 쿼리나 다양한 데이터 유형을 처리할 때 유리합니다.\\n\\n5. **모듈화된 설계**: 각 모듈은 독립적으로 작동하므로, 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다. 이는 유지보수와 디버깅을 용이하게 합니다.\\n\\n6. **적응성**: Modular RAG는 다양한 작업 요구에 맞춰 모듈을 재구성하거나 교체할 수 있어, 변화하는 환경에 빠르게 적응할 수 있습니다.\\n\\n이러한 이점들은 Modular RAG가 복잡한 정보 검색 및 생성 작업을 수행하는 데 있어 매우 유용한 도구임을 보여줍니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[94mresponse\u001b[0m:\n",
      "No more steps needed.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mfinal_report\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[94mresponse\u001b[0m:\n",
      "# 최종 보고서: Modular RAG와 Naive RAG의 비교 및 프로덕션 레벨에서의 이점\n",
      "\n",
      "## 1. 서론\n",
      "본 보고서는 Modular RAG(모듈형 RAG)와 Naive RAG(단순 RAG)의 정의, 구조, 작동 방식, 주요 차이점 및 Modular RAG가 프로덕션 레벨에서 제공하는 이점을 설명합니다. 이를 통해 두 시스템의 특성과 Modular RAG의 장점을 명확히 이해할 수 있습니다.\n",
      "\n",
      "## 2. RAG의 정의\n",
      "\n",
      "### 2.1 Modular RAG\n",
      "- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\n",
      "- **특징**:\n",
      "  - **모듈화**: 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\n",
      "  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있습니다.\n",
      "  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\n",
      "\n",
      "### 2.2 Naive RAG\n",
      "- **정의**: Naive RAG는 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\n",
      "- **특징**:\n",
      "  - **단순성**: 구현이 간단하고 직관적입니다.\n",
      "  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\n",
      "  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\n",
      "\n",
      "## 3. 구조 및 작동 방식\n",
      "\n",
      "### 3.1 Modular RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. **모듈화된 구성 요소**: 정보 검색 모듈과 생성 모듈로 구성됩니다.\n",
      "  2. **데이터 흐름**: 사용자가 질문을 입력하면 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\n",
      "\n",
      "- **작동 방식**:\n",
      "  1. 쿼리 입력\n",
      "  2. 정보 검색\n",
      "  3. 정보 필터링\n",
      "  4. 응답 생성\n",
      "  5. 결과 출력\n",
      "\n",
      "### 3.2 Naive RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. 질문 입력\n",
      "  2. 정보 검색\n",
      "  3. 문서 선택\n",
      "  4. 생성 모델\n",
      "\n",
      "- **작동 방식**:\n",
      "  - 검색 단계와 생성 단계로 나뉘며, 전통적인 정보 검색 기법을 사용하여 관련 문서를 검색하고, 이를 바탕으로 답변을 생성합니다.\n",
      "\n",
      "## 4. 주요 차이점 비교\n",
      "\n",
      "1. **구조적 유연성**: Modular RAG는 모듈화된 구조로 유연성과 확장성을 제공하며, Naive RAG는 고정된 구조로 한계가 있습니다.\n",
      "2. **모듈화**: Modular RAG는 각 모듈이 특정 역할을 담당하여 다양한 작업을 처리할 수 있지만, Naive RAG는 단일 흐름으로 진행됩니다.\n",
      "3. **성능 최적화**: Modular RAG는 각 모듈을 독립적으로 최적화할 수 있어 성능 개선이 용이합니다.\n",
      "4. **적응성**: Modular RAG는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 반면, Naive RAG는 적응성이 부족합니다.\n",
      "\n",
      "## 5. Modular RAG의 프로덕션 레벨 이점\n",
      "\n",
      "1. **유연성**: 다양한 모듈로 구성되어 있어 시스템을 조정할 수 있는 능력을 제공합니다.\n",
      "2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있습니다.\n",
      "3. **효율성**: 각 모듈이 특정 작업에 최적화되어 전체 시스템의 성능을 향상시킵니다.\n",
      "4. **개선된 정확성**: 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다.\n",
      "5. **모듈화된 설계**: 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
      "6. **적응성**: 변화하는 환경에 빠르게 적응할 수 있습니다.\n",
      "\n",
      "## 6. 결론\n",
      "Modular RAG는 Naive RAG의 한계를 극복하고, 더 나은 성능과 유연성을 제공하는 정보 검색 및 생성 시스템입니다. 이러한 특성 덕분에 Modular RAG는 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_print import invoke_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점을 설명해줘\"\n",
    "}\n",
    "\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 최종 보고서: Modular RAG와 Naive RAG의 비교 및 프로덕션 레벨에서의 이점\n",
      "\n",
      "## 1. 서론\n",
      "본 보고서는 Modular RAG(모듈형 RAG)와 Naive RAG(단순 RAG)의 정의, 구조, 작동 방식, 주요 차이점 및 Modular RAG가 프로덕션 레벨에서 제공하는 이점을 설명합니다. 이를 통해 두 시스템의 특성과 Modular RAG의 장점을 명확히 이해할 수 있습니다.\n",
      "\n",
      "## 2. RAG의 정의\n",
      "\n",
      "### 2.1 Modular RAG\n",
      "- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\n",
      "- **특징**:\n",
      "  - **모듈화**: 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\n",
      "  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있습니다.\n",
      "  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\n",
      "\n",
      "### 2.2 Naive RAG\n",
      "- **정의**: Naive RAG는 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\n",
      "- **특징**:\n",
      "  - **단순성**: 구현이 간단하고 직관적입니다.\n",
      "  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\n",
      "  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\n",
      "\n",
      "## 3. 구조 및 작동 방식\n",
      "\n",
      "### 3.1 Modular RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. **모듈화된 구성 요소**: 정보 검색 모듈과 생성 모듈로 구성됩니다.\n",
      "  2. **데이터 흐름**: 사용자가 질문을 입력하면 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\n",
      "\n",
      "- **작동 방식**:\n",
      "  1. 쿼리 입력\n",
      "  2. 정보 검색\n",
      "  3. 정보 필터링\n",
      "  4. 응답 생성\n",
      "  5. 결과 출력\n",
      "\n",
      "### 3.2 Naive RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. 질문 입력\n",
      "  2. 정보 검색\n",
      "  3. 문서 선택\n",
      "  4. 생성 모델\n",
      "\n",
      "- **작동 방식**:\n",
      "  - 검색 단계와 생성 단계로 나뉘며, 전통적인 정보 검색 기법을 사용하여 관련 문서를 검색하고, 이를 바탕으로 답변을 생성합니다.\n",
      "\n",
      "## 4. 주요 차이점 비교\n",
      "\n",
      "1. **구조적 유연성**: Modular RAG는 모듈화된 구조로 유연성과 확장성을 제공하며, Naive RAG는 고정된 구조로 한계가 있습니다.\n",
      "2. **모듈화**: Modular RAG는 각 모듈이 특정 역할을 담당하여 다양한 작업을 처리할 수 있지만, Naive RAG는 단일 흐름으로 진행됩니다.\n",
      "3. **성능 최적화**: Modular RAG는 각 모듈을 독립적으로 최적화할 수 있어 성능 개선이 용이합니다.\n",
      "4. **적응성**: Modular RAG는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 반면, Naive RAG는 적응성이 부족합니다.\n",
      "\n",
      "## 5. Modular RAG의 프로덕션 레벨 이점\n",
      "\n",
      "1. **유연성**: 다양한 모듈로 구성되어 있어 시스템을 조정할 수 있는 능력을 제공합니다.\n",
      "2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있습니다.\n",
      "3. **효율성**: 각 모듈이 특정 작업에 최적화되어 전체 시스템의 성능을 향상시킵니다.\n",
      "4. **개선된 정확성**: 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다.\n",
      "5. **모듈화된 설계**: 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
      "6. **적응성**: 변화하는 환경에 빠르게 적응할 수 있습니다.\n",
      "\n",
      "## 6. 결론\n",
      "Modular RAG는 Naive RAG의 한계를 극복하고, 더 나은 성능과 유연성을 제공하는 정보 검색 및 생성 시스템입니다. 이러한 특성 덕분에 Modular RAG는 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "snapshot = app.get_state(config).values\n",
    "print(snapshot[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 최종 보고서: Modular RAG와 Naive RAG의 비교 및 프로덕션 레벨에서의 이점\n",
       "\n",
       "## 1. 서론\n",
       "본 보고서는 Modular RAG(모듈형 RAG)와 Naive RAG(단순 RAG)의 정의, 구조, 작동 방식, 주요 차이점 및 Modular RAG가 프로덕션 레벨에서 제공하는 이점을 설명합니다. 이를 통해 두 시스템의 특성과 Modular RAG의 장점을 명확히 이해할 수 있습니다.\n",
       "\n",
       "## 2. RAG의 정의\n",
       "\n",
       "### 2.1 Modular RAG\n",
       "- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\n",
       "- **특징**:\n",
       "  - **모듈화**: 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\n",
       "  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있습니다.\n",
       "  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\n",
       "\n",
       "### 2.2 Naive RAG\n",
       "- **정의**: Naive RAG는 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\n",
       "- **특징**:\n",
       "  - **단순성**: 구현이 간단하고 직관적입니다.\n",
       "  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\n",
       "  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\n",
       "\n",
       "## 3. 구조 및 작동 방식\n",
       "\n",
       "### 3.1 Modular RAG의 구조 및 작동 방식\n",
       "- **구조**:\n",
       "  1. **모듈화된 구성 요소**: 정보 검색 모듈과 생성 모듈로 구성됩니다.\n",
       "  2. **데이터 흐름**: 사용자가 질문을 입력하면 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\n",
       "\n",
       "- **작동 방식**:\n",
       "  1. 쿼리 입력\n",
       "  2. 정보 검색\n",
       "  3. 정보 필터링\n",
       "  4. 응답 생성\n",
       "  5. 결과 출력\n",
       "\n",
       "### 3.2 Naive RAG의 구조 및 작동 방식\n",
       "- **구조**:\n",
       "  1. 질문 입력\n",
       "  2. 정보 검색\n",
       "  3. 문서 선택\n",
       "  4. 생성 모델\n",
       "\n",
       "- **작동 방식**:\n",
       "  - 검색 단계와 생성 단계로 나뉘며, 전통적인 정보 검색 기법을 사용하여 관련 문서를 검색하고, 이를 바탕으로 답변을 생성합니다.\n",
       "\n",
       "## 4. 주요 차이점 비교\n",
       "\n",
       "1. **구조적 유연성**: Modular RAG는 모듈화된 구조로 유연성과 확장성을 제공하며, Naive RAG는 고정된 구조로 한계가 있습니다.\n",
       "2. **모듈화**: Modular RAG는 각 모듈이 특정 역할을 담당하여 다양한 작업을 처리할 수 있지만, Naive RAG는 단일 흐름으로 진행됩니다.\n",
       "3. **성능 최적화**: Modular RAG는 각 모듈을 독립적으로 최적화할 수 있어 성능 개선이 용이합니다.\n",
       "4. **적응성**: Modular RAG는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 반면, Naive RAG는 적응성이 부족합니다.\n",
       "\n",
       "## 5. Modular RAG의 프로덕션 레벨 이점\n",
       "\n",
       "1. **유연성**: 다양한 모듈로 구성되어 있어 시스템을 조정할 수 있는 능력을 제공합니다.\n",
       "2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있습니다.\n",
       "3. **효율성**: 각 모듈이 특정 작업에 최적화되어 전체 시스템의 성능을 향상시킵니다.\n",
       "4. **개선된 정확성**: 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다.\n",
       "5. **모듈화된 설계**: 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
       "6. **적응성**: 변화하는 환경에 빠르게 적응할 수 있습니다.\n",
       "\n",
       "## 6. 결론\n",
       "Modular RAG는 Naive RAG의 한계를 극복하고, 더 나은 성능과 유연성을 제공하는 정보 검색 및 생성 시스템입니다. 이러한 특성 덕분에 Modular RAG는 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(snapshot[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 최종 보고서: Modular RAG와 Naive RAG의 비교 및 프로덕션 레벨에서의 이점\n",
      "\n",
      "## 1. 서론\n",
      "본 보고서는 Modular RAG(모듈형 RAG)와 Naive RAG(단순 RAG)의 정의, 구조, 작동 방식, 주요 차이점 및 Modular RAG가 프로덕션 레벨에서 제공하는 이점을 설명합니다. 이를 통해 두 시스템의 특성과 Modular RAG의 장점을 명확히 이해할 수 있습니다.\n",
      "\n",
      "## 2. RAG의 정의\n",
      "\n",
      "### 2.1 Modular RAG\n",
      "- **정의**: Modular RAG는 RAG 시스템을 모듈화하여 유연성과 확장성을 높인 구조입니다. 각 모듈은 독립적으로 작동하며, 다양한 데이터 소스와 기술을 결합하여 최적의 결과를 도출합니다.\n",
      "- **특징**:\n",
      "  - **모듈화**: 다양한 모듈을 조합하여 시스템을 구성할 수 있습니다.\n",
      "  - **유연성**: 필요에 따라 모듈을 추가하거나 변경할 수 있습니다.\n",
      "  - **병렬 처리**: 여러 하위 모듈을 동시에 실행하고 결과를 병합할 수 있습니다.\n",
      "\n",
      "### 2.2 Naive RAG\n",
      "- **정의**: Naive RAG는 사용자의 쿼리에 대해 외부 지식 소스에서 관련 정보를 검색하고 이를 LLM에 전달하여 응답을 생성하는 방식입니다.\n",
      "- **특징**:\n",
      "  - **단순성**: 구현이 간단하고 직관적입니다.\n",
      "  - **정보 검색**: 사용자의 쿼리를 기반으로 외부 데이터베이스에서 관련 정보를 검색합니다.\n",
      "  - **응답 생성**: 검색된 정보를 바탕으로 LLM이 응답을 생성합니다.\n",
      "\n",
      "## 3. 구조 및 작동 방식\n",
      "\n",
      "### 3.1 Modular RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. **모듈화된 구성 요소**: 정보 검색 모듈과 생성 모듈로 구성됩니다.\n",
      "  2. **데이터 흐름**: 사용자가 질문을 입력하면 정보 검색 모듈이 관련 데이터를 검색하고, 이 데이터를 생성 모듈에 전달하여 최종 응답을 생성합니다.\n",
      "\n",
      "- **작동 방식**:\n",
      "  1. 쿼리 입력\n",
      "  2. 정보 검색\n",
      "  3. 정보 필터링\n",
      "  4. 응답 생성\n",
      "  5. 결과 출력\n",
      "\n",
      "### 3.2 Naive RAG의 구조 및 작동 방식\n",
      "- **구조**:\n",
      "  1. 질문 입력\n",
      "  2. 정보 검색\n",
      "  3. 문서 선택\n",
      "  4. 생성 모델\n",
      "\n",
      "- **작동 방식**:\n",
      "  - 검색 단계와 생성 단계로 나뉘며, 전통적인 정보 검색 기법을 사용하여 관련 문서를 검색하고, 이를 바탕으로 답변을 생성합니다.\n",
      "\n",
      "## 4. 주요 차이점 비교\n",
      "\n",
      "1. **구조적 유연성**: Modular RAG는 모듈화된 구조로 유연성과 확장성을 제공하며, Naive RAG는 고정된 구조로 한계가 있습니다.\n",
      "2. **모듈화**: Modular RAG는 각 모듈이 특정 역할을 담당하여 다양한 작업을 처리할 수 있지만, Naive RAG는 단일 흐름으로 진행됩니다.\n",
      "3. **성능 최적화**: Modular RAG는 각 모듈을 독립적으로 최적화할 수 있어 성능 개선이 용이합니다.\n",
      "4. **적응성**: Modular RAG는 다양한 시나리오에 맞춰 빠르게 적응할 수 있는 반면, Naive RAG는 적응성이 부족합니다.\n",
      "\n",
      "## 5. Modular RAG의 프로덕션 레벨 이점\n",
      "\n",
      "1. **유연성**: 다양한 모듈로 구성되어 있어 시스템을 조정할 수 있는 능력을 제공합니다.\n",
      "2. **확장성**: 새로운 모듈이나 기능을 쉽게 추가하거나 교체할 수 있습니다.\n",
      "3. **효율성**: 각 모듈이 특정 작업에 최적화되어 전체 시스템의 성능을 향상시킵니다.\n",
      "4. **개선된 정확성**: 다양한 데이터 소스를 통합하여 더 정확하고 관련성 높은 결과를 생성할 수 있습니다.\n",
      "5. **모듈화된 설계**: 특정 모듈에 문제가 발생하더라도 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
      "6. **적응성**: 변화하는 환경에 빠르게 적응할 수 있습니다.\n",
      "\n",
      "## 6. 결론\n",
      "Modular RAG는 Naive RAG의 한계를 극복하고, 더 나은 성능과 유연성을 제공하는 정보 검색 및 생성 시스템입니다. 이러한 특성 덕분에 Modular RAG는 다양한 AI 응용 프로그램에 적합한 솔루션으로 자리잡고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(snapshot[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-note-2024-PFhCXHTX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
