{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 환경변수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../common\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatOpenAI\n",
    "\n",
    "OpenAI 사의 채팅 전용 Large Language Model(LLM) 입니다.\n",
    "객체를 생성할때 필요한 매개변수는 다음과 같습니다.\n",
    "\n",
    "`temperature`\n",
    "- 사용할 샘플링의 온도는 0.0 ~ 2.0 사이의 값을 가지며, 0.0 이면 창의성이 없고, 2.0 이면 창의성이 높아집니다.  \n",
    "\n",
    "`model_name: 모델명`  \n",
    "- gpt-4o\n",
    "- gpt-3.5-turbo  \n",
    "\n",
    "`max_tokens: 최대 토큰수`  \n",
    "- 채팅 완성에서 생성할 토큰의 최대 개수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"LANGCHAIN_TRACING_V2\": os.getenv(\"LANGCHAIN_TRACING_V2\"),\n",
    "    \"LANGCHAIN_ENDPOINT\": os.getenv(\"LANGCHAIN_ENDPOINT\"),\n",
    "    \"LANGCHAIN_API_KEY\": os.getenv(\"LANGCHAIN_API_KEY\"),\n",
    "    \"LANGCHAIN_PROJECT\": os.getenv(\"LANGCHAIN_PROJECT\"),\n",
    "}\n",
    "print(env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:50:31.182681Z",
     "start_time": "2024-11-06T04:50:31.176191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith 추적이 활성화되었습니다. [프로젝트명: langchain_note_2024]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langsmith_logging import langsmith\n",
    "\n",
    "# 인스턴스를 생성할 때 필요한 매개변수를 전달합니다.\n",
    "langsmith(project_name=os.getenv(\"LANGCHAIN_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatOpenAI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:50:36.122829Z",
     "start_time": "2024-11-06T04:50:33.107664Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 질의\n",
    "print(f\"[답변]: {llm.invoke(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:50:37.483011Z",
     "start_time": "2024-11-06T04:50:36.949690Z"
    }
   },
   "outputs": [],
   "source": [
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 질의\n",
    "response = llm.invoke(question)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:50:40.180205Z",
     "start_time": "2024-11-06T04:50:40.176782Z"
    }
   },
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:51:16.002915Z",
     "start_time": "2024-11-06T04:51:15.999055Z"
    }
   },
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogProb 활성화\n",
    "토큰: 문장을 구성하는 작은 단위로, 단어나 글자 같은 요소. 예를 들어, \"안녕하세요\"라는 문장이 있을 때, 이 문장을 여러 개의 토큰으로 나눌 수 있다.  \n",
    "확률: 인공지능 모델이 각 토큰을 예상할 확률을 의미 예를 들어, 모델이 \"안녕\" 다음에 올 단어로 \"하세요\"를 예상할 때, \"하세요\"가 나올 가능성에 대한 확률을 말한다.  \n",
    "로그 값: 이 확률을 다루기 쉽게 만들기 위해 **로그(logarithm)**라는 수학적인 변환을 하는 것인데, 보통 아주 작은 숫자를 더 쉽게 다루기 위해 사용된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:56:13.015036Z",
     "start_time": "2024-11-06T04:56:12.409319Z"
    }
   },
   "outputs": [],
   "source": [
    "# 객체 생성\n",
    "llm_with_logprob = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # 최대 토큰수\n",
    "    model_name=\"gpt-3.5-turbo\",  # 모델명\n",
    ").bind(logprobs=True)\n",
    "\n",
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 질의\n",
    "response = llm_with_logprob.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스트리밍 출력\n",
    "질의의 대한 답변을 실시간으로 받는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T04:58:38.110083Z",
     "start_time": "2024-11-06T04:58:29.085447Z"
    }
   },
   "outputs": [],
   "source": [
    "# 스트림 방식으로 질의\n",
    "# answer 에 스트리밍 답변의 결과를 받습니다.\n",
    "answer = llm.stream(\"서울에서 가을에 산책하기 좋은 장소는? (10곳 추천)\")\n",
    "\n",
    "# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 멀티모달 모델(이미지 인식)\n",
    "\"모달(modal)\"은 보통 \"모드(mode)\" 또는 \"형태(form)\"라는 의미를 지니며, **멀티 모달(multimodal)** 에서 \"모달\"은 정보나 데이터를 전달하는 다양한 형태나 방식을 뜻 함\n",
    "\n",
    "멀티모달의 뜻은 여러가지 형태의 정보를 통합하여 처리하는 기술이나 접근 방식\n",
    "\n",
    "- 텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보  \n",
    "- 이미지: 사진, 그래픽, 그림 등 시각적 정보  \n",
    "- 오디오: 음성, 음악, 소리 효과 등의 청각적 정보  \n",
    "- 비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T07:02:58.413215Z",
     "start_time": "2024-11-06T07:02:53.818827Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from multimodal import MultiModal\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # 최대 토큰수\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "multimodal_llm = MultiModal(llm)\n",
    "\n",
    "IMAGE = (\"../images/sample.png\", \"image\")\n",
    "\n",
    "# 로컬 PC 에 저장되어 있는 이미지의 경로 입력\n",
    "IMAGE_PATH_FROM_FILE = \"../images/sample.png\"\n",
    "\n",
    "# 이미지 파일로 부터 질의(스트림 방식)\n",
    "answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)\n",
    "\n",
    "# 스트리밍 방식으로 각 토큰을 출력\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T07:15:04.977696Z",
     "start_time": "2024-11-06T07:14:58.533451Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from multimodal import MultiModal\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # 최대 토큰수\n",
    "    model_name=\"gpt-4o\",  # 모델명\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"당신은 UI/UX를 디자인하는 기획 AI 어시스턴트 입니다. \n",
    "당신의 임무는 주어진 화면을 바탕으로 보안되어야 할 점들을 찾아 친절하게 답변하는 것입니다.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"당신에게 주어진 이미지는 웹 디자인 화면입니다. 좀 더 개선되었으면 하는 것을 알려주세요.\"\"\"\n",
    "\n",
    "multimodal_llm = MultiModal(llm, system_prompt, user_prompt)\n",
    "\n",
    "IMAGE = (\"../images/sample.png\", \"image\")\n",
    "\n",
    "# 로컬 PC 에 저장되어 있는 이미지의 경로 입력\n",
    "IMAGE_PATH_FROM_FILE = \"../images/sample.png\"\n",
    "\n",
    "# 이미지 파일로 부터 질의(스트림 방식)\n",
    "answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)\n",
    "\n",
    "# 스트리밍 방식으로 각 토큰을 출력\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-note-2024-PFhCXHTX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
